{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 615,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016260162601626016,
      "grad_norm": 14.123039245605469,
      "learning_rate": 0.0002,
      "loss": 3.6058,
      "step": 1
    },
    {
      "epoch": 0.0032520325203252032,
      "grad_norm": 12.442439079284668,
      "learning_rate": 0.00019983739837398375,
      "loss": 2.7201,
      "step": 2
    },
    {
      "epoch": 0.004878048780487805,
      "grad_norm": 10.934565544128418,
      "learning_rate": 0.0001996747967479675,
      "loss": 1.943,
      "step": 3
    },
    {
      "epoch": 0.0065040650406504065,
      "grad_norm": 9.618895530700684,
      "learning_rate": 0.00019951219512195123,
      "loss": 1.4261,
      "step": 4
    },
    {
      "epoch": 0.008130081300813009,
      "grad_norm": 7.239424228668213,
      "learning_rate": 0.00019934959349593497,
      "loss": 0.8741,
      "step": 5
    },
    {
      "epoch": 0.00975609756097561,
      "grad_norm": 4.3384785652160645,
      "learning_rate": 0.0001991869918699187,
      "loss": 0.5331,
      "step": 6
    },
    {
      "epoch": 0.011382113821138212,
      "grad_norm": 2.646343231201172,
      "learning_rate": 0.00019902439024390244,
      "loss": 0.3537,
      "step": 7
    },
    {
      "epoch": 0.013008130081300813,
      "grad_norm": 1.4339319467544556,
      "learning_rate": 0.00019886178861788618,
      "loss": 0.2425,
      "step": 8
    },
    {
      "epoch": 0.014634146341463415,
      "grad_norm": 0.8906335234642029,
      "learning_rate": 0.00019869918699186992,
      "loss": 0.1912,
      "step": 9
    },
    {
      "epoch": 0.016260162601626018,
      "grad_norm": 0.5569081902503967,
      "learning_rate": 0.00019853658536585366,
      "loss": 0.1556,
      "step": 10
    },
    {
      "epoch": 0.01788617886178862,
      "grad_norm": 0.36139819025993347,
      "learning_rate": 0.0001983739837398374,
      "loss": 0.1384,
      "step": 11
    },
    {
      "epoch": 0.01951219512195122,
      "grad_norm": 0.2682724893093109,
      "learning_rate": 0.00019821138211382117,
      "loss": 0.1375,
      "step": 12
    },
    {
      "epoch": 0.02113821138211382,
      "grad_norm": 0.2210274189710617,
      "learning_rate": 0.00019804878048780488,
      "loss": 0.1195,
      "step": 13
    },
    {
      "epoch": 0.022764227642276424,
      "grad_norm": 0.17774926126003265,
      "learning_rate": 0.00019788617886178862,
      "loss": 0.111,
      "step": 14
    },
    {
      "epoch": 0.024390243902439025,
      "grad_norm": 0.17169004678726196,
      "learning_rate": 0.00019772357723577238,
      "loss": 0.1309,
      "step": 15
    },
    {
      "epoch": 0.026016260162601626,
      "grad_norm": 0.16225352883338928,
      "learning_rate": 0.0001975609756097561,
      "loss": 0.1157,
      "step": 16
    },
    {
      "epoch": 0.027642276422764227,
      "grad_norm": 0.14445935189723969,
      "learning_rate": 0.00019739837398373983,
      "loss": 0.1056,
      "step": 17
    },
    {
      "epoch": 0.02926829268292683,
      "grad_norm": 0.13701938092708588,
      "learning_rate": 0.0001972357723577236,
      "loss": 0.0979,
      "step": 18
    },
    {
      "epoch": 0.030894308943089432,
      "grad_norm": 0.14380183815956116,
      "learning_rate": 0.0001970731707317073,
      "loss": 0.0985,
      "step": 19
    },
    {
      "epoch": 0.032520325203252036,
      "grad_norm": 0.13301730155944824,
      "learning_rate": 0.00019691056910569105,
      "loss": 0.0929,
      "step": 20
    },
    {
      "epoch": 0.03414634146341464,
      "grad_norm": 0.14615321159362793,
      "learning_rate": 0.00019674796747967482,
      "loss": 0.0946,
      "step": 21
    },
    {
      "epoch": 0.03577235772357724,
      "grad_norm": 0.1557343602180481,
      "learning_rate": 0.00019658536585365856,
      "loss": 0.0867,
      "step": 22
    },
    {
      "epoch": 0.03739837398373984,
      "grad_norm": 0.14032889902591705,
      "learning_rate": 0.00019642276422764227,
      "loss": 0.0945,
      "step": 23
    },
    {
      "epoch": 0.03902439024390244,
      "grad_norm": 0.13826340436935425,
      "learning_rate": 0.00019626016260162603,
      "loss": 0.0795,
      "step": 24
    },
    {
      "epoch": 0.04065040650406504,
      "grad_norm": 0.15749295055866241,
      "learning_rate": 0.00019609756097560977,
      "loss": 0.0822,
      "step": 25
    },
    {
      "epoch": 0.04227642276422764,
      "grad_norm": 0.15631099045276642,
      "learning_rate": 0.00019593495934959348,
      "loss": 0.0767,
      "step": 26
    },
    {
      "epoch": 0.04390243902439024,
      "grad_norm": 0.14138931035995483,
      "learning_rate": 0.00019577235772357725,
      "loss": 0.0779,
      "step": 27
    },
    {
      "epoch": 0.04552845528455285,
      "grad_norm": 0.1426640897989273,
      "learning_rate": 0.000195609756097561,
      "loss": 0.0749,
      "step": 28
    },
    {
      "epoch": 0.04715447154471545,
      "grad_norm": 0.1442413330078125,
      "learning_rate": 0.00019544715447154473,
      "loss": 0.0685,
      "step": 29
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 0.1501743495464325,
      "learning_rate": 0.00019528455284552847,
      "loss": 0.0679,
      "step": 30
    },
    {
      "epoch": 0.05040650406504065,
      "grad_norm": 0.1304495483636856,
      "learning_rate": 0.0001951219512195122,
      "loss": 0.0769,
      "step": 31
    },
    {
      "epoch": 0.05203252032520325,
      "grad_norm": 0.15433509647846222,
      "learning_rate": 0.00019495934959349594,
      "loss": 0.0614,
      "step": 32
    },
    {
      "epoch": 0.05365853658536585,
      "grad_norm": 0.14335307478904724,
      "learning_rate": 0.00019479674796747968,
      "loss": 0.0656,
      "step": 33
    },
    {
      "epoch": 0.055284552845528454,
      "grad_norm": 0.15990345180034637,
      "learning_rate": 0.00019463414634146342,
      "loss": 0.0508,
      "step": 34
    },
    {
      "epoch": 0.056910569105691054,
      "grad_norm": 0.14366264641284943,
      "learning_rate": 0.00019447154471544716,
      "loss": 0.0553,
      "step": 35
    },
    {
      "epoch": 0.05853658536585366,
      "grad_norm": 0.20658202469348907,
      "learning_rate": 0.0001943089430894309,
      "loss": 0.0771,
      "step": 36
    },
    {
      "epoch": 0.06016260162601626,
      "grad_norm": 0.15433137118816376,
      "learning_rate": 0.00019414634146341464,
      "loss": 0.0582,
      "step": 37
    },
    {
      "epoch": 0.061788617886178863,
      "grad_norm": 0.15748094022274017,
      "learning_rate": 0.0001939837398373984,
      "loss": 0.0588,
      "step": 38
    },
    {
      "epoch": 0.06341463414634146,
      "grad_norm": 0.13017624616622925,
      "learning_rate": 0.00019382113821138212,
      "loss": 0.0519,
      "step": 39
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 0.16890975832939148,
      "learning_rate": 0.00019365853658536586,
      "loss": 0.0522,
      "step": 40
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.11865324527025223,
      "learning_rate": 0.00019349593495934962,
      "loss": 0.0317,
      "step": 41
    },
    {
      "epoch": 0.06829268292682927,
      "grad_norm": 0.12453564256429672,
      "learning_rate": 0.00019333333333333333,
      "loss": 0.0445,
      "step": 42
    },
    {
      "epoch": 0.06991869918699187,
      "grad_norm": 0.12275237590074539,
      "learning_rate": 0.00019317073170731707,
      "loss": 0.0436,
      "step": 43
    },
    {
      "epoch": 0.07154471544715447,
      "grad_norm": 0.13192838430404663,
      "learning_rate": 0.00019300813008130084,
      "loss": 0.0422,
      "step": 44
    },
    {
      "epoch": 0.07317073170731707,
      "grad_norm": 0.10686997324228287,
      "learning_rate": 0.00019284552845528455,
      "loss": 0.0352,
      "step": 45
    },
    {
      "epoch": 0.07479674796747968,
      "grad_norm": 0.10088750720024109,
      "learning_rate": 0.0001926829268292683,
      "loss": 0.0312,
      "step": 46
    },
    {
      "epoch": 0.07642276422764227,
      "grad_norm": 0.10405315458774567,
      "learning_rate": 0.00019252032520325206,
      "loss": 0.025,
      "step": 47
    },
    {
      "epoch": 0.07804878048780488,
      "grad_norm": 0.11609800904989243,
      "learning_rate": 0.0001923577235772358,
      "loss": 0.0374,
      "step": 48
    },
    {
      "epoch": 0.07967479674796749,
      "grad_norm": 0.10765493661165237,
      "learning_rate": 0.0001921951219512195,
      "loss": 0.0399,
      "step": 49
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 0.1294170469045639,
      "learning_rate": 0.00019203252032520327,
      "loss": 0.0415,
      "step": 50
    },
    {
      "epoch": 0.08292682926829269,
      "grad_norm": 0.09018323570489883,
      "learning_rate": 0.000191869918699187,
      "loss": 0.0372,
      "step": 51
    },
    {
      "epoch": 0.08455284552845528,
      "grad_norm": 0.08691918849945068,
      "learning_rate": 0.00019170731707317072,
      "loss": 0.024,
      "step": 52
    },
    {
      "epoch": 0.08617886178861789,
      "grad_norm": 0.13573385775089264,
      "learning_rate": 0.0001915447154471545,
      "loss": 0.0384,
      "step": 53
    },
    {
      "epoch": 0.08780487804878048,
      "grad_norm": 0.07842563092708588,
      "learning_rate": 0.00019138211382113823,
      "loss": 0.0182,
      "step": 54
    },
    {
      "epoch": 0.08943089430894309,
      "grad_norm": 0.125136598944664,
      "learning_rate": 0.00019121951219512194,
      "loss": 0.0516,
      "step": 55
    },
    {
      "epoch": 0.0910569105691057,
      "grad_norm": 0.06792032718658447,
      "learning_rate": 0.0001910569105691057,
      "loss": 0.0223,
      "step": 56
    },
    {
      "epoch": 0.09268292682926829,
      "grad_norm": 0.1047743409872055,
      "learning_rate": 0.00019089430894308945,
      "loss": 0.0368,
      "step": 57
    },
    {
      "epoch": 0.0943089430894309,
      "grad_norm": 0.09451661258935928,
      "learning_rate": 0.00019073170731707319,
      "loss": 0.0132,
      "step": 58
    },
    {
      "epoch": 0.0959349593495935,
      "grad_norm": 0.09582289308309555,
      "learning_rate": 0.00019056910569105692,
      "loss": 0.0309,
      "step": 59
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 0.10035395622253418,
      "learning_rate": 0.00019040650406504066,
      "loss": 0.0416,
      "step": 60
    },
    {
      "epoch": 0.0991869918699187,
      "grad_norm": 0.08037086576223373,
      "learning_rate": 0.0001902439024390244,
      "loss": 0.032,
      "step": 61
    },
    {
      "epoch": 0.1008130081300813,
      "grad_norm": 0.07024725526571274,
      "learning_rate": 0.00019008130081300814,
      "loss": 0.0269,
      "step": 62
    },
    {
      "epoch": 0.1024390243902439,
      "grad_norm": 0.07535912841558456,
      "learning_rate": 0.00018991869918699188,
      "loss": 0.026,
      "step": 63
    },
    {
      "epoch": 0.1040650406504065,
      "grad_norm": 0.10366050899028778,
      "learning_rate": 0.00018975609756097562,
      "loss": 0.0162,
      "step": 64
    },
    {
      "epoch": 0.10569105691056911,
      "grad_norm": 0.08227387070655823,
      "learning_rate": 0.00018959349593495936,
      "loss": 0.0217,
      "step": 65
    },
    {
      "epoch": 0.1073170731707317,
      "grad_norm": 0.0741656944155693,
      "learning_rate": 0.0001894308943089431,
      "loss": 0.0301,
      "step": 66
    },
    {
      "epoch": 0.10894308943089431,
      "grad_norm": 0.10647080093622208,
      "learning_rate": 0.00018926829268292684,
      "loss": 0.0252,
      "step": 67
    },
    {
      "epoch": 0.11056910569105691,
      "grad_norm": 0.07371451705694199,
      "learning_rate": 0.00018910569105691057,
      "loss": 0.0318,
      "step": 68
    },
    {
      "epoch": 0.11219512195121951,
      "grad_norm": 0.05689657852053642,
      "learning_rate": 0.00018894308943089431,
      "loss": 0.0076,
      "step": 69
    },
    {
      "epoch": 0.11382113821138211,
      "grad_norm": 0.09139333665370941,
      "learning_rate": 0.00018878048780487805,
      "loss": 0.023,
      "step": 70
    },
    {
      "epoch": 0.11544715447154472,
      "grad_norm": 0.07399555295705795,
      "learning_rate": 0.0001886178861788618,
      "loss": 0.0294,
      "step": 71
    },
    {
      "epoch": 0.11707317073170732,
      "grad_norm": 0.08114836364984512,
      "learning_rate": 0.00018845528455284553,
      "loss": 0.028,
      "step": 72
    },
    {
      "epoch": 0.11869918699186992,
      "grad_norm": 0.07716210186481476,
      "learning_rate": 0.00018829268292682927,
      "loss": 0.0232,
      "step": 73
    },
    {
      "epoch": 0.12032520325203253,
      "grad_norm": 0.07454870641231537,
      "learning_rate": 0.000188130081300813,
      "loss": 0.0167,
      "step": 74
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 0.10065023601055145,
      "learning_rate": 0.00018796747967479675,
      "loss": 0.0285,
      "step": 75
    },
    {
      "epoch": 0.12357723577235773,
      "grad_norm": 0.07416273653507233,
      "learning_rate": 0.0001878048780487805,
      "loss": 0.02,
      "step": 76
    },
    {
      "epoch": 0.12520325203252033,
      "grad_norm": 0.06341444700956345,
      "learning_rate": 0.00018764227642276425,
      "loss": 0.0187,
      "step": 77
    },
    {
      "epoch": 0.12682926829268293,
      "grad_norm": 0.06446120142936707,
      "learning_rate": 0.00018747967479674796,
      "loss": 0.0087,
      "step": 78
    },
    {
      "epoch": 0.12845528455284552,
      "grad_norm": 0.060524340718984604,
      "learning_rate": 0.00018731707317073173,
      "loss": 0.0134,
      "step": 79
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 0.04947732016444206,
      "learning_rate": 0.00018715447154471547,
      "loss": 0.0067,
      "step": 80
    },
    {
      "epoch": 0.13170731707317074,
      "grad_norm": 0.05538910627365112,
      "learning_rate": 0.00018699186991869918,
      "loss": 0.009,
      "step": 81
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.08997729420661926,
      "learning_rate": 0.00018682926829268295,
      "loss": 0.0226,
      "step": 82
    },
    {
      "epoch": 0.13495934959349593,
      "grad_norm": 0.05830923467874527,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.0118,
      "step": 83
    },
    {
      "epoch": 0.13658536585365855,
      "grad_norm": 0.05442948266863823,
      "learning_rate": 0.0001865040650406504,
      "loss": 0.0136,
      "step": 84
    },
    {
      "epoch": 0.13821138211382114,
      "grad_norm": 0.049664344638586044,
      "learning_rate": 0.00018634146341463416,
      "loss": 0.0086,
      "step": 85
    },
    {
      "epoch": 0.13983739837398373,
      "grad_norm": 0.07180367410182953,
      "learning_rate": 0.0001861788617886179,
      "loss": 0.0145,
      "step": 86
    },
    {
      "epoch": 0.14146341463414633,
      "grad_norm": 0.08550627529621124,
      "learning_rate": 0.00018601626016260164,
      "loss": 0.0318,
      "step": 87
    },
    {
      "epoch": 0.14308943089430895,
      "grad_norm": 0.1153000071644783,
      "learning_rate": 0.00018585365853658538,
      "loss": 0.021,
      "step": 88
    },
    {
      "epoch": 0.14471544715447154,
      "grad_norm": 0.11239909380674362,
      "learning_rate": 0.00018569105691056912,
      "loss": 0.0339,
      "step": 89
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 0.050321921706199646,
      "learning_rate": 0.00018552845528455286,
      "loss": 0.0155,
      "step": 90
    },
    {
      "epoch": 0.14796747967479676,
      "grad_norm": 0.08015507459640503,
      "learning_rate": 0.0001853658536585366,
      "loss": 0.0169,
      "step": 91
    },
    {
      "epoch": 0.14959349593495935,
      "grad_norm": 0.06360334157943726,
      "learning_rate": 0.00018520325203252034,
      "loss": 0.0042,
      "step": 92
    },
    {
      "epoch": 0.15121951219512195,
      "grad_norm": 0.05382903292775154,
      "learning_rate": 0.00018504065040650408,
      "loss": 0.0147,
      "step": 93
    },
    {
      "epoch": 0.15284552845528454,
      "grad_norm": 0.04847598448395729,
      "learning_rate": 0.00018487804878048782,
      "loss": 0.0056,
      "step": 94
    },
    {
      "epoch": 0.15447154471544716,
      "grad_norm": 0.06303137540817261,
      "learning_rate": 0.00018471544715447155,
      "loss": 0.0109,
      "step": 95
    },
    {
      "epoch": 0.15609756097560976,
      "grad_norm": 0.061439476907253265,
      "learning_rate": 0.0001845528455284553,
      "loss": 0.018,
      "step": 96
    },
    {
      "epoch": 0.15772357723577235,
      "grad_norm": 0.06210711970925331,
      "learning_rate": 0.00018439024390243903,
      "loss": 0.0195,
      "step": 97
    },
    {
      "epoch": 0.15934959349593497,
      "grad_norm": 0.05871272832155228,
      "learning_rate": 0.00018422764227642277,
      "loss": 0.0114,
      "step": 98
    },
    {
      "epoch": 0.16097560975609757,
      "grad_norm": 0.03230946511030197,
      "learning_rate": 0.0001840650406504065,
      "loss": 0.0028,
      "step": 99
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 0.06922970712184906,
      "learning_rate": 0.00018390243902439025,
      "loss": 0.019,
      "step": 100
    },
    {
      "epoch": 0.16422764227642275,
      "grad_norm": 0.06678000837564468,
      "learning_rate": 0.000183739837398374,
      "loss": 0.0083,
      "step": 101
    },
    {
      "epoch": 0.16585365853658537,
      "grad_norm": 0.07466066628694534,
      "learning_rate": 0.00018357723577235773,
      "loss": 0.0248,
      "step": 102
    },
    {
      "epoch": 0.16747967479674797,
      "grad_norm": 0.10639944672584534,
      "learning_rate": 0.0001834146341463415,
      "loss": 0.0297,
      "step": 103
    },
    {
      "epoch": 0.16910569105691056,
      "grad_norm": 0.08071240782737732,
      "learning_rate": 0.0001832520325203252,
      "loss": 0.0139,
      "step": 104
    },
    {
      "epoch": 0.17073170731707318,
      "grad_norm": 0.04677708446979523,
      "learning_rate": 0.00018308943089430894,
      "loss": 0.0026,
      "step": 105
    },
    {
      "epoch": 0.17235772357723578,
      "grad_norm": 0.040342919528484344,
      "learning_rate": 0.0001829268292682927,
      "loss": 0.0026,
      "step": 106
    },
    {
      "epoch": 0.17398373983739837,
      "grad_norm": 0.09369560331106186,
      "learning_rate": 0.00018276422764227642,
      "loss": 0.0246,
      "step": 107
    },
    {
      "epoch": 0.17560975609756097,
      "grad_norm": 0.04911818727850914,
      "learning_rate": 0.00018260162601626016,
      "loss": 0.0118,
      "step": 108
    },
    {
      "epoch": 0.1772357723577236,
      "grad_norm": 0.06689990311861038,
      "learning_rate": 0.00018243902439024393,
      "loss": 0.0085,
      "step": 109
    },
    {
      "epoch": 0.17886178861788618,
      "grad_norm": 0.058269962668418884,
      "learning_rate": 0.00018227642276422764,
      "loss": 0.0091,
      "step": 110
    },
    {
      "epoch": 0.18048780487804877,
      "grad_norm": 0.05585408955812454,
      "learning_rate": 0.00018211382113821138,
      "loss": 0.0169,
      "step": 111
    },
    {
      "epoch": 0.1821138211382114,
      "grad_norm": 0.07020600885152817,
      "learning_rate": 0.00018195121951219514,
      "loss": 0.0078,
      "step": 112
    },
    {
      "epoch": 0.183739837398374,
      "grad_norm": 0.06154538318514824,
      "learning_rate": 0.00018178861788617888,
      "loss": 0.015,
      "step": 113
    },
    {
      "epoch": 0.18536585365853658,
      "grad_norm": 0.05866541340947151,
      "learning_rate": 0.0001816260162601626,
      "loss": 0.0088,
      "step": 114
    },
    {
      "epoch": 0.18699186991869918,
      "grad_norm": 0.08026880025863647,
      "learning_rate": 0.00018146341463414636,
      "loss": 0.0147,
      "step": 115
    },
    {
      "epoch": 0.1886178861788618,
      "grad_norm": 0.04392948001623154,
      "learning_rate": 0.0001813008130081301,
      "loss": 0.0088,
      "step": 116
    },
    {
      "epoch": 0.1902439024390244,
      "grad_norm": 0.057295095175504684,
      "learning_rate": 0.0001811382113821138,
      "loss": 0.0089,
      "step": 117
    },
    {
      "epoch": 0.191869918699187,
      "grad_norm": 0.04084128141403198,
      "learning_rate": 0.00018097560975609758,
      "loss": 0.0085,
      "step": 118
    },
    {
      "epoch": 0.19349593495934958,
      "grad_norm": 0.07043016701936722,
      "learning_rate": 0.00018081300813008132,
      "loss": 0.0126,
      "step": 119
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 0.04601474106311798,
      "learning_rate": 0.00018065040650406503,
      "loss": 0.0066,
      "step": 120
    },
    {
      "epoch": 0.1967479674796748,
      "grad_norm": 0.08078140765428543,
      "learning_rate": 0.0001804878048780488,
      "loss": 0.011,
      "step": 121
    },
    {
      "epoch": 0.1983739837398374,
      "grad_norm": 0.0810873880982399,
      "learning_rate": 0.00018032520325203253,
      "loss": 0.019,
      "step": 122
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.07094967365264893,
      "learning_rate": 0.00018016260162601625,
      "loss": 0.0128,
      "step": 123
    },
    {
      "epoch": 0.2016260162601626,
      "grad_norm": 0.06718281656503677,
      "learning_rate": 0.00018,
      "loss": 0.0224,
      "step": 124
    },
    {
      "epoch": 0.2032520325203252,
      "grad_norm": 0.060588691383600235,
      "learning_rate": 0.00017983739837398375,
      "loss": 0.0153,
      "step": 125
    },
    {
      "epoch": 0.2048780487804878,
      "grad_norm": 0.047028928995132446,
      "learning_rate": 0.0001796747967479675,
      "loss": 0.0065,
      "step": 126
    },
    {
      "epoch": 0.20650406504065041,
      "grad_norm": 0.0495992936193943,
      "learning_rate": 0.00017951219512195123,
      "loss": 0.0098,
      "step": 127
    },
    {
      "epoch": 0.208130081300813,
      "grad_norm": 0.05529371276497841,
      "learning_rate": 0.00017934959349593497,
      "loss": 0.0147,
      "step": 128
    },
    {
      "epoch": 0.2097560975609756,
      "grad_norm": 0.04449852555990219,
      "learning_rate": 0.0001791869918699187,
      "loss": 0.0048,
      "step": 129
    },
    {
      "epoch": 0.21138211382113822,
      "grad_norm": 0.09078140556812286,
      "learning_rate": 0.00017902439024390245,
      "loss": 0.0242,
      "step": 130
    },
    {
      "epoch": 0.21300813008130082,
      "grad_norm": 0.033828720450401306,
      "learning_rate": 0.00017886178861788618,
      "loss": 0.0052,
      "step": 131
    },
    {
      "epoch": 0.2146341463414634,
      "grad_norm": 0.03269187733530998,
      "learning_rate": 0.00017869918699186995,
      "loss": 0.0019,
      "step": 132
    },
    {
      "epoch": 0.216260162601626,
      "grad_norm": 0.03835708275437355,
      "learning_rate": 0.00017853658536585366,
      "loss": 0.0051,
      "step": 133
    },
    {
      "epoch": 0.21788617886178863,
      "grad_norm": 0.06826446205377579,
      "learning_rate": 0.0001783739837398374,
      "loss": 0.0191,
      "step": 134
    },
    {
      "epoch": 0.21951219512195122,
      "grad_norm": 0.07213122397661209,
      "learning_rate": 0.00017821138211382117,
      "loss": 0.0117,
      "step": 135
    },
    {
      "epoch": 0.22113821138211381,
      "grad_norm": 0.030881166458129883,
      "learning_rate": 0.00017804878048780488,
      "loss": 0.0017,
      "step": 136
    },
    {
      "epoch": 0.22276422764227644,
      "grad_norm": 0.05711333826184273,
      "learning_rate": 0.00017788617886178862,
      "loss": 0.0036,
      "step": 137
    },
    {
      "epoch": 0.22439024390243903,
      "grad_norm": 0.04516725614666939,
      "learning_rate": 0.00017772357723577238,
      "loss": 0.0069,
      "step": 138
    },
    {
      "epoch": 0.22601626016260162,
      "grad_norm": 0.06221675127744675,
      "learning_rate": 0.0001775609756097561,
      "loss": 0.0102,
      "step": 139
    },
    {
      "epoch": 0.22764227642276422,
      "grad_norm": 0.05953823775053024,
      "learning_rate": 0.00017739837398373983,
      "loss": 0.0162,
      "step": 140
    },
    {
      "epoch": 0.22926829268292684,
      "grad_norm": 0.06301414966583252,
      "learning_rate": 0.0001772357723577236,
      "loss": 0.0074,
      "step": 141
    },
    {
      "epoch": 0.23089430894308943,
      "grad_norm": 0.0576285682618618,
      "learning_rate": 0.00017707317073170734,
      "loss": 0.0128,
      "step": 142
    },
    {
      "epoch": 0.23252032520325203,
      "grad_norm": 0.06300925463438034,
      "learning_rate": 0.00017691056910569105,
      "loss": 0.0134,
      "step": 143
    },
    {
      "epoch": 0.23414634146341465,
      "grad_norm": 0.05683533847332001,
      "learning_rate": 0.00017674796747967482,
      "loss": 0.0127,
      "step": 144
    },
    {
      "epoch": 0.23577235772357724,
      "grad_norm": 0.06731590628623962,
      "learning_rate": 0.00017658536585365856,
      "loss": 0.015,
      "step": 145
    },
    {
      "epoch": 0.23739837398373984,
      "grad_norm": 0.036470796912908554,
      "learning_rate": 0.00017642276422764227,
      "loss": 0.0039,
      "step": 146
    },
    {
      "epoch": 0.23902439024390243,
      "grad_norm": 0.04167039319872856,
      "learning_rate": 0.00017626016260162603,
      "loss": 0.0047,
      "step": 147
    },
    {
      "epoch": 0.24065040650406505,
      "grad_norm": 0.10255386680364609,
      "learning_rate": 0.00017609756097560977,
      "loss": 0.0257,
      "step": 148
    },
    {
      "epoch": 0.24227642276422764,
      "grad_norm": 0.04812590032815933,
      "learning_rate": 0.00017593495934959349,
      "loss": 0.0046,
      "step": 149
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 0.024177923798561096,
      "learning_rate": 0.00017577235772357725,
      "loss": 0.0013,
      "step": 150
    },
    {
      "epoch": 0.24552845528455283,
      "grad_norm": 0.04535578191280365,
      "learning_rate": 0.000175609756097561,
      "loss": 0.0046,
      "step": 151
    },
    {
      "epoch": 0.24715447154471545,
      "grad_norm": 0.031351860612630844,
      "learning_rate": 0.00017544715447154473,
      "loss": 0.0016,
      "step": 152
    },
    {
      "epoch": 0.24878048780487805,
      "grad_norm": 0.02234712801873684,
      "learning_rate": 0.00017528455284552847,
      "loss": 0.0012,
      "step": 153
    },
    {
      "epoch": 0.25040650406504067,
      "grad_norm": 0.02679426036775112,
      "learning_rate": 0.0001751219512195122,
      "loss": 0.0028,
      "step": 154
    },
    {
      "epoch": 0.25203252032520324,
      "grad_norm": 0.05651911348104477,
      "learning_rate": 0.00017495934959349595,
      "loss": 0.0086,
      "step": 155
    },
    {
      "epoch": 0.25365853658536586,
      "grad_norm": 0.026251357048749924,
      "learning_rate": 0.00017479674796747969,
      "loss": 0.0027,
      "step": 156
    },
    {
      "epoch": 0.2552845528455285,
      "grad_norm": 0.01795532926917076,
      "learning_rate": 0.00017463414634146342,
      "loss": 0.001,
      "step": 157
    },
    {
      "epoch": 0.25691056910569104,
      "grad_norm": 0.0567910261452198,
      "learning_rate": 0.00017447154471544716,
      "loss": 0.004,
      "step": 158
    },
    {
      "epoch": 0.25853658536585367,
      "grad_norm": 0.05590607598423958,
      "learning_rate": 0.0001743089430894309,
      "loss": 0.0079,
      "step": 159
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 0.034626446664333344,
      "learning_rate": 0.00017414634146341464,
      "loss": 0.0025,
      "step": 160
    },
    {
      "epoch": 0.26178861788617885,
      "grad_norm": 0.01331181637942791,
      "learning_rate": 0.00017398373983739838,
      "loss": 0.001,
      "step": 161
    },
    {
      "epoch": 0.2634146341463415,
      "grad_norm": 0.08110161870718002,
      "learning_rate": 0.00017382113821138212,
      "loss": 0.0105,
      "step": 162
    },
    {
      "epoch": 0.26504065040650404,
      "grad_norm": 0.06539864093065262,
      "learning_rate": 0.00017365853658536586,
      "loss": 0.0139,
      "step": 163
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.09633517265319824,
      "learning_rate": 0.0001734959349593496,
      "loss": 0.0129,
      "step": 164
    },
    {
      "epoch": 0.2682926829268293,
      "grad_norm": 0.05106749385595322,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.0071,
      "step": 165
    },
    {
      "epoch": 0.26991869918699185,
      "grad_norm": 0.0644776001572609,
      "learning_rate": 0.00017317073170731708,
      "loss": 0.0109,
      "step": 166
    },
    {
      "epoch": 0.27154471544715447,
      "grad_norm": 0.07961447536945343,
      "learning_rate": 0.00017300813008130081,
      "loss": 0.0173,
      "step": 167
    },
    {
      "epoch": 0.2731707317073171,
      "grad_norm": 0.054788727313280106,
      "learning_rate": 0.00017284552845528455,
      "loss": 0.0051,
      "step": 168
    },
    {
      "epoch": 0.27479674796747966,
      "grad_norm": 0.05671359598636627,
      "learning_rate": 0.0001726829268292683,
      "loss": 0.0103,
      "step": 169
    },
    {
      "epoch": 0.2764227642276423,
      "grad_norm": 0.10320157557725906,
      "learning_rate": 0.00017252032520325203,
      "loss": 0.0178,
      "step": 170
    },
    {
      "epoch": 0.2780487804878049,
      "grad_norm": 0.04981161653995514,
      "learning_rate": 0.0001723577235772358,
      "loss": 0.0031,
      "step": 171
    },
    {
      "epoch": 0.27967479674796747,
      "grad_norm": 0.05624835193157196,
      "learning_rate": 0.0001721951219512195,
      "loss": 0.0116,
      "step": 172
    },
    {
      "epoch": 0.2813008130081301,
      "grad_norm": 0.06805776059627533,
      "learning_rate": 0.00017203252032520325,
      "loss": 0.0167,
      "step": 173
    },
    {
      "epoch": 0.28292682926829266,
      "grad_norm": 0.038390420377254486,
      "learning_rate": 0.00017186991869918701,
      "loss": 0.0036,
      "step": 174
    },
    {
      "epoch": 0.2845528455284553,
      "grad_norm": 0.033778928220272064,
      "learning_rate": 0.00017170731707317073,
      "loss": 0.0052,
      "step": 175
    },
    {
      "epoch": 0.2861788617886179,
      "grad_norm": 0.023776765912771225,
      "learning_rate": 0.00017154471544715446,
      "loss": 0.002,
      "step": 176
    },
    {
      "epoch": 0.28780487804878047,
      "grad_norm": 0.07257568836212158,
      "learning_rate": 0.00017138211382113823,
      "loss": 0.0103,
      "step": 177
    },
    {
      "epoch": 0.2894308943089431,
      "grad_norm": 0.06379688531160355,
      "learning_rate": 0.00017121951219512194,
      "loss": 0.0099,
      "step": 178
    },
    {
      "epoch": 0.2910569105691057,
      "grad_norm": 0.18147827684879303,
      "learning_rate": 0.00017105691056910568,
      "loss": 0.0264,
      "step": 179
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 0.020248055458068848,
      "learning_rate": 0.00017089430894308945,
      "loss": 0.001,
      "step": 180
    },
    {
      "epoch": 0.2943089430894309,
      "grad_norm": 0.031300272792577744,
      "learning_rate": 0.0001707317073170732,
      "loss": 0.0031,
      "step": 181
    },
    {
      "epoch": 0.2959349593495935,
      "grad_norm": 0.046243537217378616,
      "learning_rate": 0.00017056910569105693,
      "loss": 0.0079,
      "step": 182
    },
    {
      "epoch": 0.2975609756097561,
      "grad_norm": 0.02697622962296009,
      "learning_rate": 0.00017040650406504066,
      "loss": 0.0026,
      "step": 183
    },
    {
      "epoch": 0.2991869918699187,
      "grad_norm": 0.07677304744720459,
      "learning_rate": 0.0001702439024390244,
      "loss": 0.013,
      "step": 184
    },
    {
      "epoch": 0.3008130081300813,
      "grad_norm": 0.050438620150089264,
      "learning_rate": 0.00017008130081300814,
      "loss": 0.0064,
      "step": 185
    },
    {
      "epoch": 0.3024390243902439,
      "grad_norm": 0.04538458585739136,
      "learning_rate": 0.00016991869918699188,
      "loss": 0.004,
      "step": 186
    },
    {
      "epoch": 0.3040650406504065,
      "grad_norm": 0.0314907431602478,
      "learning_rate": 0.00016975609756097562,
      "loss": 0.0038,
      "step": 187
    },
    {
      "epoch": 0.3056910569105691,
      "grad_norm": 0.0368216447532177,
      "learning_rate": 0.00016959349593495936,
      "loss": 0.004,
      "step": 188
    },
    {
      "epoch": 0.3073170731707317,
      "grad_norm": 0.047670796513557434,
      "learning_rate": 0.0001694308943089431,
      "loss": 0.0062,
      "step": 189
    },
    {
      "epoch": 0.3089430894308943,
      "grad_norm": 0.03753972798585892,
      "learning_rate": 0.00016926829268292684,
      "loss": 0.0027,
      "step": 190
    },
    {
      "epoch": 0.3105691056910569,
      "grad_norm": 0.015697093680500984,
      "learning_rate": 0.00016910569105691058,
      "loss": 0.0008,
      "step": 191
    },
    {
      "epoch": 0.3121951219512195,
      "grad_norm": 0.08974073827266693,
      "learning_rate": 0.00016894308943089432,
      "loss": 0.0055,
      "step": 192
    },
    {
      "epoch": 0.31382113821138213,
      "grad_norm": 0.04938000068068504,
      "learning_rate": 0.00016878048780487805,
      "loss": 0.0061,
      "step": 193
    },
    {
      "epoch": 0.3154471544715447,
      "grad_norm": 0.07383699715137482,
      "learning_rate": 0.0001686178861788618,
      "loss": 0.0083,
      "step": 194
    },
    {
      "epoch": 0.3170731707317073,
      "grad_norm": 0.03413151949644089,
      "learning_rate": 0.00016845528455284553,
      "loss": 0.0024,
      "step": 195
    },
    {
      "epoch": 0.31869918699186994,
      "grad_norm": 0.07202206552028656,
      "learning_rate": 0.00016829268292682927,
      "loss": 0.0079,
      "step": 196
    },
    {
      "epoch": 0.3203252032520325,
      "grad_norm": 0.06604200601577759,
      "learning_rate": 0.00016813008130081304,
      "loss": 0.0072,
      "step": 197
    },
    {
      "epoch": 0.32195121951219513,
      "grad_norm": 0.042259104549884796,
      "learning_rate": 0.00016796747967479675,
      "loss": 0.0032,
      "step": 198
    },
    {
      "epoch": 0.3235772357723577,
      "grad_norm": 0.03552434965968132,
      "learning_rate": 0.0001678048780487805,
      "loss": 0.002,
      "step": 199
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 0.031681936234235764,
      "learning_rate": 0.00016764227642276425,
      "loss": 0.0053,
      "step": 200
    },
    {
      "epoch": 0.32682926829268294,
      "grad_norm": 0.11483332514762878,
      "learning_rate": 0.00016747967479674797,
      "loss": 0.0144,
      "step": 201
    },
    {
      "epoch": 0.3284552845528455,
      "grad_norm": 0.04197550192475319,
      "learning_rate": 0.0001673170731707317,
      "loss": 0.0042,
      "step": 202
    },
    {
      "epoch": 0.3300813008130081,
      "grad_norm": 0.06192179024219513,
      "learning_rate": 0.00016715447154471547,
      "loss": 0.0078,
      "step": 203
    },
    {
      "epoch": 0.33170731707317075,
      "grad_norm": 0.05342299863696098,
      "learning_rate": 0.00016699186991869918,
      "loss": 0.007,
      "step": 204
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.04369426146149635,
      "learning_rate": 0.00016682926829268292,
      "loss": 0.0012,
      "step": 205
    },
    {
      "epoch": 0.33495934959349594,
      "grad_norm": 0.06268668919801712,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.0106,
      "step": 206
    },
    {
      "epoch": 0.33658536585365856,
      "grad_norm": 0.07172930985689163,
      "learning_rate": 0.00016650406504065043,
      "loss": 0.0055,
      "step": 207
    },
    {
      "epoch": 0.3382113821138211,
      "grad_norm": 0.06571360677480698,
      "learning_rate": 0.00016634146341463414,
      "loss": 0.0079,
      "step": 208
    },
    {
      "epoch": 0.33983739837398375,
      "grad_norm": 0.03303678333759308,
      "learning_rate": 0.0001661788617886179,
      "loss": 0.0024,
      "step": 209
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 0.02695191092789173,
      "learning_rate": 0.00016601626016260164,
      "loss": 0.0023,
      "step": 210
    },
    {
      "epoch": 0.34308943089430893,
      "grad_norm": 0.04864557832479477,
      "learning_rate": 0.00016585365853658536,
      "loss": 0.0049,
      "step": 211
    },
    {
      "epoch": 0.34471544715447155,
      "grad_norm": 0.036018937826156616,
      "learning_rate": 0.00016569105691056912,
      "loss": 0.0022,
      "step": 212
    },
    {
      "epoch": 0.3463414634146341,
      "grad_norm": 0.05603440850973129,
      "learning_rate": 0.00016552845528455286,
      "loss": 0.0017,
      "step": 213
    },
    {
      "epoch": 0.34796747967479674,
      "grad_norm": 0.032489605247974396,
      "learning_rate": 0.00016536585365853657,
      "loss": 0.0041,
      "step": 214
    },
    {
      "epoch": 0.34959349593495936,
      "grad_norm": 0.03976500406861305,
      "learning_rate": 0.00016520325203252034,
      "loss": 0.0042,
      "step": 215
    },
    {
      "epoch": 0.35121951219512193,
      "grad_norm": 0.0374918095767498,
      "learning_rate": 0.00016504065040650408,
      "loss": 0.0045,
      "step": 216
    },
    {
      "epoch": 0.35284552845528455,
      "grad_norm": 0.05623650550842285,
      "learning_rate": 0.00016487804878048782,
      "loss": 0.0028,
      "step": 217
    },
    {
      "epoch": 0.3544715447154472,
      "grad_norm": 0.07391159236431122,
      "learning_rate": 0.00016471544715447156,
      "loss": 0.011,
      "step": 218
    },
    {
      "epoch": 0.35609756097560974,
      "grad_norm": 0.04198432341217995,
      "learning_rate": 0.0001645528455284553,
      "loss": 0.0043,
      "step": 219
    },
    {
      "epoch": 0.35772357723577236,
      "grad_norm": 0.06713885068893433,
      "learning_rate": 0.00016439024390243903,
      "loss": 0.0065,
      "step": 220
    },
    {
      "epoch": 0.359349593495935,
      "grad_norm": 0.07107122987508774,
      "learning_rate": 0.00016422764227642277,
      "loss": 0.0108,
      "step": 221
    },
    {
      "epoch": 0.36097560975609755,
      "grad_norm": 0.039641816169023514,
      "learning_rate": 0.0001640650406504065,
      "loss": 0.0049,
      "step": 222
    },
    {
      "epoch": 0.36260162601626017,
      "grad_norm": 0.0785885602235794,
      "learning_rate": 0.00016390243902439025,
      "loss": 0.0139,
      "step": 223
    },
    {
      "epoch": 0.3642276422764228,
      "grad_norm": 0.05685996636748314,
      "learning_rate": 0.000163739837398374,
      "loss": 0.0045,
      "step": 224
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 0.042499840259552,
      "learning_rate": 0.00016357723577235773,
      "loss": 0.004,
      "step": 225
    },
    {
      "epoch": 0.367479674796748,
      "grad_norm": 0.03637044131755829,
      "learning_rate": 0.00016341463414634147,
      "loss": 0.0027,
      "step": 226
    },
    {
      "epoch": 0.36910569105691055,
      "grad_norm": 0.03910870477557182,
      "learning_rate": 0.0001632520325203252,
      "loss": 0.0044,
      "step": 227
    },
    {
      "epoch": 0.37073170731707317,
      "grad_norm": 0.06795119494199753,
      "learning_rate": 0.00016308943089430895,
      "loss": 0.0086,
      "step": 228
    },
    {
      "epoch": 0.3723577235772358,
      "grad_norm": 0.08367984741926193,
      "learning_rate": 0.00016292682926829268,
      "loss": 0.0124,
      "step": 229
    },
    {
      "epoch": 0.37398373983739835,
      "grad_norm": 0.03947695717215538,
      "learning_rate": 0.00016276422764227642,
      "loss": 0.0019,
      "step": 230
    },
    {
      "epoch": 0.375609756097561,
      "grad_norm": 0.05513155087828636,
      "learning_rate": 0.00016260162601626016,
      "loss": 0.0044,
      "step": 231
    },
    {
      "epoch": 0.3772357723577236,
      "grad_norm": 0.05772537365555763,
      "learning_rate": 0.00016243902439024393,
      "loss": 0.0065,
      "step": 232
    },
    {
      "epoch": 0.37886178861788616,
      "grad_norm": 0.0595276914536953,
      "learning_rate": 0.00016227642276422764,
      "loss": 0.0079,
      "step": 233
    },
    {
      "epoch": 0.3804878048780488,
      "grad_norm": 0.041617315262556076,
      "learning_rate": 0.00016211382113821138,
      "loss": 0.0056,
      "step": 234
    },
    {
      "epoch": 0.3821138211382114,
      "grad_norm": 0.008105425164103508,
      "learning_rate": 0.00016195121951219515,
      "loss": 0.0005,
      "step": 235
    },
    {
      "epoch": 0.383739837398374,
      "grad_norm": 0.024288572371006012,
      "learning_rate": 0.00016178861788617888,
      "loss": 0.0016,
      "step": 236
    },
    {
      "epoch": 0.3853658536585366,
      "grad_norm": 0.05020412430167198,
      "learning_rate": 0.0001616260162601626,
      "loss": 0.0032,
      "step": 237
    },
    {
      "epoch": 0.38699186991869916,
      "grad_norm": 0.040060855448246,
      "learning_rate": 0.00016146341463414636,
      "loss": 0.0033,
      "step": 238
    },
    {
      "epoch": 0.3886178861788618,
      "grad_norm": 0.03679739311337471,
      "learning_rate": 0.0001613008130081301,
      "loss": 0.0019,
      "step": 239
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 0.03246524557471275,
      "learning_rate": 0.0001611382113821138,
      "loss": 0.0017,
      "step": 240
    },
    {
      "epoch": 0.39186991869918697,
      "grad_norm": 0.053959108889102936,
      "learning_rate": 0.00016097560975609758,
      "loss": 0.0041,
      "step": 241
    },
    {
      "epoch": 0.3934959349593496,
      "grad_norm": 0.05499345809221268,
      "learning_rate": 0.00016081300813008132,
      "loss": 0.0082,
      "step": 242
    },
    {
      "epoch": 0.3951219512195122,
      "grad_norm": 0.09415078163146973,
      "learning_rate": 0.00016065040650406503,
      "loss": 0.0065,
      "step": 243
    },
    {
      "epoch": 0.3967479674796748,
      "grad_norm": 0.026768893003463745,
      "learning_rate": 0.0001604878048780488,
      "loss": 0.0021,
      "step": 244
    },
    {
      "epoch": 0.3983739837398374,
      "grad_norm": 0.059897903352975845,
      "learning_rate": 0.00016032520325203254,
      "loss": 0.0034,
      "step": 245
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.03809318691492081,
      "learning_rate": 0.00016016260162601627,
      "loss": 0.0011,
      "step": 246
    },
    {
      "epoch": 0.4016260162601626,
      "grad_norm": 0.04290259629487991,
      "learning_rate": 0.00016,
      "loss": 0.004,
      "step": 247
    },
    {
      "epoch": 0.4032520325203252,
      "grad_norm": 0.03837749734520912,
      "learning_rate": 0.00015983739837398375,
      "loss": 0.0038,
      "step": 248
    },
    {
      "epoch": 0.40487804878048783,
      "grad_norm": 0.03902510181069374,
      "learning_rate": 0.0001596747967479675,
      "loss": 0.0051,
      "step": 249
    },
    {
      "epoch": 0.4065040650406504,
      "grad_norm": 0.02119496837258339,
      "learning_rate": 0.00015951219512195123,
      "loss": 0.0016,
      "step": 250
    },
    {
      "epoch": 0.408130081300813,
      "grad_norm": 0.059587541967630386,
      "learning_rate": 0.00015934959349593497,
      "loss": 0.007,
      "step": 251
    },
    {
      "epoch": 0.4097560975609756,
      "grad_norm": 0.03809100016951561,
      "learning_rate": 0.0001591869918699187,
      "loss": 0.0028,
      "step": 252
    },
    {
      "epoch": 0.4113821138211382,
      "grad_norm": 0.008246686309576035,
      "learning_rate": 0.00015902439024390245,
      "loss": 0.0005,
      "step": 253
    },
    {
      "epoch": 0.41300813008130083,
      "grad_norm": 0.06121211126446724,
      "learning_rate": 0.00015886178861788619,
      "loss": 0.0074,
      "step": 254
    },
    {
      "epoch": 0.4146341463414634,
      "grad_norm": 0.03664132207632065,
      "learning_rate": 0.00015869918699186992,
      "loss": 0.0019,
      "step": 255
    },
    {
      "epoch": 0.416260162601626,
      "grad_norm": 0.09013177454471588,
      "learning_rate": 0.00015853658536585366,
      "loss": 0.0094,
      "step": 256
    },
    {
      "epoch": 0.41788617886178864,
      "grad_norm": 0.04787154495716095,
      "learning_rate": 0.0001583739837398374,
      "loss": 0.0088,
      "step": 257
    },
    {
      "epoch": 0.4195121951219512,
      "grad_norm": 0.04078535735607147,
      "learning_rate": 0.00015821138211382114,
      "loss": 0.0047,
      "step": 258
    },
    {
      "epoch": 0.4211382113821138,
      "grad_norm": 0.032567497342824936,
      "learning_rate": 0.00015804878048780488,
      "loss": 0.0022,
      "step": 259
    },
    {
      "epoch": 0.42276422764227645,
      "grad_norm": 0.056626107543706894,
      "learning_rate": 0.00015788617886178862,
      "loss": 0.0046,
      "step": 260
    },
    {
      "epoch": 0.424390243902439,
      "grad_norm": 0.03480905294418335,
      "learning_rate": 0.00015772357723577236,
      "loss": 0.0035,
      "step": 261
    },
    {
      "epoch": 0.42601626016260163,
      "grad_norm": 0.07597535103559494,
      "learning_rate": 0.00015756097560975612,
      "loss": 0.0107,
      "step": 262
    },
    {
      "epoch": 0.4276422764227642,
      "grad_norm": 0.06572912633419037,
      "learning_rate": 0.00015739837398373984,
      "loss": 0.0043,
      "step": 263
    },
    {
      "epoch": 0.4292682926829268,
      "grad_norm": 0.05102035775780678,
      "learning_rate": 0.00015723577235772358,
      "loss": 0.008,
      "step": 264
    },
    {
      "epoch": 0.43089430894308944,
      "grad_norm": 0.028546195477247238,
      "learning_rate": 0.00015707317073170734,
      "loss": 0.0018,
      "step": 265
    },
    {
      "epoch": 0.432520325203252,
      "grad_norm": 0.029671359807252884,
      "learning_rate": 0.00015691056910569105,
      "loss": 0.0026,
      "step": 266
    },
    {
      "epoch": 0.43414634146341463,
      "grad_norm": 0.06173054128885269,
      "learning_rate": 0.0001567479674796748,
      "loss": 0.0069,
      "step": 267
    },
    {
      "epoch": 0.43577235772357725,
      "grad_norm": 0.05086858198046684,
      "learning_rate": 0.00015658536585365856,
      "loss": 0.0063,
      "step": 268
    },
    {
      "epoch": 0.4373983739837398,
      "grad_norm": 0.02125905267894268,
      "learning_rate": 0.00015642276422764227,
      "loss": 0.0014,
      "step": 269
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 0.04446115717291832,
      "learning_rate": 0.000156260162601626,
      "loss": 0.0046,
      "step": 270
    },
    {
      "epoch": 0.44065040650406506,
      "grad_norm": 0.008476979099214077,
      "learning_rate": 0.00015609756097560978,
      "loss": 0.0005,
      "step": 271
    },
    {
      "epoch": 0.44227642276422763,
      "grad_norm": 0.052454475313425064,
      "learning_rate": 0.00015593495934959351,
      "loss": 0.0064,
      "step": 272
    },
    {
      "epoch": 0.44390243902439025,
      "grad_norm": 0.03743799030780792,
      "learning_rate": 0.00015577235772357723,
      "loss": 0.0012,
      "step": 273
    },
    {
      "epoch": 0.44552845528455287,
      "grad_norm": 0.04814663901925087,
      "learning_rate": 0.000155609756097561,
      "loss": 0.0046,
      "step": 274
    },
    {
      "epoch": 0.44715447154471544,
      "grad_norm": 0.09085031598806381,
      "learning_rate": 0.00015544715447154473,
      "loss": 0.0076,
      "step": 275
    },
    {
      "epoch": 0.44878048780487806,
      "grad_norm": 0.05821633338928223,
      "learning_rate": 0.00015528455284552844,
      "loss": 0.0049,
      "step": 276
    },
    {
      "epoch": 0.4504065040650406,
      "grad_norm": 0.02842341549694538,
      "learning_rate": 0.0001551219512195122,
      "loss": 0.0019,
      "step": 277
    },
    {
      "epoch": 0.45203252032520325,
      "grad_norm": 0.047390248626470566,
      "learning_rate": 0.00015495934959349595,
      "loss": 0.0017,
      "step": 278
    },
    {
      "epoch": 0.45365853658536587,
      "grad_norm": 0.03129230812191963,
      "learning_rate": 0.00015479674796747966,
      "loss": 0.0025,
      "step": 279
    },
    {
      "epoch": 0.45528455284552843,
      "grad_norm": 0.03706726059317589,
      "learning_rate": 0.00015463414634146343,
      "loss": 0.0025,
      "step": 280
    },
    {
      "epoch": 0.45691056910569106,
      "grad_norm": 0.02136380784213543,
      "learning_rate": 0.00015447154471544717,
      "loss": 0.0011,
      "step": 281
    },
    {
      "epoch": 0.4585365853658537,
      "grad_norm": 0.041168052703142166,
      "learning_rate": 0.0001543089430894309,
      "loss": 0.0032,
      "step": 282
    },
    {
      "epoch": 0.46016260162601624,
      "grad_norm": 0.06333127617835999,
      "learning_rate": 0.00015414634146341464,
      "loss": 0.006,
      "step": 283
    },
    {
      "epoch": 0.46178861788617886,
      "grad_norm": 0.04016856849193573,
      "learning_rate": 0.00015398373983739838,
      "loss": 0.0035,
      "step": 284
    },
    {
      "epoch": 0.4634146341463415,
      "grad_norm": 0.05800430849194527,
      "learning_rate": 0.00015382113821138212,
      "loss": 0.004,
      "step": 285
    },
    {
      "epoch": 0.46504065040650405,
      "grad_norm": 0.031819798052310944,
      "learning_rate": 0.00015365853658536586,
      "loss": 0.0007,
      "step": 286
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.06838291138410568,
      "learning_rate": 0.0001534959349593496,
      "loss": 0.0055,
      "step": 287
    },
    {
      "epoch": 0.4682926829268293,
      "grad_norm": 0.04758811369538307,
      "learning_rate": 0.00015333333333333334,
      "loss": 0.0039,
      "step": 288
    },
    {
      "epoch": 0.46991869918699186,
      "grad_norm": 0.042291514575481415,
      "learning_rate": 0.00015317073170731708,
      "loss": 0.0034,
      "step": 289
    },
    {
      "epoch": 0.4715447154471545,
      "grad_norm": 0.038803260773420334,
      "learning_rate": 0.00015300813008130082,
      "loss": 0.0026,
      "step": 290
    },
    {
      "epoch": 0.47317073170731705,
      "grad_norm": 0.036684513092041016,
      "learning_rate": 0.00015284552845528458,
      "loss": 0.0034,
      "step": 291
    },
    {
      "epoch": 0.47479674796747967,
      "grad_norm": 0.0919487252831459,
      "learning_rate": 0.0001526829268292683,
      "loss": 0.0118,
      "step": 292
    },
    {
      "epoch": 0.4764227642276423,
      "grad_norm": 0.018654925748705864,
      "learning_rate": 0.00015252032520325203,
      "loss": 0.0006,
      "step": 293
    },
    {
      "epoch": 0.47804878048780486,
      "grad_norm": 0.04378391429781914,
      "learning_rate": 0.0001523577235772358,
      "loss": 0.0028,
      "step": 294
    },
    {
      "epoch": 0.4796747967479675,
      "grad_norm": 0.021690063178539276,
      "learning_rate": 0.0001521951219512195,
      "loss": 0.0014,
      "step": 295
    },
    {
      "epoch": 0.4813008130081301,
      "grad_norm": 0.05425732582807541,
      "learning_rate": 0.00015203252032520325,
      "loss": 0.0046,
      "step": 296
    },
    {
      "epoch": 0.48292682926829267,
      "grad_norm": 0.09478655457496643,
      "learning_rate": 0.00015186991869918702,
      "loss": 0.0062,
      "step": 297
    },
    {
      "epoch": 0.4845528455284553,
      "grad_norm": 0.08030550926923752,
      "learning_rate": 0.00015170731707317073,
      "loss": 0.0071,
      "step": 298
    },
    {
      "epoch": 0.4861788617886179,
      "grad_norm": 0.09851697832345963,
      "learning_rate": 0.00015154471544715447,
      "loss": 0.0058,
      "step": 299
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 0.08546419441699982,
      "learning_rate": 0.00015138211382113823,
      "loss": 0.0097,
      "step": 300
    },
    {
      "epoch": 0.4894308943089431,
      "grad_norm": 0.02204154245555401,
      "learning_rate": 0.00015121951219512197,
      "loss": 0.0013,
      "step": 301
    },
    {
      "epoch": 0.49105691056910566,
      "grad_norm": 0.02301298826932907,
      "learning_rate": 0.00015105691056910568,
      "loss": 0.0012,
      "step": 302
    },
    {
      "epoch": 0.4926829268292683,
      "grad_norm": 0.012738016434013844,
      "learning_rate": 0.00015089430894308945,
      "loss": 0.0006,
      "step": 303
    },
    {
      "epoch": 0.4943089430894309,
      "grad_norm": 0.02279171161353588,
      "learning_rate": 0.0001507317073170732,
      "loss": 0.0013,
      "step": 304
    },
    {
      "epoch": 0.4959349593495935,
      "grad_norm": 0.05987592041492462,
      "learning_rate": 0.0001505691056910569,
      "loss": 0.0053,
      "step": 305
    },
    {
      "epoch": 0.4975609756097561,
      "grad_norm": 0.05977104604244232,
      "learning_rate": 0.00015040650406504067,
      "loss": 0.0025,
      "step": 306
    },
    {
      "epoch": 0.4991869918699187,
      "grad_norm": 0.04674098640680313,
      "learning_rate": 0.0001502439024390244,
      "loss": 0.0038,
      "step": 307
    },
    {
      "epoch": 0.5008130081300813,
      "grad_norm": 0.048203691840171814,
      "learning_rate": 0.00015008130081300812,
      "loss": 0.0023,
      "step": 308
    },
    {
      "epoch": 0.5024390243902439,
      "grad_norm": 0.02530977874994278,
      "learning_rate": 0.00014991869918699188,
      "loss": 0.0021,
      "step": 309
    },
    {
      "epoch": 0.5040650406504065,
      "grad_norm": 0.024972444400191307,
      "learning_rate": 0.00014975609756097562,
      "loss": 0.0013,
      "step": 310
    },
    {
      "epoch": 0.5056910569105691,
      "grad_norm": 0.004017941653728485,
      "learning_rate": 0.00014959349593495936,
      "loss": 0.0003,
      "step": 311
    },
    {
      "epoch": 0.5073170731707317,
      "grad_norm": 0.05017799884080887,
      "learning_rate": 0.0001494308943089431,
      "loss": 0.0045,
      "step": 312
    },
    {
      "epoch": 0.5089430894308943,
      "grad_norm": 0.05301947519183159,
      "learning_rate": 0.00014926829268292684,
      "loss": 0.0036,
      "step": 313
    },
    {
      "epoch": 0.510569105691057,
      "grad_norm": 0.05363637953996658,
      "learning_rate": 0.00014910569105691058,
      "loss": 0.0044,
      "step": 314
    },
    {
      "epoch": 0.5121951219512195,
      "grad_norm": 0.03636287897825241,
      "learning_rate": 0.00014894308943089432,
      "loss": 0.0022,
      "step": 315
    },
    {
      "epoch": 0.5138211382113821,
      "grad_norm": 0.028038306161761284,
      "learning_rate": 0.00014878048780487806,
      "loss": 0.0021,
      "step": 316
    },
    {
      "epoch": 0.5154471544715448,
      "grad_norm": 0.03675807639956474,
      "learning_rate": 0.0001486178861788618,
      "loss": 0.0007,
      "step": 317
    },
    {
      "epoch": 0.5170731707317073,
      "grad_norm": 0.05961129069328308,
      "learning_rate": 0.00014845528455284553,
      "loss": 0.0036,
      "step": 318
    },
    {
      "epoch": 0.5186991869918699,
      "grad_norm": 0.018406610935926437,
      "learning_rate": 0.00014829268292682927,
      "loss": 0.001,
      "step": 319
    },
    {
      "epoch": 0.5203252032520326,
      "grad_norm": 0.024211697280406952,
      "learning_rate": 0.000148130081300813,
      "loss": 0.0007,
      "step": 320
    },
    {
      "epoch": 0.5219512195121951,
      "grad_norm": 0.0223006010055542,
      "learning_rate": 0.00014796747967479675,
      "loss": 0.0009,
      "step": 321
    },
    {
      "epoch": 0.5235772357723577,
      "grad_norm": 0.025664936751127243,
      "learning_rate": 0.0001478048780487805,
      "loss": 0.0015,
      "step": 322
    },
    {
      "epoch": 0.5252032520325203,
      "grad_norm": 0.07756369560956955,
      "learning_rate": 0.00014764227642276423,
      "loss": 0.0026,
      "step": 323
    },
    {
      "epoch": 0.526829268292683,
      "grad_norm": 0.04779999703168869,
      "learning_rate": 0.00014747967479674797,
      "loss": 0.0024,
      "step": 324
    },
    {
      "epoch": 0.5284552845528455,
      "grad_norm": 0.0432940237224102,
      "learning_rate": 0.0001473170731707317,
      "loss": 0.0015,
      "step": 325
    },
    {
      "epoch": 0.5300813008130081,
      "grad_norm": 0.07308786362409592,
      "learning_rate": 0.00014715447154471545,
      "loss": 0.0083,
      "step": 326
    },
    {
      "epoch": 0.5317073170731708,
      "grad_norm": 0.08420983701944351,
      "learning_rate": 0.0001469918699186992,
      "loss": 0.0061,
      "step": 327
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.05637168511748314,
      "learning_rate": 0.00014682926829268292,
      "loss": 0.0037,
      "step": 328
    },
    {
      "epoch": 0.5349593495934959,
      "grad_norm": 0.011459868401288986,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.0004,
      "step": 329
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 0.04229405149817467,
      "learning_rate": 0.00014650406504065043,
      "loss": 0.0052,
      "step": 330
    },
    {
      "epoch": 0.5382113821138211,
      "grad_norm": 0.08318997919559479,
      "learning_rate": 0.00014634146341463414,
      "loss": 0.012,
      "step": 331
    },
    {
      "epoch": 0.5398373983739837,
      "grad_norm": 0.027448266744613647,
      "learning_rate": 0.00014617886178861788,
      "loss": 0.001,
      "step": 332
    },
    {
      "epoch": 0.5414634146341464,
      "grad_norm": 0.03405987471342087,
      "learning_rate": 0.00014601626016260165,
      "loss": 0.002,
      "step": 333
    },
    {
      "epoch": 0.5430894308943089,
      "grad_norm": 0.032117780297994614,
      "learning_rate": 0.00014585365853658536,
      "loss": 0.0005,
      "step": 334
    },
    {
      "epoch": 0.5447154471544715,
      "grad_norm": 0.022048333659768105,
      "learning_rate": 0.00014569105691056912,
      "loss": 0.0005,
      "step": 335
    },
    {
      "epoch": 0.5463414634146342,
      "grad_norm": 0.043050795793533325,
      "learning_rate": 0.00014552845528455286,
      "loss": 0.0036,
      "step": 336
    },
    {
      "epoch": 0.5479674796747968,
      "grad_norm": 0.02210851013660431,
      "learning_rate": 0.0001453658536585366,
      "loss": 0.0011,
      "step": 337
    },
    {
      "epoch": 0.5495934959349593,
      "grad_norm": 0.08371196687221527,
      "learning_rate": 0.00014520325203252034,
      "loss": 0.0108,
      "step": 338
    },
    {
      "epoch": 0.551219512195122,
      "grad_norm": 0.03730907291173935,
      "learning_rate": 0.00014504065040650408,
      "loss": 0.0023,
      "step": 339
    },
    {
      "epoch": 0.5528455284552846,
      "grad_norm": 0.04696402698755264,
      "learning_rate": 0.00014487804878048782,
      "loss": 0.0033,
      "step": 340
    },
    {
      "epoch": 0.5544715447154471,
      "grad_norm": 0.01265543233603239,
      "learning_rate": 0.00014471544715447156,
      "loss": 0.0005,
      "step": 341
    },
    {
      "epoch": 0.5560975609756098,
      "grad_norm": 0.02745240554213524,
      "learning_rate": 0.0001445528455284553,
      "loss": 0.0026,
      "step": 342
    },
    {
      "epoch": 0.5577235772357724,
      "grad_norm": 0.04942648112773895,
      "learning_rate": 0.00014439024390243904,
      "loss": 0.0037,
      "step": 343
    },
    {
      "epoch": 0.5593495934959349,
      "grad_norm": 0.008476071991026402,
      "learning_rate": 0.00014422764227642277,
      "loss": 0.0004,
      "step": 344
    },
    {
      "epoch": 0.5609756097560976,
      "grad_norm": 0.050052739679813385,
      "learning_rate": 0.0001440650406504065,
      "loss": 0.0032,
      "step": 345
    },
    {
      "epoch": 0.5626016260162602,
      "grad_norm": 0.05567610263824463,
      "learning_rate": 0.00014390243902439025,
      "loss": 0.0049,
      "step": 346
    },
    {
      "epoch": 0.5642276422764227,
      "grad_norm": 0.04730698838829994,
      "learning_rate": 0.000143739837398374,
      "loss": 0.001,
      "step": 347
    },
    {
      "epoch": 0.5658536585365853,
      "grad_norm": 0.06813085824251175,
      "learning_rate": 0.00014357723577235773,
      "loss": 0.0065,
      "step": 348
    },
    {
      "epoch": 0.567479674796748,
      "grad_norm": 0.027026068419218063,
      "learning_rate": 0.00014341463414634147,
      "loss": 0.002,
      "step": 349
    },
    {
      "epoch": 0.5691056910569106,
      "grad_norm": 0.008108194917440414,
      "learning_rate": 0.0001432520325203252,
      "loss": 0.0003,
      "step": 350
    },
    {
      "epoch": 0.5707317073170731,
      "grad_norm": 0.021709604188799858,
      "learning_rate": 0.00014308943089430895,
      "loss": 0.0004,
      "step": 351
    },
    {
      "epoch": 0.5723577235772358,
      "grad_norm": 0.04537516087293625,
      "learning_rate": 0.00014292682926829269,
      "loss": 0.0016,
      "step": 352
    },
    {
      "epoch": 0.5739837398373984,
      "grad_norm": 0.06790541112422943,
      "learning_rate": 0.00014276422764227643,
      "loss": 0.0039,
      "step": 353
    },
    {
      "epoch": 0.5756097560975609,
      "grad_norm": 0.05915749445557594,
      "learning_rate": 0.00014260162601626016,
      "loss": 0.0066,
      "step": 354
    },
    {
      "epoch": 0.5772357723577236,
      "grad_norm": 0.05809753015637398,
      "learning_rate": 0.0001424390243902439,
      "loss": 0.0055,
      "step": 355
    },
    {
      "epoch": 0.5788617886178862,
      "grad_norm": 0.06579986214637756,
      "learning_rate": 0.00014227642276422767,
      "loss": 0.0035,
      "step": 356
    },
    {
      "epoch": 0.5804878048780487,
      "grad_norm": 0.019875263795256615,
      "learning_rate": 0.00014211382113821138,
      "loss": 0.0011,
      "step": 357
    },
    {
      "epoch": 0.5821138211382114,
      "grad_norm": 0.029589928686618805,
      "learning_rate": 0.00014195121951219512,
      "loss": 0.0018,
      "step": 358
    },
    {
      "epoch": 0.583739837398374,
      "grad_norm": 0.0400996096432209,
      "learning_rate": 0.00014178861788617889,
      "loss": 0.0021,
      "step": 359
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 0.05294010043144226,
      "learning_rate": 0.0001416260162601626,
      "loss": 0.0025,
      "step": 360
    },
    {
      "epoch": 0.5869918699186992,
      "grad_norm": 0.049299150705337524,
      "learning_rate": 0.00014146341463414634,
      "loss": 0.0026,
      "step": 361
    },
    {
      "epoch": 0.5886178861788618,
      "grad_norm": 0.04363460838794708,
      "learning_rate": 0.0001413008130081301,
      "loss": 0.0023,
      "step": 362
    },
    {
      "epoch": 0.5902439024390244,
      "grad_norm": 0.030172506347298622,
      "learning_rate": 0.00014113821138211381,
      "loss": 0.0019,
      "step": 363
    },
    {
      "epoch": 0.591869918699187,
      "grad_norm": 0.02734779193997383,
      "learning_rate": 0.00014097560975609755,
      "loss": 0.0007,
      "step": 364
    },
    {
      "epoch": 0.5934959349593496,
      "grad_norm": 0.03403576835989952,
      "learning_rate": 0.00014081300813008132,
      "loss": 0.0009,
      "step": 365
    },
    {
      "epoch": 0.5951219512195122,
      "grad_norm": 0.011000935919582844,
      "learning_rate": 0.00014065040650406506,
      "loss": 0.0003,
      "step": 366
    },
    {
      "epoch": 0.5967479674796748,
      "grad_norm": 0.02606494165956974,
      "learning_rate": 0.00014048780487804877,
      "loss": 0.001,
      "step": 367
    },
    {
      "epoch": 0.5983739837398374,
      "grad_norm": 0.02679901011288166,
      "learning_rate": 0.00014032520325203254,
      "loss": 0.0018,
      "step": 368
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.03595108911395073,
      "learning_rate": 0.00014016260162601628,
      "loss": 0.0018,
      "step": 369
    },
    {
      "epoch": 0.6016260162601627,
      "grad_norm": 0.05407336726784706,
      "learning_rate": 0.00014,
      "loss": 0.0026,
      "step": 370
    },
    {
      "epoch": 0.6032520325203252,
      "grad_norm": 0.030202345922589302,
      "learning_rate": 0.00013983739837398375,
      "loss": 0.0015,
      "step": 371
    },
    {
      "epoch": 0.6048780487804878,
      "grad_norm": 0.08269347995519638,
      "learning_rate": 0.0001396747967479675,
      "loss": 0.0057,
      "step": 372
    },
    {
      "epoch": 0.6065040650406504,
      "grad_norm": 0.015995895490050316,
      "learning_rate": 0.0001395121951219512,
      "loss": 0.0008,
      "step": 373
    },
    {
      "epoch": 0.608130081300813,
      "grad_norm": 0.07359179109334946,
      "learning_rate": 0.00013934959349593497,
      "loss": 0.0061,
      "step": 374
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 0.06993608921766281,
      "learning_rate": 0.0001391869918699187,
      "loss": 0.0076,
      "step": 375
    },
    {
      "epoch": 0.6113821138211382,
      "grad_norm": 0.043971456587314606,
      "learning_rate": 0.00013902439024390245,
      "loss": 0.0031,
      "step": 376
    },
    {
      "epoch": 0.6130081300813008,
      "grad_norm": 0.035905253142118454,
      "learning_rate": 0.0001388617886178862,
      "loss": 0.0015,
      "step": 377
    },
    {
      "epoch": 0.6146341463414634,
      "grad_norm": 0.04560420289635658,
      "learning_rate": 0.00013869918699186993,
      "loss": 0.0008,
      "step": 378
    },
    {
      "epoch": 0.616260162601626,
      "grad_norm": 0.009953766129910946,
      "learning_rate": 0.00013853658536585367,
      "loss": 0.0003,
      "step": 379
    },
    {
      "epoch": 0.6178861788617886,
      "grad_norm": 0.03815978392958641,
      "learning_rate": 0.0001383739837398374,
      "loss": 0.002,
      "step": 380
    },
    {
      "epoch": 0.6195121951219512,
      "grad_norm": 0.008472801186144352,
      "learning_rate": 0.00013821138211382114,
      "loss": 0.0004,
      "step": 381
    },
    {
      "epoch": 0.6211382113821138,
      "grad_norm": 0.06264209747314453,
      "learning_rate": 0.00013804878048780488,
      "loss": 0.006,
      "step": 382
    },
    {
      "epoch": 0.6227642276422765,
      "grad_norm": 0.006059740204364061,
      "learning_rate": 0.00013788617886178862,
      "loss": 0.0003,
      "step": 383
    },
    {
      "epoch": 0.624390243902439,
      "grad_norm": 0.04096158221364021,
      "learning_rate": 0.00013772357723577236,
      "loss": 0.0026,
      "step": 384
    },
    {
      "epoch": 0.6260162601626016,
      "grad_norm": 0.06331557035446167,
      "learning_rate": 0.0001375609756097561,
      "loss": 0.0074,
      "step": 385
    },
    {
      "epoch": 0.6276422764227643,
      "grad_norm": 0.05059782788157463,
      "learning_rate": 0.00013739837398373984,
      "loss": 0.0045,
      "step": 386
    },
    {
      "epoch": 0.6292682926829268,
      "grad_norm": 0.04114658758044243,
      "learning_rate": 0.00013723577235772358,
      "loss": 0.0015,
      "step": 387
    },
    {
      "epoch": 0.6308943089430894,
      "grad_norm": 0.04135163128376007,
      "learning_rate": 0.00013707317073170734,
      "loss": 0.0042,
      "step": 388
    },
    {
      "epoch": 0.6325203252032521,
      "grad_norm": 0.05520135536789894,
      "learning_rate": 0.00013691056910569106,
      "loss": 0.0055,
      "step": 389
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 0.021615512669086456,
      "learning_rate": 0.0001367479674796748,
      "loss": 0.0009,
      "step": 390
    },
    {
      "epoch": 0.6357723577235772,
      "grad_norm": 0.04998411238193512,
      "learning_rate": 0.00013658536585365856,
      "loss": 0.004,
      "step": 391
    },
    {
      "epoch": 0.6373983739837399,
      "grad_norm": 0.02765563502907753,
      "learning_rate": 0.00013642276422764227,
      "loss": 0.0015,
      "step": 392
    },
    {
      "epoch": 0.6390243902439025,
      "grad_norm": 0.027994269505143166,
      "learning_rate": 0.000136260162601626,
      "loss": 0.0015,
      "step": 393
    },
    {
      "epoch": 0.640650406504065,
      "grad_norm": 0.025956694036722183,
      "learning_rate": 0.00013609756097560978,
      "loss": 0.0015,
      "step": 394
    },
    {
      "epoch": 0.6422764227642277,
      "grad_norm": 0.045004431158304214,
      "learning_rate": 0.00013593495934959352,
      "loss": 0.0029,
      "step": 395
    },
    {
      "epoch": 0.6439024390243903,
      "grad_norm": 0.012444769963622093,
      "learning_rate": 0.00013577235772357723,
      "loss": 0.0006,
      "step": 396
    },
    {
      "epoch": 0.6455284552845528,
      "grad_norm": 0.007664056029170752,
      "learning_rate": 0.000135609756097561,
      "loss": 0.0003,
      "step": 397
    },
    {
      "epoch": 0.6471544715447154,
      "grad_norm": 0.07797913253307343,
      "learning_rate": 0.00013544715447154473,
      "loss": 0.0054,
      "step": 398
    },
    {
      "epoch": 0.6487804878048781,
      "grad_norm": 0.042493656277656555,
      "learning_rate": 0.00013528455284552844,
      "loss": 0.0017,
      "step": 399
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 0.06429119408130646,
      "learning_rate": 0.0001351219512195122,
      "loss": 0.0046,
      "step": 400
    },
    {
      "epoch": 0.6520325203252032,
      "grad_norm": 0.062457289546728134,
      "learning_rate": 0.00013495934959349595,
      "loss": 0.0039,
      "step": 401
    },
    {
      "epoch": 0.6536585365853659,
      "grad_norm": 0.043257568031549454,
      "learning_rate": 0.00013479674796747966,
      "loss": 0.0037,
      "step": 402
    },
    {
      "epoch": 0.6552845528455284,
      "grad_norm": 0.03842322528362274,
      "learning_rate": 0.00013463414634146343,
      "loss": 0.0016,
      "step": 403
    },
    {
      "epoch": 0.656910569105691,
      "grad_norm": 0.03916073963046074,
      "learning_rate": 0.00013447154471544717,
      "loss": 0.0013,
      "step": 404
    },
    {
      "epoch": 0.6585365853658537,
      "grad_norm": 0.024477075785398483,
      "learning_rate": 0.0001343089430894309,
      "loss": 0.0011,
      "step": 405
    },
    {
      "epoch": 0.6601626016260163,
      "grad_norm": 0.045938216149806976,
      "learning_rate": 0.00013414634146341464,
      "loss": 0.003,
      "step": 406
    },
    {
      "epoch": 0.6617886178861788,
      "grad_norm": 0.03394032269716263,
      "learning_rate": 0.00013398373983739838,
      "loss": 0.001,
      "step": 407
    },
    {
      "epoch": 0.6634146341463415,
      "grad_norm": 0.0203282181173563,
      "learning_rate": 0.00013382113821138212,
      "loss": 0.0009,
      "step": 408
    },
    {
      "epoch": 0.6650406504065041,
      "grad_norm": 0.014730355702340603,
      "learning_rate": 0.00013365853658536586,
      "loss": 0.0007,
      "step": 409
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.005610968917608261,
      "learning_rate": 0.0001334959349593496,
      "loss": 0.0003,
      "step": 410
    },
    {
      "epoch": 0.6682926829268293,
      "grad_norm": 0.05725783109664917,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.0043,
      "step": 411
    },
    {
      "epoch": 0.6699186991869919,
      "grad_norm": 0.00306798261590302,
      "learning_rate": 0.00013317073170731708,
      "loss": 0.0002,
      "step": 412
    },
    {
      "epoch": 0.6715447154471544,
      "grad_norm": 0.04567965492606163,
      "learning_rate": 0.00013300813008130082,
      "loss": 0.0025,
      "step": 413
    },
    {
      "epoch": 0.6731707317073171,
      "grad_norm": 0.0034624331165105104,
      "learning_rate": 0.00013284552845528456,
      "loss": 0.0002,
      "step": 414
    },
    {
      "epoch": 0.6747967479674797,
      "grad_norm": 0.02753608301281929,
      "learning_rate": 0.0001326829268292683,
      "loss": 0.0011,
      "step": 415
    },
    {
      "epoch": 0.6764227642276422,
      "grad_norm": 0.07352620363235474,
      "learning_rate": 0.00013252032520325203,
      "loss": 0.0049,
      "step": 416
    },
    {
      "epoch": 0.6780487804878049,
      "grad_norm": 0.022383153438568115,
      "learning_rate": 0.00013235772357723577,
      "loss": 0.0007,
      "step": 417
    },
    {
      "epoch": 0.6796747967479675,
      "grad_norm": 0.030799895524978638,
      "learning_rate": 0.0001321951219512195,
      "loss": 0.0021,
      "step": 418
    },
    {
      "epoch": 0.6813008130081301,
      "grad_norm": 0.03935651853680611,
      "learning_rate": 0.00013203252032520325,
      "loss": 0.0021,
      "step": 419
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 0.021667126566171646,
      "learning_rate": 0.000131869918699187,
      "loss": 0.0004,
      "step": 420
    },
    {
      "epoch": 0.6845528455284553,
      "grad_norm": 0.0378342904150486,
      "learning_rate": 0.00013170731707317076,
      "loss": 0.0023,
      "step": 421
    },
    {
      "epoch": 0.6861788617886179,
      "grad_norm": 0.05022097006440163,
      "learning_rate": 0.00013154471544715447,
      "loss": 0.0035,
      "step": 422
    },
    {
      "epoch": 0.6878048780487804,
      "grad_norm": 0.044789813458919525,
      "learning_rate": 0.0001313821138211382,
      "loss": 0.0033,
      "step": 423
    },
    {
      "epoch": 0.6894308943089431,
      "grad_norm": 0.01575024425983429,
      "learning_rate": 0.00013121951219512197,
      "loss": 0.0007,
      "step": 424
    },
    {
      "epoch": 0.6910569105691057,
      "grad_norm": 0.0169359277933836,
      "learning_rate": 0.00013105691056910569,
      "loss": 0.0006,
      "step": 425
    },
    {
      "epoch": 0.6926829268292682,
      "grad_norm": 0.028870290145277977,
      "learning_rate": 0.00013089430894308942,
      "loss": 0.0015,
      "step": 426
    },
    {
      "epoch": 0.6943089430894309,
      "grad_norm": 0.006265566218644381,
      "learning_rate": 0.0001307317073170732,
      "loss": 0.0002,
      "step": 427
    },
    {
      "epoch": 0.6959349593495935,
      "grad_norm": 0.0681835189461708,
      "learning_rate": 0.0001305691056910569,
      "loss": 0.0038,
      "step": 428
    },
    {
      "epoch": 0.697560975609756,
      "grad_norm": 0.04927810654044151,
      "learning_rate": 0.00013040650406504064,
      "loss": 0.0056,
      "step": 429
    },
    {
      "epoch": 0.6991869918699187,
      "grad_norm": 0.05472636595368385,
      "learning_rate": 0.0001302439024390244,
      "loss": 0.0049,
      "step": 430
    },
    {
      "epoch": 0.7008130081300813,
      "grad_norm": 0.03326030075550079,
      "learning_rate": 0.00013008130081300815,
      "loss": 0.0011,
      "step": 431
    },
    {
      "epoch": 0.7024390243902439,
      "grad_norm": 0.009500794112682343,
      "learning_rate": 0.00012991869918699186,
      "loss": 0.0004,
      "step": 432
    },
    {
      "epoch": 0.7040650406504065,
      "grad_norm": 0.015952110290527344,
      "learning_rate": 0.00012975609756097562,
      "loss": 0.0006,
      "step": 433
    },
    {
      "epoch": 0.7056910569105691,
      "grad_norm": 0.0048037501983344555,
      "learning_rate": 0.00012959349593495936,
      "loss": 0.0002,
      "step": 434
    },
    {
      "epoch": 0.7073170731707317,
      "grad_norm": 0.013434546068310738,
      "learning_rate": 0.00012943089430894308,
      "loss": 0.0005,
      "step": 435
    },
    {
      "epoch": 0.7089430894308943,
      "grad_norm": 0.07665044814348221,
      "learning_rate": 0.00012926829268292684,
      "loss": 0.0096,
      "step": 436
    },
    {
      "epoch": 0.7105691056910569,
      "grad_norm": 0.019465599209070206,
      "learning_rate": 0.00012910569105691058,
      "loss": 0.0006,
      "step": 437
    },
    {
      "epoch": 0.7121951219512195,
      "grad_norm": 0.05842122808098793,
      "learning_rate": 0.00012894308943089432,
      "loss": 0.0051,
      "step": 438
    },
    {
      "epoch": 0.7138211382113822,
      "grad_norm": 0.03476549685001373,
      "learning_rate": 0.00012878048780487806,
      "loss": 0.0015,
      "step": 439
    },
    {
      "epoch": 0.7154471544715447,
      "grad_norm": 0.038131747394800186,
      "learning_rate": 0.0001286178861788618,
      "loss": 0.0013,
      "step": 440
    },
    {
      "epoch": 0.7170731707317073,
      "grad_norm": 0.04565577954053879,
      "learning_rate": 0.00012845528455284554,
      "loss": 0.0036,
      "step": 441
    },
    {
      "epoch": 0.71869918699187,
      "grad_norm": 0.07976550608873367,
      "learning_rate": 0.00012829268292682927,
      "loss": 0.0088,
      "step": 442
    },
    {
      "epoch": 0.7203252032520325,
      "grad_norm": 0.024210834875702858,
      "learning_rate": 0.00012813008130081301,
      "loss": 0.0007,
      "step": 443
    },
    {
      "epoch": 0.7219512195121951,
      "grad_norm": 0.04592128470540047,
      "learning_rate": 0.00012796747967479675,
      "loss": 0.0015,
      "step": 444
    },
    {
      "epoch": 0.7235772357723578,
      "grad_norm": 0.020742984488606453,
      "learning_rate": 0.0001278048780487805,
      "loss": 0.0013,
      "step": 445
    },
    {
      "epoch": 0.7252032520325203,
      "grad_norm": 0.01040639914572239,
      "learning_rate": 0.00012764227642276423,
      "loss": 0.0003,
      "step": 446
    },
    {
      "epoch": 0.7268292682926829,
      "grad_norm": 0.03810832276940346,
      "learning_rate": 0.00012747967479674797,
      "loss": 0.0023,
      "step": 447
    },
    {
      "epoch": 0.7284552845528456,
      "grad_norm": 0.0026015646290034056,
      "learning_rate": 0.0001273170731707317,
      "loss": 0.0002,
      "step": 448
    },
    {
      "epoch": 0.7300813008130081,
      "grad_norm": 0.02090383879840374,
      "learning_rate": 0.00012715447154471545,
      "loss": 0.0007,
      "step": 449
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 0.030612654983997345,
      "learning_rate": 0.00012699186991869921,
      "loss": 0.0016,
      "step": 450
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.02979256771504879,
      "learning_rate": 0.00012682926829268293,
      "loss": 0.0022,
      "step": 451
    },
    {
      "epoch": 0.734959349593496,
      "grad_norm": 0.016268040984869003,
      "learning_rate": 0.00012666666666666666,
      "loss": 0.0003,
      "step": 452
    },
    {
      "epoch": 0.7365853658536585,
      "grad_norm": 0.04419322684407234,
      "learning_rate": 0.00012650406504065043,
      "loss": 0.0022,
      "step": 453
    },
    {
      "epoch": 0.7382113821138211,
      "grad_norm": 0.07264306396245956,
      "learning_rate": 0.00012634146341463414,
      "loss": 0.0052,
      "step": 454
    },
    {
      "epoch": 0.7398373983739838,
      "grad_norm": 0.03466745465993881,
      "learning_rate": 0.00012617886178861788,
      "loss": 0.0021,
      "step": 455
    },
    {
      "epoch": 0.7414634146341463,
      "grad_norm": 0.08261137455701828,
      "learning_rate": 0.00012601626016260165,
      "loss": 0.0048,
      "step": 456
    },
    {
      "epoch": 0.7430894308943089,
      "grad_norm": 0.03311735764145851,
      "learning_rate": 0.00012585365853658536,
      "loss": 0.002,
      "step": 457
    },
    {
      "epoch": 0.7447154471544716,
      "grad_norm": 0.027840545400977135,
      "learning_rate": 0.0001256910569105691,
      "loss": 0.0012,
      "step": 458
    },
    {
      "epoch": 0.7463414634146341,
      "grad_norm": 0.0727102980017662,
      "learning_rate": 0.00012552845528455286,
      "loss": 0.0028,
      "step": 459
    },
    {
      "epoch": 0.7479674796747967,
      "grad_norm": 0.10644757747650146,
      "learning_rate": 0.0001253658536585366,
      "loss": 0.0086,
      "step": 460
    },
    {
      "epoch": 0.7495934959349594,
      "grad_norm": 0.03786543011665344,
      "learning_rate": 0.00012520325203252032,
      "loss": 0.0019,
      "step": 461
    },
    {
      "epoch": 0.751219512195122,
      "grad_norm": 0.048276159912347794,
      "learning_rate": 0.00012504065040650408,
      "loss": 0.0033,
      "step": 462
    },
    {
      "epoch": 0.7528455284552845,
      "grad_norm": 0.01945447362959385,
      "learning_rate": 0.00012487804878048782,
      "loss": 0.0005,
      "step": 463
    },
    {
      "epoch": 0.7544715447154472,
      "grad_norm": 0.030971255153417587,
      "learning_rate": 0.00012471544715447153,
      "loss": 0.0019,
      "step": 464
    },
    {
      "epoch": 0.7560975609756098,
      "grad_norm": 0.0476507730782032,
      "learning_rate": 0.0001245528455284553,
      "loss": 0.0041,
      "step": 465
    },
    {
      "epoch": 0.7577235772357723,
      "grad_norm": 0.04508371651172638,
      "learning_rate": 0.00012439024390243904,
      "loss": 0.0027,
      "step": 466
    },
    {
      "epoch": 0.759349593495935,
      "grad_norm": 0.02546669915318489,
      "learning_rate": 0.00012422764227642275,
      "loss": 0.0013,
      "step": 467
    },
    {
      "epoch": 0.7609756097560976,
      "grad_norm": 0.02323518693447113,
      "learning_rate": 0.00012406504065040652,
      "loss": 0.0009,
      "step": 468
    },
    {
      "epoch": 0.7626016260162601,
      "grad_norm": 0.03841609135270119,
      "learning_rate": 0.00012390243902439025,
      "loss": 0.0023,
      "step": 469
    },
    {
      "epoch": 0.7642276422764228,
      "grad_norm": 0.07417786866426468,
      "learning_rate": 0.000123739837398374,
      "loss": 0.0028,
      "step": 470
    },
    {
      "epoch": 0.7658536585365854,
      "grad_norm": 0.07709995657205582,
      "learning_rate": 0.00012357723577235773,
      "loss": 0.0079,
      "step": 471
    },
    {
      "epoch": 0.767479674796748,
      "grad_norm": 0.03812519833445549,
      "learning_rate": 0.00012341463414634147,
      "loss": 0.0023,
      "step": 472
    },
    {
      "epoch": 0.7691056910569106,
      "grad_norm": 0.030005669221282005,
      "learning_rate": 0.0001232520325203252,
      "loss": 0.0015,
      "step": 473
    },
    {
      "epoch": 0.7707317073170732,
      "grad_norm": 0.06345708668231964,
      "learning_rate": 0.00012308943089430895,
      "loss": 0.0016,
      "step": 474
    },
    {
      "epoch": 0.7723577235772358,
      "grad_norm": 0.0031643323600292206,
      "learning_rate": 0.0001229268292682927,
      "loss": 0.0002,
      "step": 475
    },
    {
      "epoch": 0.7739837398373983,
      "grad_norm": 0.005016733426600695,
      "learning_rate": 0.00012276422764227643,
      "loss": 0.0003,
      "step": 476
    },
    {
      "epoch": 0.775609756097561,
      "grad_norm": 0.05273330956697464,
      "learning_rate": 0.00012260162601626017,
      "loss": 0.0028,
      "step": 477
    },
    {
      "epoch": 0.7772357723577236,
      "grad_norm": 0.01227838546037674,
      "learning_rate": 0.0001224390243902439,
      "loss": 0.0005,
      "step": 478
    },
    {
      "epoch": 0.7788617886178861,
      "grad_norm": 0.05786914750933647,
      "learning_rate": 0.00012227642276422764,
      "loss": 0.0024,
      "step": 479
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 0.0029559931717813015,
      "learning_rate": 0.00012211382113821138,
      "loss": 0.0002,
      "step": 480
    },
    {
      "epoch": 0.7821138211382114,
      "grad_norm": 0.021030550822615623,
      "learning_rate": 0.00012195121951219512,
      "loss": 0.0003,
      "step": 481
    },
    {
      "epoch": 0.7837398373983739,
      "grad_norm": 0.10761956870555878,
      "learning_rate": 0.00012178861788617886,
      "loss": 0.0055,
      "step": 482
    },
    {
      "epoch": 0.7853658536585366,
      "grad_norm": 0.04578990116715431,
      "learning_rate": 0.00012162601626016261,
      "loss": 0.0031,
      "step": 483
    },
    {
      "epoch": 0.7869918699186992,
      "grad_norm": 0.07672619074583054,
      "learning_rate": 0.00012146341463414634,
      "loss": 0.0078,
      "step": 484
    },
    {
      "epoch": 0.7886178861788617,
      "grad_norm": 0.040395379066467285,
      "learning_rate": 0.00012130081300813008,
      "loss": 0.0013,
      "step": 485
    },
    {
      "epoch": 0.7902439024390244,
      "grad_norm": 0.0027492314111441374,
      "learning_rate": 0.00012113821138211383,
      "loss": 0.0002,
      "step": 486
    },
    {
      "epoch": 0.791869918699187,
      "grad_norm": 0.020994307473301888,
      "learning_rate": 0.00012097560975609757,
      "loss": 0.0007,
      "step": 487
    },
    {
      "epoch": 0.7934959349593496,
      "grad_norm": 0.030907196924090385,
      "learning_rate": 0.00012081300813008132,
      "loss": 0.0008,
      "step": 488
    },
    {
      "epoch": 0.7951219512195122,
      "grad_norm": 0.06392280012369156,
      "learning_rate": 0.00012065040650406505,
      "loss": 0.0048,
      "step": 489
    },
    {
      "epoch": 0.7967479674796748,
      "grad_norm": 0.04470368102192879,
      "learning_rate": 0.00012048780487804879,
      "loss": 0.0012,
      "step": 490
    },
    {
      "epoch": 0.7983739837398374,
      "grad_norm": 0.04805390164256096,
      "learning_rate": 0.00012032520325203254,
      "loss": 0.0028,
      "step": 491
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.05485748127102852,
      "learning_rate": 0.00012016260162601626,
      "loss": 0.0059,
      "step": 492
    },
    {
      "epoch": 0.8016260162601626,
      "grad_norm": 0.00901495385915041,
      "learning_rate": 0.00012,
      "loss": 0.0003,
      "step": 493
    },
    {
      "epoch": 0.8032520325203252,
      "grad_norm": 0.033151764422655106,
      "learning_rate": 0.00011983739837398376,
      "loss": 0.0015,
      "step": 494
    },
    {
      "epoch": 0.8048780487804879,
      "grad_norm": 0.0457472987473011,
      "learning_rate": 0.0001196747967479675,
      "loss": 0.0015,
      "step": 495
    },
    {
      "epoch": 0.8065040650406504,
      "grad_norm": 0.026971641927957535,
      "learning_rate": 0.00011951219512195122,
      "loss": 0.0008,
      "step": 496
    },
    {
      "epoch": 0.808130081300813,
      "grad_norm": 0.0381942093372345,
      "learning_rate": 0.00011934959349593497,
      "loss": 0.0022,
      "step": 497
    },
    {
      "epoch": 0.8097560975609757,
      "grad_norm": 0.028191309422254562,
      "learning_rate": 0.00011918699186991871,
      "loss": 0.0014,
      "step": 498
    },
    {
      "epoch": 0.8113821138211382,
      "grad_norm": 0.020040923729538918,
      "learning_rate": 0.00011902439024390244,
      "loss": 0.0009,
      "step": 499
    },
    {
      "epoch": 0.8130081300813008,
      "grad_norm": 0.028104929253458977,
      "learning_rate": 0.00011886178861788619,
      "loss": 0.002,
      "step": 500
    },
    {
      "epoch": 0.8146341463414634,
      "grad_norm": 0.01615731045603752,
      "learning_rate": 0.00011869918699186993,
      "loss": 0.0005,
      "step": 501
    },
    {
      "epoch": 0.816260162601626,
      "grad_norm": 0.016076011583209038,
      "learning_rate": 0.00011853658536585365,
      "loss": 0.0007,
      "step": 502
    },
    {
      "epoch": 0.8178861788617886,
      "grad_norm": 0.04311200976371765,
      "learning_rate": 0.0001183739837398374,
      "loss": 0.003,
      "step": 503
    },
    {
      "epoch": 0.8195121951219512,
      "grad_norm": 0.03994676098227501,
      "learning_rate": 0.00011821138211382115,
      "loss": 0.0031,
      "step": 504
    },
    {
      "epoch": 0.8211382113821138,
      "grad_norm": 0.03172750398516655,
      "learning_rate": 0.00011804878048780488,
      "loss": 0.0011,
      "step": 505
    },
    {
      "epoch": 0.8227642276422764,
      "grad_norm": 0.04845735803246498,
      "learning_rate": 0.00011788617886178864,
      "loss": 0.0033,
      "step": 506
    },
    {
      "epoch": 0.824390243902439,
      "grad_norm": 0.018561698496341705,
      "learning_rate": 0.00011772357723577236,
      "loss": 0.0006,
      "step": 507
    },
    {
      "epoch": 0.8260162601626017,
      "grad_norm": 0.021360432729125023,
      "learning_rate": 0.0001175609756097561,
      "loss": 0.0009,
      "step": 508
    },
    {
      "epoch": 0.8276422764227642,
      "grad_norm": 0.016144506633281708,
      "learning_rate": 0.00011739837398373985,
      "loss": 0.001,
      "step": 509
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 0.09096671640872955,
      "learning_rate": 0.00011723577235772358,
      "loss": 0.0081,
      "step": 510
    },
    {
      "epoch": 0.8308943089430895,
      "grad_norm": 0.01564052887260914,
      "learning_rate": 0.00011707317073170732,
      "loss": 0.0005,
      "step": 511
    },
    {
      "epoch": 0.832520325203252,
      "grad_norm": 0.0628076121211052,
      "learning_rate": 0.00011691056910569107,
      "loss": 0.0064,
      "step": 512
    },
    {
      "epoch": 0.8341463414634146,
      "grad_norm": 0.03254637122154236,
      "learning_rate": 0.0001167479674796748,
      "loss": 0.0012,
      "step": 513
    },
    {
      "epoch": 0.8357723577235773,
      "grad_norm": 0.052123911678791046,
      "learning_rate": 0.00011658536585365853,
      "loss": 0.0054,
      "step": 514
    },
    {
      "epoch": 0.8373983739837398,
      "grad_norm": 0.011211443692445755,
      "learning_rate": 0.00011642276422764229,
      "loss": 0.0005,
      "step": 515
    },
    {
      "epoch": 0.8390243902439024,
      "grad_norm": 0.016485324129462242,
      "learning_rate": 0.00011626016260162603,
      "loss": 0.0007,
      "step": 516
    },
    {
      "epoch": 0.8406504065040651,
      "grad_norm": 0.04991016536951065,
      "learning_rate": 0.00011609756097560975,
      "loss": 0.0021,
      "step": 517
    },
    {
      "epoch": 0.8422764227642277,
      "grad_norm": 0.015922604128718376,
      "learning_rate": 0.0001159349593495935,
      "loss": 0.0005,
      "step": 518
    },
    {
      "epoch": 0.8439024390243902,
      "grad_norm": 0.023434707894921303,
      "learning_rate": 0.00011577235772357724,
      "loss": 0.0015,
      "step": 519
    },
    {
      "epoch": 0.8455284552845529,
      "grad_norm": 0.024555252864956856,
      "learning_rate": 0.00011560975609756097,
      "loss": 0.0011,
      "step": 520
    },
    {
      "epoch": 0.8471544715447155,
      "grad_norm": 0.04176575317978859,
      "learning_rate": 0.00011544715447154472,
      "loss": 0.0026,
      "step": 521
    },
    {
      "epoch": 0.848780487804878,
      "grad_norm": 0.02413194440305233,
      "learning_rate": 0.00011528455284552846,
      "loss": 0.0008,
      "step": 522
    },
    {
      "epoch": 0.8504065040650407,
      "grad_norm": 0.037769243121147156,
      "learning_rate": 0.00011512195121951219,
      "loss": 0.0027,
      "step": 523
    },
    {
      "epoch": 0.8520325203252033,
      "grad_norm": 0.036895595490932465,
      "learning_rate": 0.00011495934959349595,
      "loss": 0.0029,
      "step": 524
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 0.03181309252977371,
      "learning_rate": 0.00011479674796747968,
      "loss": 0.0013,
      "step": 525
    },
    {
      "epoch": 0.8552845528455284,
      "grad_norm": 0.02323925495147705,
      "learning_rate": 0.00011463414634146342,
      "loss": 0.0009,
      "step": 526
    },
    {
      "epoch": 0.8569105691056911,
      "grad_norm": 0.05257396772503853,
      "learning_rate": 0.00011447154471544717,
      "loss": 0.0026,
      "step": 527
    },
    {
      "epoch": 0.8585365853658536,
      "grad_norm": 0.013914236798882484,
      "learning_rate": 0.0001143089430894309,
      "loss": 0.0006,
      "step": 528
    },
    {
      "epoch": 0.8601626016260162,
      "grad_norm": 0.011155262589454651,
      "learning_rate": 0.00011414634146341463,
      "loss": 0.0002,
      "step": 529
    },
    {
      "epoch": 0.8617886178861789,
      "grad_norm": 0.015880966559052467,
      "learning_rate": 0.00011398373983739839,
      "loss": 0.0007,
      "step": 530
    },
    {
      "epoch": 0.8634146341463415,
      "grad_norm": 0.025967376306653023,
      "learning_rate": 0.00011382113821138211,
      "loss": 0.0009,
      "step": 531
    },
    {
      "epoch": 0.865040650406504,
      "grad_norm": 0.01948608085513115,
      "learning_rate": 0.00011365853658536585,
      "loss": 0.0007,
      "step": 532
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.018302444368600845,
      "learning_rate": 0.0001134959349593496,
      "loss": 0.0007,
      "step": 533
    },
    {
      "epoch": 0.8682926829268293,
      "grad_norm": 0.028660833835601807,
      "learning_rate": 0.00011333333333333334,
      "loss": 0.0019,
      "step": 534
    },
    {
      "epoch": 0.8699186991869918,
      "grad_norm": 0.021785728633403778,
      "learning_rate": 0.00011317073170731707,
      "loss": 0.0011,
      "step": 535
    },
    {
      "epoch": 0.8715447154471545,
      "grad_norm": 0.031346190720796585,
      "learning_rate": 0.00011300813008130082,
      "loss": 0.001,
      "step": 536
    },
    {
      "epoch": 0.8731707317073171,
      "grad_norm": 0.02406279183924198,
      "learning_rate": 0.00011284552845528456,
      "loss": 0.0014,
      "step": 537
    },
    {
      "epoch": 0.8747967479674796,
      "grad_norm": 0.04375109821557999,
      "learning_rate": 0.00011268292682926828,
      "loss": 0.0031,
      "step": 538
    },
    {
      "epoch": 0.8764227642276423,
      "grad_norm": 0.018627827987074852,
      "learning_rate": 0.00011252032520325204,
      "loss": 0.0006,
      "step": 539
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 0.027309613302350044,
      "learning_rate": 0.00011235772357723578,
      "loss": 0.0008,
      "step": 540
    },
    {
      "epoch": 0.8796747967479674,
      "grad_norm": 0.03513088449835777,
      "learning_rate": 0.00011219512195121953,
      "loss": 0.0025,
      "step": 541
    },
    {
      "epoch": 0.8813008130081301,
      "grad_norm": 0.053457196801900864,
      "learning_rate": 0.00011203252032520327,
      "loss": 0.0035,
      "step": 542
    },
    {
      "epoch": 0.8829268292682927,
      "grad_norm": 0.017759982496500015,
      "learning_rate": 0.00011186991869918699,
      "loss": 0.0002,
      "step": 543
    },
    {
      "epoch": 0.8845528455284553,
      "grad_norm": 0.005492689553648233,
      "learning_rate": 0.00011170731707317074,
      "loss": 0.0002,
      "step": 544
    },
    {
      "epoch": 0.8861788617886179,
      "grad_norm": 0.058742325752973557,
      "learning_rate": 0.00011154471544715448,
      "loss": 0.005,
      "step": 545
    },
    {
      "epoch": 0.8878048780487805,
      "grad_norm": 0.018566960468888283,
      "learning_rate": 0.00011138211382113821,
      "loss": 0.0005,
      "step": 546
    },
    {
      "epoch": 0.8894308943089431,
      "grad_norm": 0.02755313739180565,
      "learning_rate": 0.00011121951219512196,
      "loss": 0.0012,
      "step": 547
    },
    {
      "epoch": 0.8910569105691057,
      "grad_norm": 0.024027865380048752,
      "learning_rate": 0.0001110569105691057,
      "loss": 0.0009,
      "step": 548
    },
    {
      "epoch": 0.8926829268292683,
      "grad_norm": 0.014305217191576958,
      "learning_rate": 0.00011089430894308943,
      "loss": 0.0006,
      "step": 549
    },
    {
      "epoch": 0.8943089430894309,
      "grad_norm": 0.03786611557006836,
      "learning_rate": 0.00011073170731707319,
      "loss": 0.0035,
      "step": 550
    },
    {
      "epoch": 0.8959349593495934,
      "grad_norm": 0.07740436494350433,
      "learning_rate": 0.00011056910569105692,
      "loss": 0.0047,
      "step": 551
    },
    {
      "epoch": 0.8975609756097561,
      "grad_norm": 0.028541529551148415,
      "learning_rate": 0.00011040650406504066,
      "loss": 0.001,
      "step": 552
    },
    {
      "epoch": 0.8991869918699187,
      "grad_norm": 0.026759367436170578,
      "learning_rate": 0.00011024390243902441,
      "loss": 0.0008,
      "step": 553
    },
    {
      "epoch": 0.9008130081300812,
      "grad_norm": 0.048677392303943634,
      "learning_rate": 0.00011008130081300813,
      "loss": 0.0044,
      "step": 554
    },
    {
      "epoch": 0.9024390243902439,
      "grad_norm": 0.01839292421936989,
      "learning_rate": 0.00010991869918699187,
      "loss": 0.0007,
      "step": 555
    },
    {
      "epoch": 0.9040650406504065,
      "grad_norm": 0.0452253632247448,
      "learning_rate": 0.00010975609756097563,
      "loss": 0.0012,
      "step": 556
    },
    {
      "epoch": 0.9056910569105691,
      "grad_norm": 0.012644516304135323,
      "learning_rate": 0.00010959349593495935,
      "loss": 0.0005,
      "step": 557
    },
    {
      "epoch": 0.9073170731707317,
      "grad_norm": 0.036521803587675095,
      "learning_rate": 0.00010943089430894309,
      "loss": 0.0019,
      "step": 558
    },
    {
      "epoch": 0.9089430894308943,
      "grad_norm": 0.04588788375258446,
      "learning_rate": 0.00010926829268292684,
      "loss": 0.0049,
      "step": 559
    },
    {
      "epoch": 0.9105691056910569,
      "grad_norm": 0.025674177333712578,
      "learning_rate": 0.00010910569105691057,
      "loss": 0.0009,
      "step": 560
    },
    {
      "epoch": 0.9121951219512195,
      "grad_norm": 0.05303553119301796,
      "learning_rate": 0.00010894308943089431,
      "loss": 0.0036,
      "step": 561
    },
    {
      "epoch": 0.9138211382113821,
      "grad_norm": 0.06171534210443497,
      "learning_rate": 0.00010878048780487806,
      "loss": 0.0034,
      "step": 562
    },
    {
      "epoch": 0.9154471544715447,
      "grad_norm": 0.032720163464546204,
      "learning_rate": 0.0001086178861788618,
      "loss": 0.0012,
      "step": 563
    },
    {
      "epoch": 0.9170731707317074,
      "grad_norm": 0.041798610240221024,
      "learning_rate": 0.00010845528455284552,
      "loss": 0.0008,
      "step": 564
    },
    {
      "epoch": 0.9186991869918699,
      "grad_norm": 0.05163143202662468,
      "learning_rate": 0.00010829268292682928,
      "loss": 0.0055,
      "step": 565
    },
    {
      "epoch": 0.9203252032520325,
      "grad_norm": 0.06489244103431702,
      "learning_rate": 0.00010813008130081302,
      "loss": 0.004,
      "step": 566
    },
    {
      "epoch": 0.9219512195121952,
      "grad_norm": 0.01523418165743351,
      "learning_rate": 0.00010796747967479674,
      "loss": 0.0004,
      "step": 567
    },
    {
      "epoch": 0.9235772357723577,
      "grad_norm": 0.006014470476657152,
      "learning_rate": 0.0001078048780487805,
      "loss": 0.0002,
      "step": 568
    },
    {
      "epoch": 0.9252032520325203,
      "grad_norm": 0.030240919440984726,
      "learning_rate": 0.00010764227642276423,
      "loss": 0.0008,
      "step": 569
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 0.01644095964729786,
      "learning_rate": 0.00010747967479674796,
      "loss": 0.0006,
      "step": 570
    },
    {
      "epoch": 0.9284552845528455,
      "grad_norm": 0.027487248182296753,
      "learning_rate": 0.00010731707317073172,
      "loss": 0.0008,
      "step": 571
    },
    {
      "epoch": 0.9300813008130081,
      "grad_norm": 0.02853054739534855,
      "learning_rate": 0.00010715447154471545,
      "loss": 0.0012,
      "step": 572
    },
    {
      "epoch": 0.9317073170731708,
      "grad_norm": 0.035733114928007126,
      "learning_rate": 0.00010699186991869919,
      "loss": 0.0022,
      "step": 573
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.0017652700189501047,
      "learning_rate": 0.00010682926829268294,
      "loss": 0.0001,
      "step": 574
    },
    {
      "epoch": 0.9349593495934959,
      "grad_norm": 0.033423926681280136,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.002,
      "step": 575
    },
    {
      "epoch": 0.9365853658536586,
      "grad_norm": 0.0750659927725792,
      "learning_rate": 0.0001065040650406504,
      "loss": 0.0023,
      "step": 576
    },
    {
      "epoch": 0.9382113821138212,
      "grad_norm": 0.011079270392656326,
      "learning_rate": 0.00010634146341463416,
      "loss": 0.0004,
      "step": 577
    },
    {
      "epoch": 0.9398373983739837,
      "grad_norm": 0.015469861216843128,
      "learning_rate": 0.00010617886178861788,
      "loss": 0.0006,
      "step": 578
    },
    {
      "epoch": 0.9414634146341463,
      "grad_norm": 0.018353410065174103,
      "learning_rate": 0.00010601626016260162,
      "loss": 0.0006,
      "step": 579
    },
    {
      "epoch": 0.943089430894309,
      "grad_norm": 0.028354918584227562,
      "learning_rate": 0.00010585365853658537,
      "loss": 0.002,
      "step": 580
    },
    {
      "epoch": 0.9447154471544715,
      "grad_norm": 0.04888761788606644,
      "learning_rate": 0.00010569105691056911,
      "loss": 0.0038,
      "step": 581
    },
    {
      "epoch": 0.9463414634146341,
      "grad_norm": 0.04139448329806328,
      "learning_rate": 0.00010552845528455284,
      "loss": 0.0017,
      "step": 582
    },
    {
      "epoch": 0.9479674796747968,
      "grad_norm": 0.046597670763731,
      "learning_rate": 0.00010536585365853659,
      "loss": 0.0023,
      "step": 583
    },
    {
      "epoch": 0.9495934959349593,
      "grad_norm": 0.005212120246142149,
      "learning_rate": 0.00010520325203252033,
      "loss": 0.0002,
      "step": 584
    },
    {
      "epoch": 0.9512195121951219,
      "grad_norm": 0.0357334278523922,
      "learning_rate": 0.00010504065040650406,
      "loss": 0.0017,
      "step": 585
    },
    {
      "epoch": 0.9528455284552846,
      "grad_norm": 0.03520767763257027,
      "learning_rate": 0.00010487804878048781,
      "loss": 0.0021,
      "step": 586
    },
    {
      "epoch": 0.9544715447154472,
      "grad_norm": 0.03216665983200073,
      "learning_rate": 0.00010471544715447155,
      "loss": 0.0006,
      "step": 587
    },
    {
      "epoch": 0.9560975609756097,
      "grad_norm": 0.01625550352036953,
      "learning_rate": 0.00010455284552845527,
      "loss": 0.0005,
      "step": 588
    },
    {
      "epoch": 0.9577235772357724,
      "grad_norm": 0.03498171269893646,
      "learning_rate": 0.00010439024390243904,
      "loss": 0.001,
      "step": 589
    },
    {
      "epoch": 0.959349593495935,
      "grad_norm": 0.021105607971549034,
      "learning_rate": 0.00010422764227642276,
      "loss": 0.0008,
      "step": 590
    },
    {
      "epoch": 0.9609756097560975,
      "grad_norm": 0.006631603930145502,
      "learning_rate": 0.00010406504065040652,
      "loss": 0.0002,
      "step": 591
    },
    {
      "epoch": 0.9626016260162602,
      "grad_norm": 0.01798001490533352,
      "learning_rate": 0.00010390243902439026,
      "loss": 0.0008,
      "step": 592
    },
    {
      "epoch": 0.9642276422764228,
      "grad_norm": 0.023252597078680992,
      "learning_rate": 0.00010373983739837398,
      "loss": 0.0008,
      "step": 593
    },
    {
      "epoch": 0.9658536585365853,
      "grad_norm": 0.01537206768989563,
      "learning_rate": 0.00010357723577235773,
      "loss": 0.0007,
      "step": 594
    },
    {
      "epoch": 0.967479674796748,
      "grad_norm": 0.03023386187851429,
      "learning_rate": 0.00010341463414634147,
      "loss": 0.0017,
      "step": 595
    },
    {
      "epoch": 0.9691056910569106,
      "grad_norm": 0.023562436923384666,
      "learning_rate": 0.0001032520325203252,
      "loss": 0.0012,
      "step": 596
    },
    {
      "epoch": 0.9707317073170731,
      "grad_norm": 0.019724881276488304,
      "learning_rate": 0.00010308943089430896,
      "loss": 0.0009,
      "step": 597
    },
    {
      "epoch": 0.9723577235772358,
      "grad_norm": 0.02794792503118515,
      "learning_rate": 0.00010292682926829269,
      "loss": 0.0013,
      "step": 598
    },
    {
      "epoch": 0.9739837398373984,
      "grad_norm": 0.018562523648142815,
      "learning_rate": 0.00010276422764227643,
      "loss": 0.0007,
      "step": 599
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 0.061098743230104446,
      "learning_rate": 0.00010260162601626018,
      "loss": 0.0051,
      "step": 600
    },
    {
      "epoch": 0.9772357723577236,
      "grad_norm": 0.02799287624657154,
      "learning_rate": 0.0001024390243902439,
      "loss": 0.0011,
      "step": 601
    },
    {
      "epoch": 0.9788617886178862,
      "grad_norm": 0.014771801419556141,
      "learning_rate": 0.00010227642276422765,
      "loss": 0.0004,
      "step": 602
    },
    {
      "epoch": 0.9804878048780488,
      "grad_norm": 0.046715617179870605,
      "learning_rate": 0.0001021138211382114,
      "loss": 0.003,
      "step": 603
    },
    {
      "epoch": 0.9821138211382113,
      "grad_norm": 0.002003920031711459,
      "learning_rate": 0.00010195121951219512,
      "loss": 0.0001,
      "step": 604
    },
    {
      "epoch": 0.983739837398374,
      "grad_norm": 0.025536799803376198,
      "learning_rate": 0.00010178861788617886,
      "loss": 0.0008,
      "step": 605
    },
    {
      "epoch": 0.9853658536585366,
      "grad_norm": 0.036047108471393585,
      "learning_rate": 0.00010162601626016262,
      "loss": 0.0024,
      "step": 606
    },
    {
      "epoch": 0.9869918699186991,
      "grad_norm": 0.052953463047742844,
      "learning_rate": 0.00010146341463414635,
      "loss": 0.0028,
      "step": 607
    },
    {
      "epoch": 0.9886178861788618,
      "grad_norm": 0.06137080118060112,
      "learning_rate": 0.00010130081300813008,
      "loss": 0.0022,
      "step": 608
    },
    {
      "epoch": 0.9902439024390244,
      "grad_norm": 0.05845019221305847,
      "learning_rate": 0.00010113821138211383,
      "loss": 0.0005,
      "step": 609
    },
    {
      "epoch": 0.991869918699187,
      "grad_norm": 0.006013177335262299,
      "learning_rate": 0.00010097560975609757,
      "loss": 0.0002,
      "step": 610
    },
    {
      "epoch": 0.9934959349593496,
      "grad_norm": 0.0023975882213562727,
      "learning_rate": 0.0001008130081300813,
      "loss": 0.0001,
      "step": 611
    },
    {
      "epoch": 0.9951219512195122,
      "grad_norm": 0.027645491063594818,
      "learning_rate": 0.00010065040650406505,
      "loss": 0.0012,
      "step": 612
    },
    {
      "epoch": 0.9967479674796748,
      "grad_norm": 0.029636425897479057,
      "learning_rate": 0.00010048780487804879,
      "loss": 0.0016,
      "step": 613
    },
    {
      "epoch": 0.9983739837398374,
      "grad_norm": 0.037054501473903656,
      "learning_rate": 0.00010032520325203251,
      "loss": 0.0017,
      "step": 614
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.056419797241687775,
      "learning_rate": 0.00010016260162601627,
      "loss": 0.0031,
      "step": 615
    }
  ],
  "logging_steps": 1,
  "max_steps": 1230,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4596911214428160.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
