{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1230,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016260162601626016,
      "grad_norm": 14.123039245605469,
      "learning_rate": 0.0002,
      "loss": 3.6058,
      "step": 1
    },
    {
      "epoch": 0.0032520325203252032,
      "grad_norm": 12.442439079284668,
      "learning_rate": 0.00019983739837398375,
      "loss": 2.7201,
      "step": 2
    },
    {
      "epoch": 0.004878048780487805,
      "grad_norm": 10.934565544128418,
      "learning_rate": 0.0001996747967479675,
      "loss": 1.943,
      "step": 3
    },
    {
      "epoch": 0.0065040650406504065,
      "grad_norm": 9.618895530700684,
      "learning_rate": 0.00019951219512195123,
      "loss": 1.4261,
      "step": 4
    },
    {
      "epoch": 0.008130081300813009,
      "grad_norm": 7.239424228668213,
      "learning_rate": 0.00019934959349593497,
      "loss": 0.8741,
      "step": 5
    },
    {
      "epoch": 0.00975609756097561,
      "grad_norm": 4.3384785652160645,
      "learning_rate": 0.0001991869918699187,
      "loss": 0.5331,
      "step": 6
    },
    {
      "epoch": 0.011382113821138212,
      "grad_norm": 2.646343231201172,
      "learning_rate": 0.00019902439024390244,
      "loss": 0.3537,
      "step": 7
    },
    {
      "epoch": 0.013008130081300813,
      "grad_norm": 1.4339319467544556,
      "learning_rate": 0.00019886178861788618,
      "loss": 0.2425,
      "step": 8
    },
    {
      "epoch": 0.014634146341463415,
      "grad_norm": 0.8906335234642029,
      "learning_rate": 0.00019869918699186992,
      "loss": 0.1912,
      "step": 9
    },
    {
      "epoch": 0.016260162601626018,
      "grad_norm": 0.5569081902503967,
      "learning_rate": 0.00019853658536585366,
      "loss": 0.1556,
      "step": 10
    },
    {
      "epoch": 0.01788617886178862,
      "grad_norm": 0.36139819025993347,
      "learning_rate": 0.0001983739837398374,
      "loss": 0.1384,
      "step": 11
    },
    {
      "epoch": 0.01951219512195122,
      "grad_norm": 0.2682724893093109,
      "learning_rate": 0.00019821138211382117,
      "loss": 0.1375,
      "step": 12
    },
    {
      "epoch": 0.02113821138211382,
      "grad_norm": 0.2210274189710617,
      "learning_rate": 0.00019804878048780488,
      "loss": 0.1195,
      "step": 13
    },
    {
      "epoch": 0.022764227642276424,
      "grad_norm": 0.17774926126003265,
      "learning_rate": 0.00019788617886178862,
      "loss": 0.111,
      "step": 14
    },
    {
      "epoch": 0.024390243902439025,
      "grad_norm": 0.17169004678726196,
      "learning_rate": 0.00019772357723577238,
      "loss": 0.1309,
      "step": 15
    },
    {
      "epoch": 0.026016260162601626,
      "grad_norm": 0.16225352883338928,
      "learning_rate": 0.0001975609756097561,
      "loss": 0.1157,
      "step": 16
    },
    {
      "epoch": 0.027642276422764227,
      "grad_norm": 0.14445935189723969,
      "learning_rate": 0.00019739837398373983,
      "loss": 0.1056,
      "step": 17
    },
    {
      "epoch": 0.02926829268292683,
      "grad_norm": 0.13701938092708588,
      "learning_rate": 0.0001972357723577236,
      "loss": 0.0979,
      "step": 18
    },
    {
      "epoch": 0.030894308943089432,
      "grad_norm": 0.14380183815956116,
      "learning_rate": 0.0001970731707317073,
      "loss": 0.0985,
      "step": 19
    },
    {
      "epoch": 0.032520325203252036,
      "grad_norm": 0.13301730155944824,
      "learning_rate": 0.00019691056910569105,
      "loss": 0.0929,
      "step": 20
    },
    {
      "epoch": 0.03414634146341464,
      "grad_norm": 0.14615321159362793,
      "learning_rate": 0.00019674796747967482,
      "loss": 0.0946,
      "step": 21
    },
    {
      "epoch": 0.03577235772357724,
      "grad_norm": 0.1557343602180481,
      "learning_rate": 0.00019658536585365856,
      "loss": 0.0867,
      "step": 22
    },
    {
      "epoch": 0.03739837398373984,
      "grad_norm": 0.14032889902591705,
      "learning_rate": 0.00019642276422764227,
      "loss": 0.0945,
      "step": 23
    },
    {
      "epoch": 0.03902439024390244,
      "grad_norm": 0.13826340436935425,
      "learning_rate": 0.00019626016260162603,
      "loss": 0.0795,
      "step": 24
    },
    {
      "epoch": 0.04065040650406504,
      "grad_norm": 0.15749295055866241,
      "learning_rate": 0.00019609756097560977,
      "loss": 0.0822,
      "step": 25
    },
    {
      "epoch": 0.04227642276422764,
      "grad_norm": 0.15631099045276642,
      "learning_rate": 0.00019593495934959348,
      "loss": 0.0767,
      "step": 26
    },
    {
      "epoch": 0.04390243902439024,
      "grad_norm": 0.14138931035995483,
      "learning_rate": 0.00019577235772357725,
      "loss": 0.0779,
      "step": 27
    },
    {
      "epoch": 0.04552845528455285,
      "grad_norm": 0.1426640897989273,
      "learning_rate": 0.000195609756097561,
      "loss": 0.0749,
      "step": 28
    },
    {
      "epoch": 0.04715447154471545,
      "grad_norm": 0.1442413330078125,
      "learning_rate": 0.00019544715447154473,
      "loss": 0.0685,
      "step": 29
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 0.1501743495464325,
      "learning_rate": 0.00019528455284552847,
      "loss": 0.0679,
      "step": 30
    },
    {
      "epoch": 0.05040650406504065,
      "grad_norm": 0.1304495483636856,
      "learning_rate": 0.0001951219512195122,
      "loss": 0.0769,
      "step": 31
    },
    {
      "epoch": 0.05203252032520325,
      "grad_norm": 0.15433509647846222,
      "learning_rate": 0.00019495934959349594,
      "loss": 0.0614,
      "step": 32
    },
    {
      "epoch": 0.05365853658536585,
      "grad_norm": 0.14335307478904724,
      "learning_rate": 0.00019479674796747968,
      "loss": 0.0656,
      "step": 33
    },
    {
      "epoch": 0.055284552845528454,
      "grad_norm": 0.15990345180034637,
      "learning_rate": 0.00019463414634146342,
      "loss": 0.0508,
      "step": 34
    },
    {
      "epoch": 0.056910569105691054,
      "grad_norm": 0.14366264641284943,
      "learning_rate": 0.00019447154471544716,
      "loss": 0.0553,
      "step": 35
    },
    {
      "epoch": 0.05853658536585366,
      "grad_norm": 0.20658202469348907,
      "learning_rate": 0.0001943089430894309,
      "loss": 0.0771,
      "step": 36
    },
    {
      "epoch": 0.06016260162601626,
      "grad_norm": 0.15433137118816376,
      "learning_rate": 0.00019414634146341464,
      "loss": 0.0582,
      "step": 37
    },
    {
      "epoch": 0.061788617886178863,
      "grad_norm": 0.15748094022274017,
      "learning_rate": 0.0001939837398373984,
      "loss": 0.0588,
      "step": 38
    },
    {
      "epoch": 0.06341463414634146,
      "grad_norm": 0.13017624616622925,
      "learning_rate": 0.00019382113821138212,
      "loss": 0.0519,
      "step": 39
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 0.16890975832939148,
      "learning_rate": 0.00019365853658536586,
      "loss": 0.0522,
      "step": 40
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.11865324527025223,
      "learning_rate": 0.00019349593495934962,
      "loss": 0.0317,
      "step": 41
    },
    {
      "epoch": 0.06829268292682927,
      "grad_norm": 0.12453564256429672,
      "learning_rate": 0.00019333333333333333,
      "loss": 0.0445,
      "step": 42
    },
    {
      "epoch": 0.06991869918699187,
      "grad_norm": 0.12275237590074539,
      "learning_rate": 0.00019317073170731707,
      "loss": 0.0436,
      "step": 43
    },
    {
      "epoch": 0.07154471544715447,
      "grad_norm": 0.13192838430404663,
      "learning_rate": 0.00019300813008130084,
      "loss": 0.0422,
      "step": 44
    },
    {
      "epoch": 0.07317073170731707,
      "grad_norm": 0.10686997324228287,
      "learning_rate": 0.00019284552845528455,
      "loss": 0.0352,
      "step": 45
    },
    {
      "epoch": 0.07479674796747968,
      "grad_norm": 0.10088750720024109,
      "learning_rate": 0.0001926829268292683,
      "loss": 0.0312,
      "step": 46
    },
    {
      "epoch": 0.07642276422764227,
      "grad_norm": 0.10405315458774567,
      "learning_rate": 0.00019252032520325206,
      "loss": 0.025,
      "step": 47
    },
    {
      "epoch": 0.07804878048780488,
      "grad_norm": 0.11609800904989243,
      "learning_rate": 0.0001923577235772358,
      "loss": 0.0374,
      "step": 48
    },
    {
      "epoch": 0.07967479674796749,
      "grad_norm": 0.10765493661165237,
      "learning_rate": 0.0001921951219512195,
      "loss": 0.0399,
      "step": 49
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 0.1294170469045639,
      "learning_rate": 0.00019203252032520327,
      "loss": 0.0415,
      "step": 50
    },
    {
      "epoch": 0.08292682926829269,
      "grad_norm": 0.09018323570489883,
      "learning_rate": 0.000191869918699187,
      "loss": 0.0372,
      "step": 51
    },
    {
      "epoch": 0.08455284552845528,
      "grad_norm": 0.08691918849945068,
      "learning_rate": 0.00019170731707317072,
      "loss": 0.024,
      "step": 52
    },
    {
      "epoch": 0.08617886178861789,
      "grad_norm": 0.13573385775089264,
      "learning_rate": 0.0001915447154471545,
      "loss": 0.0384,
      "step": 53
    },
    {
      "epoch": 0.08780487804878048,
      "grad_norm": 0.07842563092708588,
      "learning_rate": 0.00019138211382113823,
      "loss": 0.0182,
      "step": 54
    },
    {
      "epoch": 0.08943089430894309,
      "grad_norm": 0.125136598944664,
      "learning_rate": 0.00019121951219512194,
      "loss": 0.0516,
      "step": 55
    },
    {
      "epoch": 0.0910569105691057,
      "grad_norm": 0.06792032718658447,
      "learning_rate": 0.0001910569105691057,
      "loss": 0.0223,
      "step": 56
    },
    {
      "epoch": 0.09268292682926829,
      "grad_norm": 0.1047743409872055,
      "learning_rate": 0.00019089430894308945,
      "loss": 0.0368,
      "step": 57
    },
    {
      "epoch": 0.0943089430894309,
      "grad_norm": 0.09451661258935928,
      "learning_rate": 0.00019073170731707319,
      "loss": 0.0132,
      "step": 58
    },
    {
      "epoch": 0.0959349593495935,
      "grad_norm": 0.09582289308309555,
      "learning_rate": 0.00019056910569105692,
      "loss": 0.0309,
      "step": 59
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 0.10035395622253418,
      "learning_rate": 0.00019040650406504066,
      "loss": 0.0416,
      "step": 60
    },
    {
      "epoch": 0.0991869918699187,
      "grad_norm": 0.08037086576223373,
      "learning_rate": 0.0001902439024390244,
      "loss": 0.032,
      "step": 61
    },
    {
      "epoch": 0.1008130081300813,
      "grad_norm": 0.07024725526571274,
      "learning_rate": 0.00019008130081300814,
      "loss": 0.0269,
      "step": 62
    },
    {
      "epoch": 0.1024390243902439,
      "grad_norm": 0.07535912841558456,
      "learning_rate": 0.00018991869918699188,
      "loss": 0.026,
      "step": 63
    },
    {
      "epoch": 0.1040650406504065,
      "grad_norm": 0.10366050899028778,
      "learning_rate": 0.00018975609756097562,
      "loss": 0.0162,
      "step": 64
    },
    {
      "epoch": 0.10569105691056911,
      "grad_norm": 0.08227387070655823,
      "learning_rate": 0.00018959349593495936,
      "loss": 0.0217,
      "step": 65
    },
    {
      "epoch": 0.1073170731707317,
      "grad_norm": 0.0741656944155693,
      "learning_rate": 0.0001894308943089431,
      "loss": 0.0301,
      "step": 66
    },
    {
      "epoch": 0.10894308943089431,
      "grad_norm": 0.10647080093622208,
      "learning_rate": 0.00018926829268292684,
      "loss": 0.0252,
      "step": 67
    },
    {
      "epoch": 0.11056910569105691,
      "grad_norm": 0.07371451705694199,
      "learning_rate": 0.00018910569105691057,
      "loss": 0.0318,
      "step": 68
    },
    {
      "epoch": 0.11219512195121951,
      "grad_norm": 0.05689657852053642,
      "learning_rate": 0.00018894308943089431,
      "loss": 0.0076,
      "step": 69
    },
    {
      "epoch": 0.11382113821138211,
      "grad_norm": 0.09139333665370941,
      "learning_rate": 0.00018878048780487805,
      "loss": 0.023,
      "step": 70
    },
    {
      "epoch": 0.11544715447154472,
      "grad_norm": 0.07399555295705795,
      "learning_rate": 0.0001886178861788618,
      "loss": 0.0294,
      "step": 71
    },
    {
      "epoch": 0.11707317073170732,
      "grad_norm": 0.08114836364984512,
      "learning_rate": 0.00018845528455284553,
      "loss": 0.028,
      "step": 72
    },
    {
      "epoch": 0.11869918699186992,
      "grad_norm": 0.07716210186481476,
      "learning_rate": 0.00018829268292682927,
      "loss": 0.0232,
      "step": 73
    },
    {
      "epoch": 0.12032520325203253,
      "grad_norm": 0.07454870641231537,
      "learning_rate": 0.000188130081300813,
      "loss": 0.0167,
      "step": 74
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 0.10065023601055145,
      "learning_rate": 0.00018796747967479675,
      "loss": 0.0285,
      "step": 75
    },
    {
      "epoch": 0.12357723577235773,
      "grad_norm": 0.07416273653507233,
      "learning_rate": 0.0001878048780487805,
      "loss": 0.02,
      "step": 76
    },
    {
      "epoch": 0.12520325203252033,
      "grad_norm": 0.06341444700956345,
      "learning_rate": 0.00018764227642276425,
      "loss": 0.0187,
      "step": 77
    },
    {
      "epoch": 0.12682926829268293,
      "grad_norm": 0.06446120142936707,
      "learning_rate": 0.00018747967479674796,
      "loss": 0.0087,
      "step": 78
    },
    {
      "epoch": 0.12845528455284552,
      "grad_norm": 0.060524340718984604,
      "learning_rate": 0.00018731707317073173,
      "loss": 0.0134,
      "step": 79
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 0.04947732016444206,
      "learning_rate": 0.00018715447154471547,
      "loss": 0.0067,
      "step": 80
    },
    {
      "epoch": 0.13170731707317074,
      "grad_norm": 0.05538910627365112,
      "learning_rate": 0.00018699186991869918,
      "loss": 0.009,
      "step": 81
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.08997729420661926,
      "learning_rate": 0.00018682926829268295,
      "loss": 0.0226,
      "step": 82
    },
    {
      "epoch": 0.13495934959349593,
      "grad_norm": 0.05830923467874527,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.0118,
      "step": 83
    },
    {
      "epoch": 0.13658536585365855,
      "grad_norm": 0.05442948266863823,
      "learning_rate": 0.0001865040650406504,
      "loss": 0.0136,
      "step": 84
    },
    {
      "epoch": 0.13821138211382114,
      "grad_norm": 0.049664344638586044,
      "learning_rate": 0.00018634146341463416,
      "loss": 0.0086,
      "step": 85
    },
    {
      "epoch": 0.13983739837398373,
      "grad_norm": 0.07180367410182953,
      "learning_rate": 0.0001861788617886179,
      "loss": 0.0145,
      "step": 86
    },
    {
      "epoch": 0.14146341463414633,
      "grad_norm": 0.08550627529621124,
      "learning_rate": 0.00018601626016260164,
      "loss": 0.0318,
      "step": 87
    },
    {
      "epoch": 0.14308943089430895,
      "grad_norm": 0.1153000071644783,
      "learning_rate": 0.00018585365853658538,
      "loss": 0.021,
      "step": 88
    },
    {
      "epoch": 0.14471544715447154,
      "grad_norm": 0.11239909380674362,
      "learning_rate": 0.00018569105691056912,
      "loss": 0.0339,
      "step": 89
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 0.050321921706199646,
      "learning_rate": 0.00018552845528455286,
      "loss": 0.0155,
      "step": 90
    },
    {
      "epoch": 0.14796747967479676,
      "grad_norm": 0.08015507459640503,
      "learning_rate": 0.0001853658536585366,
      "loss": 0.0169,
      "step": 91
    },
    {
      "epoch": 0.14959349593495935,
      "grad_norm": 0.06360334157943726,
      "learning_rate": 0.00018520325203252034,
      "loss": 0.0042,
      "step": 92
    },
    {
      "epoch": 0.15121951219512195,
      "grad_norm": 0.05382903292775154,
      "learning_rate": 0.00018504065040650408,
      "loss": 0.0147,
      "step": 93
    },
    {
      "epoch": 0.15284552845528454,
      "grad_norm": 0.04847598448395729,
      "learning_rate": 0.00018487804878048782,
      "loss": 0.0056,
      "step": 94
    },
    {
      "epoch": 0.15447154471544716,
      "grad_norm": 0.06303137540817261,
      "learning_rate": 0.00018471544715447155,
      "loss": 0.0109,
      "step": 95
    },
    {
      "epoch": 0.15609756097560976,
      "grad_norm": 0.061439476907253265,
      "learning_rate": 0.0001845528455284553,
      "loss": 0.018,
      "step": 96
    },
    {
      "epoch": 0.15772357723577235,
      "grad_norm": 0.06210711970925331,
      "learning_rate": 0.00018439024390243903,
      "loss": 0.0195,
      "step": 97
    },
    {
      "epoch": 0.15934959349593497,
      "grad_norm": 0.05871272832155228,
      "learning_rate": 0.00018422764227642277,
      "loss": 0.0114,
      "step": 98
    },
    {
      "epoch": 0.16097560975609757,
      "grad_norm": 0.03230946511030197,
      "learning_rate": 0.0001840650406504065,
      "loss": 0.0028,
      "step": 99
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 0.06922970712184906,
      "learning_rate": 0.00018390243902439025,
      "loss": 0.019,
      "step": 100
    },
    {
      "epoch": 0.16422764227642275,
      "grad_norm": 0.06678000837564468,
      "learning_rate": 0.000183739837398374,
      "loss": 0.0083,
      "step": 101
    },
    {
      "epoch": 0.16585365853658537,
      "grad_norm": 0.07466066628694534,
      "learning_rate": 0.00018357723577235773,
      "loss": 0.0248,
      "step": 102
    },
    {
      "epoch": 0.16747967479674797,
      "grad_norm": 0.10639944672584534,
      "learning_rate": 0.0001834146341463415,
      "loss": 0.0297,
      "step": 103
    },
    {
      "epoch": 0.16910569105691056,
      "grad_norm": 0.08071240782737732,
      "learning_rate": 0.0001832520325203252,
      "loss": 0.0139,
      "step": 104
    },
    {
      "epoch": 0.17073170731707318,
      "grad_norm": 0.04677708446979523,
      "learning_rate": 0.00018308943089430894,
      "loss": 0.0026,
      "step": 105
    },
    {
      "epoch": 0.17235772357723578,
      "grad_norm": 0.040342919528484344,
      "learning_rate": 0.0001829268292682927,
      "loss": 0.0026,
      "step": 106
    },
    {
      "epoch": 0.17398373983739837,
      "grad_norm": 0.09369560331106186,
      "learning_rate": 0.00018276422764227642,
      "loss": 0.0246,
      "step": 107
    },
    {
      "epoch": 0.17560975609756097,
      "grad_norm": 0.04911818727850914,
      "learning_rate": 0.00018260162601626016,
      "loss": 0.0118,
      "step": 108
    },
    {
      "epoch": 0.1772357723577236,
      "grad_norm": 0.06689990311861038,
      "learning_rate": 0.00018243902439024393,
      "loss": 0.0085,
      "step": 109
    },
    {
      "epoch": 0.17886178861788618,
      "grad_norm": 0.058269962668418884,
      "learning_rate": 0.00018227642276422764,
      "loss": 0.0091,
      "step": 110
    },
    {
      "epoch": 0.18048780487804877,
      "grad_norm": 0.05585408955812454,
      "learning_rate": 0.00018211382113821138,
      "loss": 0.0169,
      "step": 111
    },
    {
      "epoch": 0.1821138211382114,
      "grad_norm": 0.07020600885152817,
      "learning_rate": 0.00018195121951219514,
      "loss": 0.0078,
      "step": 112
    },
    {
      "epoch": 0.183739837398374,
      "grad_norm": 0.06154538318514824,
      "learning_rate": 0.00018178861788617888,
      "loss": 0.015,
      "step": 113
    },
    {
      "epoch": 0.18536585365853658,
      "grad_norm": 0.05866541340947151,
      "learning_rate": 0.0001816260162601626,
      "loss": 0.0088,
      "step": 114
    },
    {
      "epoch": 0.18699186991869918,
      "grad_norm": 0.08026880025863647,
      "learning_rate": 0.00018146341463414636,
      "loss": 0.0147,
      "step": 115
    },
    {
      "epoch": 0.1886178861788618,
      "grad_norm": 0.04392948001623154,
      "learning_rate": 0.0001813008130081301,
      "loss": 0.0088,
      "step": 116
    },
    {
      "epoch": 0.1902439024390244,
      "grad_norm": 0.057295095175504684,
      "learning_rate": 0.0001811382113821138,
      "loss": 0.0089,
      "step": 117
    },
    {
      "epoch": 0.191869918699187,
      "grad_norm": 0.04084128141403198,
      "learning_rate": 0.00018097560975609758,
      "loss": 0.0085,
      "step": 118
    },
    {
      "epoch": 0.19349593495934958,
      "grad_norm": 0.07043016701936722,
      "learning_rate": 0.00018081300813008132,
      "loss": 0.0126,
      "step": 119
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 0.04601474106311798,
      "learning_rate": 0.00018065040650406503,
      "loss": 0.0066,
      "step": 120
    },
    {
      "epoch": 0.1967479674796748,
      "grad_norm": 0.08078140765428543,
      "learning_rate": 0.0001804878048780488,
      "loss": 0.011,
      "step": 121
    },
    {
      "epoch": 0.1983739837398374,
      "grad_norm": 0.0810873880982399,
      "learning_rate": 0.00018032520325203253,
      "loss": 0.019,
      "step": 122
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.07094967365264893,
      "learning_rate": 0.00018016260162601625,
      "loss": 0.0128,
      "step": 123
    },
    {
      "epoch": 0.2016260162601626,
      "grad_norm": 0.06718281656503677,
      "learning_rate": 0.00018,
      "loss": 0.0224,
      "step": 124
    },
    {
      "epoch": 0.2032520325203252,
      "grad_norm": 0.060588691383600235,
      "learning_rate": 0.00017983739837398375,
      "loss": 0.0153,
      "step": 125
    },
    {
      "epoch": 0.2048780487804878,
      "grad_norm": 0.047028928995132446,
      "learning_rate": 0.0001796747967479675,
      "loss": 0.0065,
      "step": 126
    },
    {
      "epoch": 0.20650406504065041,
      "grad_norm": 0.0495992936193943,
      "learning_rate": 0.00017951219512195123,
      "loss": 0.0098,
      "step": 127
    },
    {
      "epoch": 0.208130081300813,
      "grad_norm": 0.05529371276497841,
      "learning_rate": 0.00017934959349593497,
      "loss": 0.0147,
      "step": 128
    },
    {
      "epoch": 0.2097560975609756,
      "grad_norm": 0.04449852555990219,
      "learning_rate": 0.0001791869918699187,
      "loss": 0.0048,
      "step": 129
    },
    {
      "epoch": 0.21138211382113822,
      "grad_norm": 0.09078140556812286,
      "learning_rate": 0.00017902439024390245,
      "loss": 0.0242,
      "step": 130
    },
    {
      "epoch": 0.21300813008130082,
      "grad_norm": 0.033828720450401306,
      "learning_rate": 0.00017886178861788618,
      "loss": 0.0052,
      "step": 131
    },
    {
      "epoch": 0.2146341463414634,
      "grad_norm": 0.03269187733530998,
      "learning_rate": 0.00017869918699186995,
      "loss": 0.0019,
      "step": 132
    },
    {
      "epoch": 0.216260162601626,
      "grad_norm": 0.03835708275437355,
      "learning_rate": 0.00017853658536585366,
      "loss": 0.0051,
      "step": 133
    },
    {
      "epoch": 0.21788617886178863,
      "grad_norm": 0.06826446205377579,
      "learning_rate": 0.0001783739837398374,
      "loss": 0.0191,
      "step": 134
    },
    {
      "epoch": 0.21951219512195122,
      "grad_norm": 0.07213122397661209,
      "learning_rate": 0.00017821138211382117,
      "loss": 0.0117,
      "step": 135
    },
    {
      "epoch": 0.22113821138211381,
      "grad_norm": 0.030881166458129883,
      "learning_rate": 0.00017804878048780488,
      "loss": 0.0017,
      "step": 136
    },
    {
      "epoch": 0.22276422764227644,
      "grad_norm": 0.05711333826184273,
      "learning_rate": 0.00017788617886178862,
      "loss": 0.0036,
      "step": 137
    },
    {
      "epoch": 0.22439024390243903,
      "grad_norm": 0.04516725614666939,
      "learning_rate": 0.00017772357723577238,
      "loss": 0.0069,
      "step": 138
    },
    {
      "epoch": 0.22601626016260162,
      "grad_norm": 0.06221675127744675,
      "learning_rate": 0.0001775609756097561,
      "loss": 0.0102,
      "step": 139
    },
    {
      "epoch": 0.22764227642276422,
      "grad_norm": 0.05953823775053024,
      "learning_rate": 0.00017739837398373983,
      "loss": 0.0162,
      "step": 140
    },
    {
      "epoch": 0.22926829268292684,
      "grad_norm": 0.06301414966583252,
      "learning_rate": 0.0001772357723577236,
      "loss": 0.0074,
      "step": 141
    },
    {
      "epoch": 0.23089430894308943,
      "grad_norm": 0.0576285682618618,
      "learning_rate": 0.00017707317073170734,
      "loss": 0.0128,
      "step": 142
    },
    {
      "epoch": 0.23252032520325203,
      "grad_norm": 0.06300925463438034,
      "learning_rate": 0.00017691056910569105,
      "loss": 0.0134,
      "step": 143
    },
    {
      "epoch": 0.23414634146341465,
      "grad_norm": 0.05683533847332001,
      "learning_rate": 0.00017674796747967482,
      "loss": 0.0127,
      "step": 144
    },
    {
      "epoch": 0.23577235772357724,
      "grad_norm": 0.06731590628623962,
      "learning_rate": 0.00017658536585365856,
      "loss": 0.015,
      "step": 145
    },
    {
      "epoch": 0.23739837398373984,
      "grad_norm": 0.036470796912908554,
      "learning_rate": 0.00017642276422764227,
      "loss": 0.0039,
      "step": 146
    },
    {
      "epoch": 0.23902439024390243,
      "grad_norm": 0.04167039319872856,
      "learning_rate": 0.00017626016260162603,
      "loss": 0.0047,
      "step": 147
    },
    {
      "epoch": 0.24065040650406505,
      "grad_norm": 0.10255386680364609,
      "learning_rate": 0.00017609756097560977,
      "loss": 0.0257,
      "step": 148
    },
    {
      "epoch": 0.24227642276422764,
      "grad_norm": 0.04812590032815933,
      "learning_rate": 0.00017593495934959349,
      "loss": 0.0046,
      "step": 149
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 0.024177923798561096,
      "learning_rate": 0.00017577235772357725,
      "loss": 0.0013,
      "step": 150
    },
    {
      "epoch": 0.24552845528455283,
      "grad_norm": 0.04535578191280365,
      "learning_rate": 0.000175609756097561,
      "loss": 0.0046,
      "step": 151
    },
    {
      "epoch": 0.24715447154471545,
      "grad_norm": 0.031351860612630844,
      "learning_rate": 0.00017544715447154473,
      "loss": 0.0016,
      "step": 152
    },
    {
      "epoch": 0.24878048780487805,
      "grad_norm": 0.02234712801873684,
      "learning_rate": 0.00017528455284552847,
      "loss": 0.0012,
      "step": 153
    },
    {
      "epoch": 0.25040650406504067,
      "grad_norm": 0.02679426036775112,
      "learning_rate": 0.0001751219512195122,
      "loss": 0.0028,
      "step": 154
    },
    {
      "epoch": 0.25203252032520324,
      "grad_norm": 0.05651911348104477,
      "learning_rate": 0.00017495934959349595,
      "loss": 0.0086,
      "step": 155
    },
    {
      "epoch": 0.25365853658536586,
      "grad_norm": 0.026251357048749924,
      "learning_rate": 0.00017479674796747969,
      "loss": 0.0027,
      "step": 156
    },
    {
      "epoch": 0.2552845528455285,
      "grad_norm": 0.01795532926917076,
      "learning_rate": 0.00017463414634146342,
      "loss": 0.001,
      "step": 157
    },
    {
      "epoch": 0.25691056910569104,
      "grad_norm": 0.0567910261452198,
      "learning_rate": 0.00017447154471544716,
      "loss": 0.004,
      "step": 158
    },
    {
      "epoch": 0.25853658536585367,
      "grad_norm": 0.05590607598423958,
      "learning_rate": 0.0001743089430894309,
      "loss": 0.0079,
      "step": 159
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 0.034626446664333344,
      "learning_rate": 0.00017414634146341464,
      "loss": 0.0025,
      "step": 160
    },
    {
      "epoch": 0.26178861788617885,
      "grad_norm": 0.01331181637942791,
      "learning_rate": 0.00017398373983739838,
      "loss": 0.001,
      "step": 161
    },
    {
      "epoch": 0.2634146341463415,
      "grad_norm": 0.08110161870718002,
      "learning_rate": 0.00017382113821138212,
      "loss": 0.0105,
      "step": 162
    },
    {
      "epoch": 0.26504065040650404,
      "grad_norm": 0.06539864093065262,
      "learning_rate": 0.00017365853658536586,
      "loss": 0.0139,
      "step": 163
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.09633517265319824,
      "learning_rate": 0.0001734959349593496,
      "loss": 0.0129,
      "step": 164
    },
    {
      "epoch": 0.2682926829268293,
      "grad_norm": 0.05106749385595322,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.0071,
      "step": 165
    },
    {
      "epoch": 0.26991869918699185,
      "grad_norm": 0.0644776001572609,
      "learning_rate": 0.00017317073170731708,
      "loss": 0.0109,
      "step": 166
    },
    {
      "epoch": 0.27154471544715447,
      "grad_norm": 0.07961447536945343,
      "learning_rate": 0.00017300813008130081,
      "loss": 0.0173,
      "step": 167
    },
    {
      "epoch": 0.2731707317073171,
      "grad_norm": 0.054788727313280106,
      "learning_rate": 0.00017284552845528455,
      "loss": 0.0051,
      "step": 168
    },
    {
      "epoch": 0.27479674796747966,
      "grad_norm": 0.05671359598636627,
      "learning_rate": 0.0001726829268292683,
      "loss": 0.0103,
      "step": 169
    },
    {
      "epoch": 0.2764227642276423,
      "grad_norm": 0.10320157557725906,
      "learning_rate": 0.00017252032520325203,
      "loss": 0.0178,
      "step": 170
    },
    {
      "epoch": 0.2780487804878049,
      "grad_norm": 0.04981161653995514,
      "learning_rate": 0.0001723577235772358,
      "loss": 0.0031,
      "step": 171
    },
    {
      "epoch": 0.27967479674796747,
      "grad_norm": 0.05624835193157196,
      "learning_rate": 0.0001721951219512195,
      "loss": 0.0116,
      "step": 172
    },
    {
      "epoch": 0.2813008130081301,
      "grad_norm": 0.06805776059627533,
      "learning_rate": 0.00017203252032520325,
      "loss": 0.0167,
      "step": 173
    },
    {
      "epoch": 0.28292682926829266,
      "grad_norm": 0.038390420377254486,
      "learning_rate": 0.00017186991869918701,
      "loss": 0.0036,
      "step": 174
    },
    {
      "epoch": 0.2845528455284553,
      "grad_norm": 0.033778928220272064,
      "learning_rate": 0.00017170731707317073,
      "loss": 0.0052,
      "step": 175
    },
    {
      "epoch": 0.2861788617886179,
      "grad_norm": 0.023776765912771225,
      "learning_rate": 0.00017154471544715446,
      "loss": 0.002,
      "step": 176
    },
    {
      "epoch": 0.28780487804878047,
      "grad_norm": 0.07257568836212158,
      "learning_rate": 0.00017138211382113823,
      "loss": 0.0103,
      "step": 177
    },
    {
      "epoch": 0.2894308943089431,
      "grad_norm": 0.06379688531160355,
      "learning_rate": 0.00017121951219512194,
      "loss": 0.0099,
      "step": 178
    },
    {
      "epoch": 0.2910569105691057,
      "grad_norm": 0.18147827684879303,
      "learning_rate": 0.00017105691056910568,
      "loss": 0.0264,
      "step": 179
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 0.020248055458068848,
      "learning_rate": 0.00017089430894308945,
      "loss": 0.001,
      "step": 180
    },
    {
      "epoch": 0.2943089430894309,
      "grad_norm": 0.031300272792577744,
      "learning_rate": 0.0001707317073170732,
      "loss": 0.0031,
      "step": 181
    },
    {
      "epoch": 0.2959349593495935,
      "grad_norm": 0.046243537217378616,
      "learning_rate": 0.00017056910569105693,
      "loss": 0.0079,
      "step": 182
    },
    {
      "epoch": 0.2975609756097561,
      "grad_norm": 0.02697622962296009,
      "learning_rate": 0.00017040650406504066,
      "loss": 0.0026,
      "step": 183
    },
    {
      "epoch": 0.2991869918699187,
      "grad_norm": 0.07677304744720459,
      "learning_rate": 0.0001702439024390244,
      "loss": 0.013,
      "step": 184
    },
    {
      "epoch": 0.3008130081300813,
      "grad_norm": 0.050438620150089264,
      "learning_rate": 0.00017008130081300814,
      "loss": 0.0064,
      "step": 185
    },
    {
      "epoch": 0.3024390243902439,
      "grad_norm": 0.04538458585739136,
      "learning_rate": 0.00016991869918699188,
      "loss": 0.004,
      "step": 186
    },
    {
      "epoch": 0.3040650406504065,
      "grad_norm": 0.0314907431602478,
      "learning_rate": 0.00016975609756097562,
      "loss": 0.0038,
      "step": 187
    },
    {
      "epoch": 0.3056910569105691,
      "grad_norm": 0.0368216447532177,
      "learning_rate": 0.00016959349593495936,
      "loss": 0.004,
      "step": 188
    },
    {
      "epoch": 0.3073170731707317,
      "grad_norm": 0.047670796513557434,
      "learning_rate": 0.0001694308943089431,
      "loss": 0.0062,
      "step": 189
    },
    {
      "epoch": 0.3089430894308943,
      "grad_norm": 0.03753972798585892,
      "learning_rate": 0.00016926829268292684,
      "loss": 0.0027,
      "step": 190
    },
    {
      "epoch": 0.3105691056910569,
      "grad_norm": 0.015697093680500984,
      "learning_rate": 0.00016910569105691058,
      "loss": 0.0008,
      "step": 191
    },
    {
      "epoch": 0.3121951219512195,
      "grad_norm": 0.08974073827266693,
      "learning_rate": 0.00016894308943089432,
      "loss": 0.0055,
      "step": 192
    },
    {
      "epoch": 0.31382113821138213,
      "grad_norm": 0.04938000068068504,
      "learning_rate": 0.00016878048780487805,
      "loss": 0.0061,
      "step": 193
    },
    {
      "epoch": 0.3154471544715447,
      "grad_norm": 0.07383699715137482,
      "learning_rate": 0.0001686178861788618,
      "loss": 0.0083,
      "step": 194
    },
    {
      "epoch": 0.3170731707317073,
      "grad_norm": 0.03413151949644089,
      "learning_rate": 0.00016845528455284553,
      "loss": 0.0024,
      "step": 195
    },
    {
      "epoch": 0.31869918699186994,
      "grad_norm": 0.07202206552028656,
      "learning_rate": 0.00016829268292682927,
      "loss": 0.0079,
      "step": 196
    },
    {
      "epoch": 0.3203252032520325,
      "grad_norm": 0.06604200601577759,
      "learning_rate": 0.00016813008130081304,
      "loss": 0.0072,
      "step": 197
    },
    {
      "epoch": 0.32195121951219513,
      "grad_norm": 0.042259104549884796,
      "learning_rate": 0.00016796747967479675,
      "loss": 0.0032,
      "step": 198
    },
    {
      "epoch": 0.3235772357723577,
      "grad_norm": 0.03552434965968132,
      "learning_rate": 0.0001678048780487805,
      "loss": 0.002,
      "step": 199
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 0.031681936234235764,
      "learning_rate": 0.00016764227642276425,
      "loss": 0.0053,
      "step": 200
    },
    {
      "epoch": 0.32682926829268294,
      "grad_norm": 0.11483332514762878,
      "learning_rate": 0.00016747967479674797,
      "loss": 0.0144,
      "step": 201
    },
    {
      "epoch": 0.3284552845528455,
      "grad_norm": 0.04197550192475319,
      "learning_rate": 0.0001673170731707317,
      "loss": 0.0042,
      "step": 202
    },
    {
      "epoch": 0.3300813008130081,
      "grad_norm": 0.06192179024219513,
      "learning_rate": 0.00016715447154471547,
      "loss": 0.0078,
      "step": 203
    },
    {
      "epoch": 0.33170731707317075,
      "grad_norm": 0.05342299863696098,
      "learning_rate": 0.00016699186991869918,
      "loss": 0.007,
      "step": 204
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.04369426146149635,
      "learning_rate": 0.00016682926829268292,
      "loss": 0.0012,
      "step": 205
    },
    {
      "epoch": 0.33495934959349594,
      "grad_norm": 0.06268668919801712,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.0106,
      "step": 206
    },
    {
      "epoch": 0.33658536585365856,
      "grad_norm": 0.07172930985689163,
      "learning_rate": 0.00016650406504065043,
      "loss": 0.0055,
      "step": 207
    },
    {
      "epoch": 0.3382113821138211,
      "grad_norm": 0.06571360677480698,
      "learning_rate": 0.00016634146341463414,
      "loss": 0.0079,
      "step": 208
    },
    {
      "epoch": 0.33983739837398375,
      "grad_norm": 0.03303678333759308,
      "learning_rate": 0.0001661788617886179,
      "loss": 0.0024,
      "step": 209
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 0.02695191092789173,
      "learning_rate": 0.00016601626016260164,
      "loss": 0.0023,
      "step": 210
    },
    {
      "epoch": 0.34308943089430893,
      "grad_norm": 0.04864557832479477,
      "learning_rate": 0.00016585365853658536,
      "loss": 0.0049,
      "step": 211
    },
    {
      "epoch": 0.34471544715447155,
      "grad_norm": 0.036018937826156616,
      "learning_rate": 0.00016569105691056912,
      "loss": 0.0022,
      "step": 212
    },
    {
      "epoch": 0.3463414634146341,
      "grad_norm": 0.05603440850973129,
      "learning_rate": 0.00016552845528455286,
      "loss": 0.0017,
      "step": 213
    },
    {
      "epoch": 0.34796747967479674,
      "grad_norm": 0.032489605247974396,
      "learning_rate": 0.00016536585365853657,
      "loss": 0.0041,
      "step": 214
    },
    {
      "epoch": 0.34959349593495936,
      "grad_norm": 0.03976500406861305,
      "learning_rate": 0.00016520325203252034,
      "loss": 0.0042,
      "step": 215
    },
    {
      "epoch": 0.35121951219512193,
      "grad_norm": 0.0374918095767498,
      "learning_rate": 0.00016504065040650408,
      "loss": 0.0045,
      "step": 216
    },
    {
      "epoch": 0.35284552845528455,
      "grad_norm": 0.05623650550842285,
      "learning_rate": 0.00016487804878048782,
      "loss": 0.0028,
      "step": 217
    },
    {
      "epoch": 0.3544715447154472,
      "grad_norm": 0.07391159236431122,
      "learning_rate": 0.00016471544715447156,
      "loss": 0.011,
      "step": 218
    },
    {
      "epoch": 0.35609756097560974,
      "grad_norm": 0.04198432341217995,
      "learning_rate": 0.0001645528455284553,
      "loss": 0.0043,
      "step": 219
    },
    {
      "epoch": 0.35772357723577236,
      "grad_norm": 0.06713885068893433,
      "learning_rate": 0.00016439024390243903,
      "loss": 0.0065,
      "step": 220
    },
    {
      "epoch": 0.359349593495935,
      "grad_norm": 0.07107122987508774,
      "learning_rate": 0.00016422764227642277,
      "loss": 0.0108,
      "step": 221
    },
    {
      "epoch": 0.36097560975609755,
      "grad_norm": 0.039641816169023514,
      "learning_rate": 0.0001640650406504065,
      "loss": 0.0049,
      "step": 222
    },
    {
      "epoch": 0.36260162601626017,
      "grad_norm": 0.0785885602235794,
      "learning_rate": 0.00016390243902439025,
      "loss": 0.0139,
      "step": 223
    },
    {
      "epoch": 0.3642276422764228,
      "grad_norm": 0.05685996636748314,
      "learning_rate": 0.000163739837398374,
      "loss": 0.0045,
      "step": 224
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 0.042499840259552,
      "learning_rate": 0.00016357723577235773,
      "loss": 0.004,
      "step": 225
    },
    {
      "epoch": 0.367479674796748,
      "grad_norm": 0.03637044131755829,
      "learning_rate": 0.00016341463414634147,
      "loss": 0.0027,
      "step": 226
    },
    {
      "epoch": 0.36910569105691055,
      "grad_norm": 0.03910870477557182,
      "learning_rate": 0.0001632520325203252,
      "loss": 0.0044,
      "step": 227
    },
    {
      "epoch": 0.37073170731707317,
      "grad_norm": 0.06795119494199753,
      "learning_rate": 0.00016308943089430895,
      "loss": 0.0086,
      "step": 228
    },
    {
      "epoch": 0.3723577235772358,
      "grad_norm": 0.08367984741926193,
      "learning_rate": 0.00016292682926829268,
      "loss": 0.0124,
      "step": 229
    },
    {
      "epoch": 0.37398373983739835,
      "grad_norm": 0.03947695717215538,
      "learning_rate": 0.00016276422764227642,
      "loss": 0.0019,
      "step": 230
    },
    {
      "epoch": 0.375609756097561,
      "grad_norm": 0.05513155087828636,
      "learning_rate": 0.00016260162601626016,
      "loss": 0.0044,
      "step": 231
    },
    {
      "epoch": 0.3772357723577236,
      "grad_norm": 0.05772537365555763,
      "learning_rate": 0.00016243902439024393,
      "loss": 0.0065,
      "step": 232
    },
    {
      "epoch": 0.37886178861788616,
      "grad_norm": 0.0595276914536953,
      "learning_rate": 0.00016227642276422764,
      "loss": 0.0079,
      "step": 233
    },
    {
      "epoch": 0.3804878048780488,
      "grad_norm": 0.041617315262556076,
      "learning_rate": 0.00016211382113821138,
      "loss": 0.0056,
      "step": 234
    },
    {
      "epoch": 0.3821138211382114,
      "grad_norm": 0.008105425164103508,
      "learning_rate": 0.00016195121951219515,
      "loss": 0.0005,
      "step": 235
    },
    {
      "epoch": 0.383739837398374,
      "grad_norm": 0.024288572371006012,
      "learning_rate": 0.00016178861788617888,
      "loss": 0.0016,
      "step": 236
    },
    {
      "epoch": 0.3853658536585366,
      "grad_norm": 0.05020412430167198,
      "learning_rate": 0.0001616260162601626,
      "loss": 0.0032,
      "step": 237
    },
    {
      "epoch": 0.38699186991869916,
      "grad_norm": 0.040060855448246,
      "learning_rate": 0.00016146341463414636,
      "loss": 0.0033,
      "step": 238
    },
    {
      "epoch": 0.3886178861788618,
      "grad_norm": 0.03679739311337471,
      "learning_rate": 0.0001613008130081301,
      "loss": 0.0019,
      "step": 239
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 0.03246524557471275,
      "learning_rate": 0.0001611382113821138,
      "loss": 0.0017,
      "step": 240
    },
    {
      "epoch": 0.39186991869918697,
      "grad_norm": 0.053959108889102936,
      "learning_rate": 0.00016097560975609758,
      "loss": 0.0041,
      "step": 241
    },
    {
      "epoch": 0.3934959349593496,
      "grad_norm": 0.05499345809221268,
      "learning_rate": 0.00016081300813008132,
      "loss": 0.0082,
      "step": 242
    },
    {
      "epoch": 0.3951219512195122,
      "grad_norm": 0.09415078163146973,
      "learning_rate": 0.00016065040650406503,
      "loss": 0.0065,
      "step": 243
    },
    {
      "epoch": 0.3967479674796748,
      "grad_norm": 0.026768893003463745,
      "learning_rate": 0.0001604878048780488,
      "loss": 0.0021,
      "step": 244
    },
    {
      "epoch": 0.3983739837398374,
      "grad_norm": 0.059897903352975845,
      "learning_rate": 0.00016032520325203254,
      "loss": 0.0034,
      "step": 245
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.03809318691492081,
      "learning_rate": 0.00016016260162601627,
      "loss": 0.0011,
      "step": 246
    },
    {
      "epoch": 0.4016260162601626,
      "grad_norm": 0.04290259629487991,
      "learning_rate": 0.00016,
      "loss": 0.004,
      "step": 247
    },
    {
      "epoch": 0.4032520325203252,
      "grad_norm": 0.03837749734520912,
      "learning_rate": 0.00015983739837398375,
      "loss": 0.0038,
      "step": 248
    },
    {
      "epoch": 0.40487804878048783,
      "grad_norm": 0.03902510181069374,
      "learning_rate": 0.0001596747967479675,
      "loss": 0.0051,
      "step": 249
    },
    {
      "epoch": 0.4065040650406504,
      "grad_norm": 0.02119496837258339,
      "learning_rate": 0.00015951219512195123,
      "loss": 0.0016,
      "step": 250
    },
    {
      "epoch": 0.408130081300813,
      "grad_norm": 0.059587541967630386,
      "learning_rate": 0.00015934959349593497,
      "loss": 0.007,
      "step": 251
    },
    {
      "epoch": 0.4097560975609756,
      "grad_norm": 0.03809100016951561,
      "learning_rate": 0.0001591869918699187,
      "loss": 0.0028,
      "step": 252
    },
    {
      "epoch": 0.4113821138211382,
      "grad_norm": 0.008246686309576035,
      "learning_rate": 0.00015902439024390245,
      "loss": 0.0005,
      "step": 253
    },
    {
      "epoch": 0.41300813008130083,
      "grad_norm": 0.06121211126446724,
      "learning_rate": 0.00015886178861788619,
      "loss": 0.0074,
      "step": 254
    },
    {
      "epoch": 0.4146341463414634,
      "grad_norm": 0.03664132207632065,
      "learning_rate": 0.00015869918699186992,
      "loss": 0.0019,
      "step": 255
    },
    {
      "epoch": 0.416260162601626,
      "grad_norm": 0.09013177454471588,
      "learning_rate": 0.00015853658536585366,
      "loss": 0.0094,
      "step": 256
    },
    {
      "epoch": 0.41788617886178864,
      "grad_norm": 0.04787154495716095,
      "learning_rate": 0.0001583739837398374,
      "loss": 0.0088,
      "step": 257
    },
    {
      "epoch": 0.4195121951219512,
      "grad_norm": 0.04078535735607147,
      "learning_rate": 0.00015821138211382114,
      "loss": 0.0047,
      "step": 258
    },
    {
      "epoch": 0.4211382113821138,
      "grad_norm": 0.032567497342824936,
      "learning_rate": 0.00015804878048780488,
      "loss": 0.0022,
      "step": 259
    },
    {
      "epoch": 0.42276422764227645,
      "grad_norm": 0.056626107543706894,
      "learning_rate": 0.00015788617886178862,
      "loss": 0.0046,
      "step": 260
    },
    {
      "epoch": 0.424390243902439,
      "grad_norm": 0.03480905294418335,
      "learning_rate": 0.00015772357723577236,
      "loss": 0.0035,
      "step": 261
    },
    {
      "epoch": 0.42601626016260163,
      "grad_norm": 0.07597535103559494,
      "learning_rate": 0.00015756097560975612,
      "loss": 0.0107,
      "step": 262
    },
    {
      "epoch": 0.4276422764227642,
      "grad_norm": 0.06572912633419037,
      "learning_rate": 0.00015739837398373984,
      "loss": 0.0043,
      "step": 263
    },
    {
      "epoch": 0.4292682926829268,
      "grad_norm": 0.05102035775780678,
      "learning_rate": 0.00015723577235772358,
      "loss": 0.008,
      "step": 264
    },
    {
      "epoch": 0.43089430894308944,
      "grad_norm": 0.028546195477247238,
      "learning_rate": 0.00015707317073170734,
      "loss": 0.0018,
      "step": 265
    },
    {
      "epoch": 0.432520325203252,
      "grad_norm": 0.029671359807252884,
      "learning_rate": 0.00015691056910569105,
      "loss": 0.0026,
      "step": 266
    },
    {
      "epoch": 0.43414634146341463,
      "grad_norm": 0.06173054128885269,
      "learning_rate": 0.0001567479674796748,
      "loss": 0.0069,
      "step": 267
    },
    {
      "epoch": 0.43577235772357725,
      "grad_norm": 0.05086858198046684,
      "learning_rate": 0.00015658536585365856,
      "loss": 0.0063,
      "step": 268
    },
    {
      "epoch": 0.4373983739837398,
      "grad_norm": 0.02125905267894268,
      "learning_rate": 0.00015642276422764227,
      "loss": 0.0014,
      "step": 269
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 0.04446115717291832,
      "learning_rate": 0.000156260162601626,
      "loss": 0.0046,
      "step": 270
    },
    {
      "epoch": 0.44065040650406506,
      "grad_norm": 0.008476979099214077,
      "learning_rate": 0.00015609756097560978,
      "loss": 0.0005,
      "step": 271
    },
    {
      "epoch": 0.44227642276422763,
      "grad_norm": 0.052454475313425064,
      "learning_rate": 0.00015593495934959351,
      "loss": 0.0064,
      "step": 272
    },
    {
      "epoch": 0.44390243902439025,
      "grad_norm": 0.03743799030780792,
      "learning_rate": 0.00015577235772357723,
      "loss": 0.0012,
      "step": 273
    },
    {
      "epoch": 0.44552845528455287,
      "grad_norm": 0.04814663901925087,
      "learning_rate": 0.000155609756097561,
      "loss": 0.0046,
      "step": 274
    },
    {
      "epoch": 0.44715447154471544,
      "grad_norm": 0.09085031598806381,
      "learning_rate": 0.00015544715447154473,
      "loss": 0.0076,
      "step": 275
    },
    {
      "epoch": 0.44878048780487806,
      "grad_norm": 0.05821633338928223,
      "learning_rate": 0.00015528455284552844,
      "loss": 0.0049,
      "step": 276
    },
    {
      "epoch": 0.4504065040650406,
      "grad_norm": 0.02842341549694538,
      "learning_rate": 0.0001551219512195122,
      "loss": 0.0019,
      "step": 277
    },
    {
      "epoch": 0.45203252032520325,
      "grad_norm": 0.047390248626470566,
      "learning_rate": 0.00015495934959349595,
      "loss": 0.0017,
      "step": 278
    },
    {
      "epoch": 0.45365853658536587,
      "grad_norm": 0.03129230812191963,
      "learning_rate": 0.00015479674796747966,
      "loss": 0.0025,
      "step": 279
    },
    {
      "epoch": 0.45528455284552843,
      "grad_norm": 0.03706726059317589,
      "learning_rate": 0.00015463414634146343,
      "loss": 0.0025,
      "step": 280
    },
    {
      "epoch": 0.45691056910569106,
      "grad_norm": 0.02136380784213543,
      "learning_rate": 0.00015447154471544717,
      "loss": 0.0011,
      "step": 281
    },
    {
      "epoch": 0.4585365853658537,
      "grad_norm": 0.041168052703142166,
      "learning_rate": 0.0001543089430894309,
      "loss": 0.0032,
      "step": 282
    },
    {
      "epoch": 0.46016260162601624,
      "grad_norm": 0.06333127617835999,
      "learning_rate": 0.00015414634146341464,
      "loss": 0.006,
      "step": 283
    },
    {
      "epoch": 0.46178861788617886,
      "grad_norm": 0.04016856849193573,
      "learning_rate": 0.00015398373983739838,
      "loss": 0.0035,
      "step": 284
    },
    {
      "epoch": 0.4634146341463415,
      "grad_norm": 0.05800430849194527,
      "learning_rate": 0.00015382113821138212,
      "loss": 0.004,
      "step": 285
    },
    {
      "epoch": 0.46504065040650405,
      "grad_norm": 0.031819798052310944,
      "learning_rate": 0.00015365853658536586,
      "loss": 0.0007,
      "step": 286
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.06838291138410568,
      "learning_rate": 0.0001534959349593496,
      "loss": 0.0055,
      "step": 287
    },
    {
      "epoch": 0.4682926829268293,
      "grad_norm": 0.04758811369538307,
      "learning_rate": 0.00015333333333333334,
      "loss": 0.0039,
      "step": 288
    },
    {
      "epoch": 0.46991869918699186,
      "grad_norm": 0.042291514575481415,
      "learning_rate": 0.00015317073170731708,
      "loss": 0.0034,
      "step": 289
    },
    {
      "epoch": 0.4715447154471545,
      "grad_norm": 0.038803260773420334,
      "learning_rate": 0.00015300813008130082,
      "loss": 0.0026,
      "step": 290
    },
    {
      "epoch": 0.47317073170731705,
      "grad_norm": 0.036684513092041016,
      "learning_rate": 0.00015284552845528458,
      "loss": 0.0034,
      "step": 291
    },
    {
      "epoch": 0.47479674796747967,
      "grad_norm": 0.0919487252831459,
      "learning_rate": 0.0001526829268292683,
      "loss": 0.0118,
      "step": 292
    },
    {
      "epoch": 0.4764227642276423,
      "grad_norm": 0.018654925748705864,
      "learning_rate": 0.00015252032520325203,
      "loss": 0.0006,
      "step": 293
    },
    {
      "epoch": 0.47804878048780486,
      "grad_norm": 0.04378391429781914,
      "learning_rate": 0.0001523577235772358,
      "loss": 0.0028,
      "step": 294
    },
    {
      "epoch": 0.4796747967479675,
      "grad_norm": 0.021690063178539276,
      "learning_rate": 0.0001521951219512195,
      "loss": 0.0014,
      "step": 295
    },
    {
      "epoch": 0.4813008130081301,
      "grad_norm": 0.05425732582807541,
      "learning_rate": 0.00015203252032520325,
      "loss": 0.0046,
      "step": 296
    },
    {
      "epoch": 0.48292682926829267,
      "grad_norm": 0.09478655457496643,
      "learning_rate": 0.00015186991869918702,
      "loss": 0.0062,
      "step": 297
    },
    {
      "epoch": 0.4845528455284553,
      "grad_norm": 0.08030550926923752,
      "learning_rate": 0.00015170731707317073,
      "loss": 0.0071,
      "step": 298
    },
    {
      "epoch": 0.4861788617886179,
      "grad_norm": 0.09851697832345963,
      "learning_rate": 0.00015154471544715447,
      "loss": 0.0058,
      "step": 299
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 0.08546419441699982,
      "learning_rate": 0.00015138211382113823,
      "loss": 0.0097,
      "step": 300
    },
    {
      "epoch": 0.4894308943089431,
      "grad_norm": 0.02204154245555401,
      "learning_rate": 0.00015121951219512197,
      "loss": 0.0013,
      "step": 301
    },
    {
      "epoch": 0.49105691056910566,
      "grad_norm": 0.02301298826932907,
      "learning_rate": 0.00015105691056910568,
      "loss": 0.0012,
      "step": 302
    },
    {
      "epoch": 0.4926829268292683,
      "grad_norm": 0.012738016434013844,
      "learning_rate": 0.00015089430894308945,
      "loss": 0.0006,
      "step": 303
    },
    {
      "epoch": 0.4943089430894309,
      "grad_norm": 0.02279171161353588,
      "learning_rate": 0.0001507317073170732,
      "loss": 0.0013,
      "step": 304
    },
    {
      "epoch": 0.4959349593495935,
      "grad_norm": 0.05987592041492462,
      "learning_rate": 0.0001505691056910569,
      "loss": 0.0053,
      "step": 305
    },
    {
      "epoch": 0.4975609756097561,
      "grad_norm": 0.05977104604244232,
      "learning_rate": 0.00015040650406504067,
      "loss": 0.0025,
      "step": 306
    },
    {
      "epoch": 0.4991869918699187,
      "grad_norm": 0.04674098640680313,
      "learning_rate": 0.0001502439024390244,
      "loss": 0.0038,
      "step": 307
    },
    {
      "epoch": 0.5008130081300813,
      "grad_norm": 0.048203691840171814,
      "learning_rate": 0.00015008130081300812,
      "loss": 0.0023,
      "step": 308
    },
    {
      "epoch": 0.5024390243902439,
      "grad_norm": 0.02530977874994278,
      "learning_rate": 0.00014991869918699188,
      "loss": 0.0021,
      "step": 309
    },
    {
      "epoch": 0.5040650406504065,
      "grad_norm": 0.024972444400191307,
      "learning_rate": 0.00014975609756097562,
      "loss": 0.0013,
      "step": 310
    },
    {
      "epoch": 0.5056910569105691,
      "grad_norm": 0.004017941653728485,
      "learning_rate": 0.00014959349593495936,
      "loss": 0.0003,
      "step": 311
    },
    {
      "epoch": 0.5073170731707317,
      "grad_norm": 0.05017799884080887,
      "learning_rate": 0.0001494308943089431,
      "loss": 0.0045,
      "step": 312
    },
    {
      "epoch": 0.5089430894308943,
      "grad_norm": 0.05301947519183159,
      "learning_rate": 0.00014926829268292684,
      "loss": 0.0036,
      "step": 313
    },
    {
      "epoch": 0.510569105691057,
      "grad_norm": 0.05363637953996658,
      "learning_rate": 0.00014910569105691058,
      "loss": 0.0044,
      "step": 314
    },
    {
      "epoch": 0.5121951219512195,
      "grad_norm": 0.03636287897825241,
      "learning_rate": 0.00014894308943089432,
      "loss": 0.0022,
      "step": 315
    },
    {
      "epoch": 0.5138211382113821,
      "grad_norm": 0.028038306161761284,
      "learning_rate": 0.00014878048780487806,
      "loss": 0.0021,
      "step": 316
    },
    {
      "epoch": 0.5154471544715448,
      "grad_norm": 0.03675807639956474,
      "learning_rate": 0.0001486178861788618,
      "loss": 0.0007,
      "step": 317
    },
    {
      "epoch": 0.5170731707317073,
      "grad_norm": 0.05961129069328308,
      "learning_rate": 0.00014845528455284553,
      "loss": 0.0036,
      "step": 318
    },
    {
      "epoch": 0.5186991869918699,
      "grad_norm": 0.018406610935926437,
      "learning_rate": 0.00014829268292682927,
      "loss": 0.001,
      "step": 319
    },
    {
      "epoch": 0.5203252032520326,
      "grad_norm": 0.024211697280406952,
      "learning_rate": 0.000148130081300813,
      "loss": 0.0007,
      "step": 320
    },
    {
      "epoch": 0.5219512195121951,
      "grad_norm": 0.0223006010055542,
      "learning_rate": 0.00014796747967479675,
      "loss": 0.0009,
      "step": 321
    },
    {
      "epoch": 0.5235772357723577,
      "grad_norm": 0.025664936751127243,
      "learning_rate": 0.0001478048780487805,
      "loss": 0.0015,
      "step": 322
    },
    {
      "epoch": 0.5252032520325203,
      "grad_norm": 0.07756369560956955,
      "learning_rate": 0.00014764227642276423,
      "loss": 0.0026,
      "step": 323
    },
    {
      "epoch": 0.526829268292683,
      "grad_norm": 0.04779999703168869,
      "learning_rate": 0.00014747967479674797,
      "loss": 0.0024,
      "step": 324
    },
    {
      "epoch": 0.5284552845528455,
      "grad_norm": 0.0432940237224102,
      "learning_rate": 0.0001473170731707317,
      "loss": 0.0015,
      "step": 325
    },
    {
      "epoch": 0.5300813008130081,
      "grad_norm": 0.07308786362409592,
      "learning_rate": 0.00014715447154471545,
      "loss": 0.0083,
      "step": 326
    },
    {
      "epoch": 0.5317073170731708,
      "grad_norm": 0.08420983701944351,
      "learning_rate": 0.0001469918699186992,
      "loss": 0.0061,
      "step": 327
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.05637168511748314,
      "learning_rate": 0.00014682926829268292,
      "loss": 0.0037,
      "step": 328
    },
    {
      "epoch": 0.5349593495934959,
      "grad_norm": 0.011459868401288986,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.0004,
      "step": 329
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 0.04229405149817467,
      "learning_rate": 0.00014650406504065043,
      "loss": 0.0052,
      "step": 330
    },
    {
      "epoch": 0.5382113821138211,
      "grad_norm": 0.08318997919559479,
      "learning_rate": 0.00014634146341463414,
      "loss": 0.012,
      "step": 331
    },
    {
      "epoch": 0.5398373983739837,
      "grad_norm": 0.027448266744613647,
      "learning_rate": 0.00014617886178861788,
      "loss": 0.001,
      "step": 332
    },
    {
      "epoch": 0.5414634146341464,
      "grad_norm": 0.03405987471342087,
      "learning_rate": 0.00014601626016260165,
      "loss": 0.002,
      "step": 333
    },
    {
      "epoch": 0.5430894308943089,
      "grad_norm": 0.032117780297994614,
      "learning_rate": 0.00014585365853658536,
      "loss": 0.0005,
      "step": 334
    },
    {
      "epoch": 0.5447154471544715,
      "grad_norm": 0.022048333659768105,
      "learning_rate": 0.00014569105691056912,
      "loss": 0.0005,
      "step": 335
    },
    {
      "epoch": 0.5463414634146342,
      "grad_norm": 0.043050795793533325,
      "learning_rate": 0.00014552845528455286,
      "loss": 0.0036,
      "step": 336
    },
    {
      "epoch": 0.5479674796747968,
      "grad_norm": 0.02210851013660431,
      "learning_rate": 0.0001453658536585366,
      "loss": 0.0011,
      "step": 337
    },
    {
      "epoch": 0.5495934959349593,
      "grad_norm": 0.08371196687221527,
      "learning_rate": 0.00014520325203252034,
      "loss": 0.0108,
      "step": 338
    },
    {
      "epoch": 0.551219512195122,
      "grad_norm": 0.03730907291173935,
      "learning_rate": 0.00014504065040650408,
      "loss": 0.0023,
      "step": 339
    },
    {
      "epoch": 0.5528455284552846,
      "grad_norm": 0.04696402698755264,
      "learning_rate": 0.00014487804878048782,
      "loss": 0.0033,
      "step": 340
    },
    {
      "epoch": 0.5544715447154471,
      "grad_norm": 0.01265543233603239,
      "learning_rate": 0.00014471544715447156,
      "loss": 0.0005,
      "step": 341
    },
    {
      "epoch": 0.5560975609756098,
      "grad_norm": 0.02745240554213524,
      "learning_rate": 0.0001445528455284553,
      "loss": 0.0026,
      "step": 342
    },
    {
      "epoch": 0.5577235772357724,
      "grad_norm": 0.04942648112773895,
      "learning_rate": 0.00014439024390243904,
      "loss": 0.0037,
      "step": 343
    },
    {
      "epoch": 0.5593495934959349,
      "grad_norm": 0.008476071991026402,
      "learning_rate": 0.00014422764227642277,
      "loss": 0.0004,
      "step": 344
    },
    {
      "epoch": 0.5609756097560976,
      "grad_norm": 0.050052739679813385,
      "learning_rate": 0.0001440650406504065,
      "loss": 0.0032,
      "step": 345
    },
    {
      "epoch": 0.5626016260162602,
      "grad_norm": 0.05567610263824463,
      "learning_rate": 0.00014390243902439025,
      "loss": 0.0049,
      "step": 346
    },
    {
      "epoch": 0.5642276422764227,
      "grad_norm": 0.04730698838829994,
      "learning_rate": 0.000143739837398374,
      "loss": 0.001,
      "step": 347
    },
    {
      "epoch": 0.5658536585365853,
      "grad_norm": 0.06813085824251175,
      "learning_rate": 0.00014357723577235773,
      "loss": 0.0065,
      "step": 348
    },
    {
      "epoch": 0.567479674796748,
      "grad_norm": 0.027026068419218063,
      "learning_rate": 0.00014341463414634147,
      "loss": 0.002,
      "step": 349
    },
    {
      "epoch": 0.5691056910569106,
      "grad_norm": 0.008108194917440414,
      "learning_rate": 0.0001432520325203252,
      "loss": 0.0003,
      "step": 350
    },
    {
      "epoch": 0.5707317073170731,
      "grad_norm": 0.021709604188799858,
      "learning_rate": 0.00014308943089430895,
      "loss": 0.0004,
      "step": 351
    },
    {
      "epoch": 0.5723577235772358,
      "grad_norm": 0.04537516087293625,
      "learning_rate": 0.00014292682926829269,
      "loss": 0.0016,
      "step": 352
    },
    {
      "epoch": 0.5739837398373984,
      "grad_norm": 0.06790541112422943,
      "learning_rate": 0.00014276422764227643,
      "loss": 0.0039,
      "step": 353
    },
    {
      "epoch": 0.5756097560975609,
      "grad_norm": 0.05915749445557594,
      "learning_rate": 0.00014260162601626016,
      "loss": 0.0066,
      "step": 354
    },
    {
      "epoch": 0.5772357723577236,
      "grad_norm": 0.05809753015637398,
      "learning_rate": 0.0001424390243902439,
      "loss": 0.0055,
      "step": 355
    },
    {
      "epoch": 0.5788617886178862,
      "grad_norm": 0.06579986214637756,
      "learning_rate": 0.00014227642276422767,
      "loss": 0.0035,
      "step": 356
    },
    {
      "epoch": 0.5804878048780487,
      "grad_norm": 0.019875263795256615,
      "learning_rate": 0.00014211382113821138,
      "loss": 0.0011,
      "step": 357
    },
    {
      "epoch": 0.5821138211382114,
      "grad_norm": 0.029589928686618805,
      "learning_rate": 0.00014195121951219512,
      "loss": 0.0018,
      "step": 358
    },
    {
      "epoch": 0.583739837398374,
      "grad_norm": 0.0400996096432209,
      "learning_rate": 0.00014178861788617889,
      "loss": 0.0021,
      "step": 359
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 0.05294010043144226,
      "learning_rate": 0.0001416260162601626,
      "loss": 0.0025,
      "step": 360
    },
    {
      "epoch": 0.5869918699186992,
      "grad_norm": 0.049299150705337524,
      "learning_rate": 0.00014146341463414634,
      "loss": 0.0026,
      "step": 361
    },
    {
      "epoch": 0.5886178861788618,
      "grad_norm": 0.04363460838794708,
      "learning_rate": 0.0001413008130081301,
      "loss": 0.0023,
      "step": 362
    },
    {
      "epoch": 0.5902439024390244,
      "grad_norm": 0.030172506347298622,
      "learning_rate": 0.00014113821138211381,
      "loss": 0.0019,
      "step": 363
    },
    {
      "epoch": 0.591869918699187,
      "grad_norm": 0.02734779193997383,
      "learning_rate": 0.00014097560975609755,
      "loss": 0.0007,
      "step": 364
    },
    {
      "epoch": 0.5934959349593496,
      "grad_norm": 0.03403576835989952,
      "learning_rate": 0.00014081300813008132,
      "loss": 0.0009,
      "step": 365
    },
    {
      "epoch": 0.5951219512195122,
      "grad_norm": 0.011000935919582844,
      "learning_rate": 0.00014065040650406506,
      "loss": 0.0003,
      "step": 366
    },
    {
      "epoch": 0.5967479674796748,
      "grad_norm": 0.02606494165956974,
      "learning_rate": 0.00014048780487804877,
      "loss": 0.001,
      "step": 367
    },
    {
      "epoch": 0.5983739837398374,
      "grad_norm": 0.02679901011288166,
      "learning_rate": 0.00014032520325203254,
      "loss": 0.0018,
      "step": 368
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.03595108911395073,
      "learning_rate": 0.00014016260162601628,
      "loss": 0.0018,
      "step": 369
    },
    {
      "epoch": 0.6016260162601627,
      "grad_norm": 0.05407336726784706,
      "learning_rate": 0.00014,
      "loss": 0.0026,
      "step": 370
    },
    {
      "epoch": 0.6032520325203252,
      "grad_norm": 0.030202345922589302,
      "learning_rate": 0.00013983739837398375,
      "loss": 0.0015,
      "step": 371
    },
    {
      "epoch": 0.6048780487804878,
      "grad_norm": 0.08269347995519638,
      "learning_rate": 0.0001396747967479675,
      "loss": 0.0057,
      "step": 372
    },
    {
      "epoch": 0.6065040650406504,
      "grad_norm": 0.015995895490050316,
      "learning_rate": 0.0001395121951219512,
      "loss": 0.0008,
      "step": 373
    },
    {
      "epoch": 0.608130081300813,
      "grad_norm": 0.07359179109334946,
      "learning_rate": 0.00013934959349593497,
      "loss": 0.0061,
      "step": 374
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 0.06993608921766281,
      "learning_rate": 0.0001391869918699187,
      "loss": 0.0076,
      "step": 375
    },
    {
      "epoch": 0.6113821138211382,
      "grad_norm": 0.043971456587314606,
      "learning_rate": 0.00013902439024390245,
      "loss": 0.0031,
      "step": 376
    },
    {
      "epoch": 0.6130081300813008,
      "grad_norm": 0.035905253142118454,
      "learning_rate": 0.0001388617886178862,
      "loss": 0.0015,
      "step": 377
    },
    {
      "epoch": 0.6146341463414634,
      "grad_norm": 0.04560420289635658,
      "learning_rate": 0.00013869918699186993,
      "loss": 0.0008,
      "step": 378
    },
    {
      "epoch": 0.616260162601626,
      "grad_norm": 0.009953766129910946,
      "learning_rate": 0.00013853658536585367,
      "loss": 0.0003,
      "step": 379
    },
    {
      "epoch": 0.6178861788617886,
      "grad_norm": 0.03815978392958641,
      "learning_rate": 0.0001383739837398374,
      "loss": 0.002,
      "step": 380
    },
    {
      "epoch": 0.6195121951219512,
      "grad_norm": 0.008472801186144352,
      "learning_rate": 0.00013821138211382114,
      "loss": 0.0004,
      "step": 381
    },
    {
      "epoch": 0.6211382113821138,
      "grad_norm": 0.06264209747314453,
      "learning_rate": 0.00013804878048780488,
      "loss": 0.006,
      "step": 382
    },
    {
      "epoch": 0.6227642276422765,
      "grad_norm": 0.006059740204364061,
      "learning_rate": 0.00013788617886178862,
      "loss": 0.0003,
      "step": 383
    },
    {
      "epoch": 0.624390243902439,
      "grad_norm": 0.04096158221364021,
      "learning_rate": 0.00013772357723577236,
      "loss": 0.0026,
      "step": 384
    },
    {
      "epoch": 0.6260162601626016,
      "grad_norm": 0.06331557035446167,
      "learning_rate": 0.0001375609756097561,
      "loss": 0.0074,
      "step": 385
    },
    {
      "epoch": 0.6276422764227643,
      "grad_norm": 0.05059782788157463,
      "learning_rate": 0.00013739837398373984,
      "loss": 0.0045,
      "step": 386
    },
    {
      "epoch": 0.6292682926829268,
      "grad_norm": 0.04114658758044243,
      "learning_rate": 0.00013723577235772358,
      "loss": 0.0015,
      "step": 387
    },
    {
      "epoch": 0.6308943089430894,
      "grad_norm": 0.04135163128376007,
      "learning_rate": 0.00013707317073170734,
      "loss": 0.0042,
      "step": 388
    },
    {
      "epoch": 0.6325203252032521,
      "grad_norm": 0.05520135536789894,
      "learning_rate": 0.00013691056910569106,
      "loss": 0.0055,
      "step": 389
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 0.021615512669086456,
      "learning_rate": 0.0001367479674796748,
      "loss": 0.0009,
      "step": 390
    },
    {
      "epoch": 0.6357723577235772,
      "grad_norm": 0.04998411238193512,
      "learning_rate": 0.00013658536585365856,
      "loss": 0.004,
      "step": 391
    },
    {
      "epoch": 0.6373983739837399,
      "grad_norm": 0.02765563502907753,
      "learning_rate": 0.00013642276422764227,
      "loss": 0.0015,
      "step": 392
    },
    {
      "epoch": 0.6390243902439025,
      "grad_norm": 0.027994269505143166,
      "learning_rate": 0.000136260162601626,
      "loss": 0.0015,
      "step": 393
    },
    {
      "epoch": 0.640650406504065,
      "grad_norm": 0.025956694036722183,
      "learning_rate": 0.00013609756097560978,
      "loss": 0.0015,
      "step": 394
    },
    {
      "epoch": 0.6422764227642277,
      "grad_norm": 0.045004431158304214,
      "learning_rate": 0.00013593495934959352,
      "loss": 0.0029,
      "step": 395
    },
    {
      "epoch": 0.6439024390243903,
      "grad_norm": 0.012444769963622093,
      "learning_rate": 0.00013577235772357723,
      "loss": 0.0006,
      "step": 396
    },
    {
      "epoch": 0.6455284552845528,
      "grad_norm": 0.007664056029170752,
      "learning_rate": 0.000135609756097561,
      "loss": 0.0003,
      "step": 397
    },
    {
      "epoch": 0.6471544715447154,
      "grad_norm": 0.07797913253307343,
      "learning_rate": 0.00013544715447154473,
      "loss": 0.0054,
      "step": 398
    },
    {
      "epoch": 0.6487804878048781,
      "grad_norm": 0.042493656277656555,
      "learning_rate": 0.00013528455284552844,
      "loss": 0.0017,
      "step": 399
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 0.06429119408130646,
      "learning_rate": 0.0001351219512195122,
      "loss": 0.0046,
      "step": 400
    },
    {
      "epoch": 0.6520325203252032,
      "grad_norm": 0.062457289546728134,
      "learning_rate": 0.00013495934959349595,
      "loss": 0.0039,
      "step": 401
    },
    {
      "epoch": 0.6536585365853659,
      "grad_norm": 0.043257568031549454,
      "learning_rate": 0.00013479674796747966,
      "loss": 0.0037,
      "step": 402
    },
    {
      "epoch": 0.6552845528455284,
      "grad_norm": 0.03842322528362274,
      "learning_rate": 0.00013463414634146343,
      "loss": 0.0016,
      "step": 403
    },
    {
      "epoch": 0.656910569105691,
      "grad_norm": 0.03916073963046074,
      "learning_rate": 0.00013447154471544717,
      "loss": 0.0013,
      "step": 404
    },
    {
      "epoch": 0.6585365853658537,
      "grad_norm": 0.024477075785398483,
      "learning_rate": 0.0001343089430894309,
      "loss": 0.0011,
      "step": 405
    },
    {
      "epoch": 0.6601626016260163,
      "grad_norm": 0.045938216149806976,
      "learning_rate": 0.00013414634146341464,
      "loss": 0.003,
      "step": 406
    },
    {
      "epoch": 0.6617886178861788,
      "grad_norm": 0.03394032269716263,
      "learning_rate": 0.00013398373983739838,
      "loss": 0.001,
      "step": 407
    },
    {
      "epoch": 0.6634146341463415,
      "grad_norm": 0.0203282181173563,
      "learning_rate": 0.00013382113821138212,
      "loss": 0.0009,
      "step": 408
    },
    {
      "epoch": 0.6650406504065041,
      "grad_norm": 0.014730355702340603,
      "learning_rate": 0.00013365853658536586,
      "loss": 0.0007,
      "step": 409
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.005610968917608261,
      "learning_rate": 0.0001334959349593496,
      "loss": 0.0003,
      "step": 410
    },
    {
      "epoch": 0.6682926829268293,
      "grad_norm": 0.05725783109664917,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.0043,
      "step": 411
    },
    {
      "epoch": 0.6699186991869919,
      "grad_norm": 0.00306798261590302,
      "learning_rate": 0.00013317073170731708,
      "loss": 0.0002,
      "step": 412
    },
    {
      "epoch": 0.6715447154471544,
      "grad_norm": 0.04567965492606163,
      "learning_rate": 0.00013300813008130082,
      "loss": 0.0025,
      "step": 413
    },
    {
      "epoch": 0.6731707317073171,
      "grad_norm": 0.0034624331165105104,
      "learning_rate": 0.00013284552845528456,
      "loss": 0.0002,
      "step": 414
    },
    {
      "epoch": 0.6747967479674797,
      "grad_norm": 0.02753608301281929,
      "learning_rate": 0.0001326829268292683,
      "loss": 0.0011,
      "step": 415
    },
    {
      "epoch": 0.6764227642276422,
      "grad_norm": 0.07352620363235474,
      "learning_rate": 0.00013252032520325203,
      "loss": 0.0049,
      "step": 416
    },
    {
      "epoch": 0.6780487804878049,
      "grad_norm": 0.022383153438568115,
      "learning_rate": 0.00013235772357723577,
      "loss": 0.0007,
      "step": 417
    },
    {
      "epoch": 0.6796747967479675,
      "grad_norm": 0.030799895524978638,
      "learning_rate": 0.0001321951219512195,
      "loss": 0.0021,
      "step": 418
    },
    {
      "epoch": 0.6813008130081301,
      "grad_norm": 0.03935651853680611,
      "learning_rate": 0.00013203252032520325,
      "loss": 0.0021,
      "step": 419
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 0.021667126566171646,
      "learning_rate": 0.000131869918699187,
      "loss": 0.0004,
      "step": 420
    },
    {
      "epoch": 0.6845528455284553,
      "grad_norm": 0.0378342904150486,
      "learning_rate": 0.00013170731707317076,
      "loss": 0.0023,
      "step": 421
    },
    {
      "epoch": 0.6861788617886179,
      "grad_norm": 0.05022097006440163,
      "learning_rate": 0.00013154471544715447,
      "loss": 0.0035,
      "step": 422
    },
    {
      "epoch": 0.6878048780487804,
      "grad_norm": 0.044789813458919525,
      "learning_rate": 0.0001313821138211382,
      "loss": 0.0033,
      "step": 423
    },
    {
      "epoch": 0.6894308943089431,
      "grad_norm": 0.01575024425983429,
      "learning_rate": 0.00013121951219512197,
      "loss": 0.0007,
      "step": 424
    },
    {
      "epoch": 0.6910569105691057,
      "grad_norm": 0.0169359277933836,
      "learning_rate": 0.00013105691056910569,
      "loss": 0.0006,
      "step": 425
    },
    {
      "epoch": 0.6926829268292682,
      "grad_norm": 0.028870290145277977,
      "learning_rate": 0.00013089430894308942,
      "loss": 0.0015,
      "step": 426
    },
    {
      "epoch": 0.6943089430894309,
      "grad_norm": 0.006265566218644381,
      "learning_rate": 0.0001307317073170732,
      "loss": 0.0002,
      "step": 427
    },
    {
      "epoch": 0.6959349593495935,
      "grad_norm": 0.0681835189461708,
      "learning_rate": 0.0001305691056910569,
      "loss": 0.0038,
      "step": 428
    },
    {
      "epoch": 0.697560975609756,
      "grad_norm": 0.04927810654044151,
      "learning_rate": 0.00013040650406504064,
      "loss": 0.0056,
      "step": 429
    },
    {
      "epoch": 0.6991869918699187,
      "grad_norm": 0.05472636595368385,
      "learning_rate": 0.0001302439024390244,
      "loss": 0.0049,
      "step": 430
    },
    {
      "epoch": 0.7008130081300813,
      "grad_norm": 0.03326030075550079,
      "learning_rate": 0.00013008130081300815,
      "loss": 0.0011,
      "step": 431
    },
    {
      "epoch": 0.7024390243902439,
      "grad_norm": 0.009500794112682343,
      "learning_rate": 0.00012991869918699186,
      "loss": 0.0004,
      "step": 432
    },
    {
      "epoch": 0.7040650406504065,
      "grad_norm": 0.015952110290527344,
      "learning_rate": 0.00012975609756097562,
      "loss": 0.0006,
      "step": 433
    },
    {
      "epoch": 0.7056910569105691,
      "grad_norm": 0.0048037501983344555,
      "learning_rate": 0.00012959349593495936,
      "loss": 0.0002,
      "step": 434
    },
    {
      "epoch": 0.7073170731707317,
      "grad_norm": 0.013434546068310738,
      "learning_rate": 0.00012943089430894308,
      "loss": 0.0005,
      "step": 435
    },
    {
      "epoch": 0.7089430894308943,
      "grad_norm": 0.07665044814348221,
      "learning_rate": 0.00012926829268292684,
      "loss": 0.0096,
      "step": 436
    },
    {
      "epoch": 0.7105691056910569,
      "grad_norm": 0.019465599209070206,
      "learning_rate": 0.00012910569105691058,
      "loss": 0.0006,
      "step": 437
    },
    {
      "epoch": 0.7121951219512195,
      "grad_norm": 0.05842122808098793,
      "learning_rate": 0.00012894308943089432,
      "loss": 0.0051,
      "step": 438
    },
    {
      "epoch": 0.7138211382113822,
      "grad_norm": 0.03476549685001373,
      "learning_rate": 0.00012878048780487806,
      "loss": 0.0015,
      "step": 439
    },
    {
      "epoch": 0.7154471544715447,
      "grad_norm": 0.038131747394800186,
      "learning_rate": 0.0001286178861788618,
      "loss": 0.0013,
      "step": 440
    },
    {
      "epoch": 0.7170731707317073,
      "grad_norm": 0.04565577954053879,
      "learning_rate": 0.00012845528455284554,
      "loss": 0.0036,
      "step": 441
    },
    {
      "epoch": 0.71869918699187,
      "grad_norm": 0.07976550608873367,
      "learning_rate": 0.00012829268292682927,
      "loss": 0.0088,
      "step": 442
    },
    {
      "epoch": 0.7203252032520325,
      "grad_norm": 0.024210834875702858,
      "learning_rate": 0.00012813008130081301,
      "loss": 0.0007,
      "step": 443
    },
    {
      "epoch": 0.7219512195121951,
      "grad_norm": 0.04592128470540047,
      "learning_rate": 0.00012796747967479675,
      "loss": 0.0015,
      "step": 444
    },
    {
      "epoch": 0.7235772357723578,
      "grad_norm": 0.020742984488606453,
      "learning_rate": 0.0001278048780487805,
      "loss": 0.0013,
      "step": 445
    },
    {
      "epoch": 0.7252032520325203,
      "grad_norm": 0.01040639914572239,
      "learning_rate": 0.00012764227642276423,
      "loss": 0.0003,
      "step": 446
    },
    {
      "epoch": 0.7268292682926829,
      "grad_norm": 0.03810832276940346,
      "learning_rate": 0.00012747967479674797,
      "loss": 0.0023,
      "step": 447
    },
    {
      "epoch": 0.7284552845528456,
      "grad_norm": 0.0026015646290034056,
      "learning_rate": 0.0001273170731707317,
      "loss": 0.0002,
      "step": 448
    },
    {
      "epoch": 0.7300813008130081,
      "grad_norm": 0.02090383879840374,
      "learning_rate": 0.00012715447154471545,
      "loss": 0.0007,
      "step": 449
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 0.030612654983997345,
      "learning_rate": 0.00012699186991869921,
      "loss": 0.0016,
      "step": 450
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.02979256771504879,
      "learning_rate": 0.00012682926829268293,
      "loss": 0.0022,
      "step": 451
    },
    {
      "epoch": 0.734959349593496,
      "grad_norm": 0.016268040984869003,
      "learning_rate": 0.00012666666666666666,
      "loss": 0.0003,
      "step": 452
    },
    {
      "epoch": 0.7365853658536585,
      "grad_norm": 0.04419322684407234,
      "learning_rate": 0.00012650406504065043,
      "loss": 0.0022,
      "step": 453
    },
    {
      "epoch": 0.7382113821138211,
      "grad_norm": 0.07264306396245956,
      "learning_rate": 0.00012634146341463414,
      "loss": 0.0052,
      "step": 454
    },
    {
      "epoch": 0.7398373983739838,
      "grad_norm": 0.03466745465993881,
      "learning_rate": 0.00012617886178861788,
      "loss": 0.0021,
      "step": 455
    },
    {
      "epoch": 0.7414634146341463,
      "grad_norm": 0.08261137455701828,
      "learning_rate": 0.00012601626016260165,
      "loss": 0.0048,
      "step": 456
    },
    {
      "epoch": 0.7430894308943089,
      "grad_norm": 0.03311735764145851,
      "learning_rate": 0.00012585365853658536,
      "loss": 0.002,
      "step": 457
    },
    {
      "epoch": 0.7447154471544716,
      "grad_norm": 0.027840545400977135,
      "learning_rate": 0.0001256910569105691,
      "loss": 0.0012,
      "step": 458
    },
    {
      "epoch": 0.7463414634146341,
      "grad_norm": 0.0727102980017662,
      "learning_rate": 0.00012552845528455286,
      "loss": 0.0028,
      "step": 459
    },
    {
      "epoch": 0.7479674796747967,
      "grad_norm": 0.10644757747650146,
      "learning_rate": 0.0001253658536585366,
      "loss": 0.0086,
      "step": 460
    },
    {
      "epoch": 0.7495934959349594,
      "grad_norm": 0.03786543011665344,
      "learning_rate": 0.00012520325203252032,
      "loss": 0.0019,
      "step": 461
    },
    {
      "epoch": 0.751219512195122,
      "grad_norm": 0.048276159912347794,
      "learning_rate": 0.00012504065040650408,
      "loss": 0.0033,
      "step": 462
    },
    {
      "epoch": 0.7528455284552845,
      "grad_norm": 0.01945447362959385,
      "learning_rate": 0.00012487804878048782,
      "loss": 0.0005,
      "step": 463
    },
    {
      "epoch": 0.7544715447154472,
      "grad_norm": 0.030971255153417587,
      "learning_rate": 0.00012471544715447153,
      "loss": 0.0019,
      "step": 464
    },
    {
      "epoch": 0.7560975609756098,
      "grad_norm": 0.0476507730782032,
      "learning_rate": 0.0001245528455284553,
      "loss": 0.0041,
      "step": 465
    },
    {
      "epoch": 0.7577235772357723,
      "grad_norm": 0.04508371651172638,
      "learning_rate": 0.00012439024390243904,
      "loss": 0.0027,
      "step": 466
    },
    {
      "epoch": 0.759349593495935,
      "grad_norm": 0.02546669915318489,
      "learning_rate": 0.00012422764227642275,
      "loss": 0.0013,
      "step": 467
    },
    {
      "epoch": 0.7609756097560976,
      "grad_norm": 0.02323518693447113,
      "learning_rate": 0.00012406504065040652,
      "loss": 0.0009,
      "step": 468
    },
    {
      "epoch": 0.7626016260162601,
      "grad_norm": 0.03841609135270119,
      "learning_rate": 0.00012390243902439025,
      "loss": 0.0023,
      "step": 469
    },
    {
      "epoch": 0.7642276422764228,
      "grad_norm": 0.07417786866426468,
      "learning_rate": 0.000123739837398374,
      "loss": 0.0028,
      "step": 470
    },
    {
      "epoch": 0.7658536585365854,
      "grad_norm": 0.07709995657205582,
      "learning_rate": 0.00012357723577235773,
      "loss": 0.0079,
      "step": 471
    },
    {
      "epoch": 0.767479674796748,
      "grad_norm": 0.03812519833445549,
      "learning_rate": 0.00012341463414634147,
      "loss": 0.0023,
      "step": 472
    },
    {
      "epoch": 0.7691056910569106,
      "grad_norm": 0.030005669221282005,
      "learning_rate": 0.0001232520325203252,
      "loss": 0.0015,
      "step": 473
    },
    {
      "epoch": 0.7707317073170732,
      "grad_norm": 0.06345708668231964,
      "learning_rate": 0.00012308943089430895,
      "loss": 0.0016,
      "step": 474
    },
    {
      "epoch": 0.7723577235772358,
      "grad_norm": 0.0031643323600292206,
      "learning_rate": 0.0001229268292682927,
      "loss": 0.0002,
      "step": 475
    },
    {
      "epoch": 0.7739837398373983,
      "grad_norm": 0.005016733426600695,
      "learning_rate": 0.00012276422764227643,
      "loss": 0.0003,
      "step": 476
    },
    {
      "epoch": 0.775609756097561,
      "grad_norm": 0.05273330956697464,
      "learning_rate": 0.00012260162601626017,
      "loss": 0.0028,
      "step": 477
    },
    {
      "epoch": 0.7772357723577236,
      "grad_norm": 0.01227838546037674,
      "learning_rate": 0.0001224390243902439,
      "loss": 0.0005,
      "step": 478
    },
    {
      "epoch": 0.7788617886178861,
      "grad_norm": 0.05786914750933647,
      "learning_rate": 0.00012227642276422764,
      "loss": 0.0024,
      "step": 479
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 0.0029559931717813015,
      "learning_rate": 0.00012211382113821138,
      "loss": 0.0002,
      "step": 480
    },
    {
      "epoch": 0.7821138211382114,
      "grad_norm": 0.021030550822615623,
      "learning_rate": 0.00012195121951219512,
      "loss": 0.0003,
      "step": 481
    },
    {
      "epoch": 0.7837398373983739,
      "grad_norm": 0.10761956870555878,
      "learning_rate": 0.00012178861788617886,
      "loss": 0.0055,
      "step": 482
    },
    {
      "epoch": 0.7853658536585366,
      "grad_norm": 0.04578990116715431,
      "learning_rate": 0.00012162601626016261,
      "loss": 0.0031,
      "step": 483
    },
    {
      "epoch": 0.7869918699186992,
      "grad_norm": 0.07672619074583054,
      "learning_rate": 0.00012146341463414634,
      "loss": 0.0078,
      "step": 484
    },
    {
      "epoch": 0.7886178861788617,
      "grad_norm": 0.040395379066467285,
      "learning_rate": 0.00012130081300813008,
      "loss": 0.0013,
      "step": 485
    },
    {
      "epoch": 0.7902439024390244,
      "grad_norm": 0.0027492314111441374,
      "learning_rate": 0.00012113821138211383,
      "loss": 0.0002,
      "step": 486
    },
    {
      "epoch": 0.791869918699187,
      "grad_norm": 0.020994307473301888,
      "learning_rate": 0.00012097560975609757,
      "loss": 0.0007,
      "step": 487
    },
    {
      "epoch": 0.7934959349593496,
      "grad_norm": 0.030907196924090385,
      "learning_rate": 0.00012081300813008132,
      "loss": 0.0008,
      "step": 488
    },
    {
      "epoch": 0.7951219512195122,
      "grad_norm": 0.06392280012369156,
      "learning_rate": 0.00012065040650406505,
      "loss": 0.0048,
      "step": 489
    },
    {
      "epoch": 0.7967479674796748,
      "grad_norm": 0.04470368102192879,
      "learning_rate": 0.00012048780487804879,
      "loss": 0.0012,
      "step": 490
    },
    {
      "epoch": 0.7983739837398374,
      "grad_norm": 0.04805390164256096,
      "learning_rate": 0.00012032520325203254,
      "loss": 0.0028,
      "step": 491
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.05485748127102852,
      "learning_rate": 0.00012016260162601626,
      "loss": 0.0059,
      "step": 492
    },
    {
      "epoch": 0.8016260162601626,
      "grad_norm": 0.00901495385915041,
      "learning_rate": 0.00012,
      "loss": 0.0003,
      "step": 493
    },
    {
      "epoch": 0.8032520325203252,
      "grad_norm": 0.033151764422655106,
      "learning_rate": 0.00011983739837398376,
      "loss": 0.0015,
      "step": 494
    },
    {
      "epoch": 0.8048780487804879,
      "grad_norm": 0.0457472987473011,
      "learning_rate": 0.0001196747967479675,
      "loss": 0.0015,
      "step": 495
    },
    {
      "epoch": 0.8065040650406504,
      "grad_norm": 0.026971641927957535,
      "learning_rate": 0.00011951219512195122,
      "loss": 0.0008,
      "step": 496
    },
    {
      "epoch": 0.808130081300813,
      "grad_norm": 0.0381942093372345,
      "learning_rate": 0.00011934959349593497,
      "loss": 0.0022,
      "step": 497
    },
    {
      "epoch": 0.8097560975609757,
      "grad_norm": 0.028191309422254562,
      "learning_rate": 0.00011918699186991871,
      "loss": 0.0014,
      "step": 498
    },
    {
      "epoch": 0.8113821138211382,
      "grad_norm": 0.020040923729538918,
      "learning_rate": 0.00011902439024390244,
      "loss": 0.0009,
      "step": 499
    },
    {
      "epoch": 0.8130081300813008,
      "grad_norm": 0.028104929253458977,
      "learning_rate": 0.00011886178861788619,
      "loss": 0.002,
      "step": 500
    },
    {
      "epoch": 0.8146341463414634,
      "grad_norm": 0.01615731045603752,
      "learning_rate": 0.00011869918699186993,
      "loss": 0.0005,
      "step": 501
    },
    {
      "epoch": 0.816260162601626,
      "grad_norm": 0.016076011583209038,
      "learning_rate": 0.00011853658536585365,
      "loss": 0.0007,
      "step": 502
    },
    {
      "epoch": 0.8178861788617886,
      "grad_norm": 0.04311200976371765,
      "learning_rate": 0.0001183739837398374,
      "loss": 0.003,
      "step": 503
    },
    {
      "epoch": 0.8195121951219512,
      "grad_norm": 0.03994676098227501,
      "learning_rate": 0.00011821138211382115,
      "loss": 0.0031,
      "step": 504
    },
    {
      "epoch": 0.8211382113821138,
      "grad_norm": 0.03172750398516655,
      "learning_rate": 0.00011804878048780488,
      "loss": 0.0011,
      "step": 505
    },
    {
      "epoch": 0.8227642276422764,
      "grad_norm": 0.04845735803246498,
      "learning_rate": 0.00011788617886178864,
      "loss": 0.0033,
      "step": 506
    },
    {
      "epoch": 0.824390243902439,
      "grad_norm": 0.018561698496341705,
      "learning_rate": 0.00011772357723577236,
      "loss": 0.0006,
      "step": 507
    },
    {
      "epoch": 0.8260162601626017,
      "grad_norm": 0.021360432729125023,
      "learning_rate": 0.0001175609756097561,
      "loss": 0.0009,
      "step": 508
    },
    {
      "epoch": 0.8276422764227642,
      "grad_norm": 0.016144506633281708,
      "learning_rate": 0.00011739837398373985,
      "loss": 0.001,
      "step": 509
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 0.09096671640872955,
      "learning_rate": 0.00011723577235772358,
      "loss": 0.0081,
      "step": 510
    },
    {
      "epoch": 0.8308943089430895,
      "grad_norm": 0.01564052887260914,
      "learning_rate": 0.00011707317073170732,
      "loss": 0.0005,
      "step": 511
    },
    {
      "epoch": 0.832520325203252,
      "grad_norm": 0.0628076121211052,
      "learning_rate": 0.00011691056910569107,
      "loss": 0.0064,
      "step": 512
    },
    {
      "epoch": 0.8341463414634146,
      "grad_norm": 0.03254637122154236,
      "learning_rate": 0.0001167479674796748,
      "loss": 0.0012,
      "step": 513
    },
    {
      "epoch": 0.8357723577235773,
      "grad_norm": 0.052123911678791046,
      "learning_rate": 0.00011658536585365853,
      "loss": 0.0054,
      "step": 514
    },
    {
      "epoch": 0.8373983739837398,
      "grad_norm": 0.011211443692445755,
      "learning_rate": 0.00011642276422764229,
      "loss": 0.0005,
      "step": 515
    },
    {
      "epoch": 0.8390243902439024,
      "grad_norm": 0.016485324129462242,
      "learning_rate": 0.00011626016260162603,
      "loss": 0.0007,
      "step": 516
    },
    {
      "epoch": 0.8406504065040651,
      "grad_norm": 0.04991016536951065,
      "learning_rate": 0.00011609756097560975,
      "loss": 0.0021,
      "step": 517
    },
    {
      "epoch": 0.8422764227642277,
      "grad_norm": 0.015922604128718376,
      "learning_rate": 0.0001159349593495935,
      "loss": 0.0005,
      "step": 518
    },
    {
      "epoch": 0.8439024390243902,
      "grad_norm": 0.023434707894921303,
      "learning_rate": 0.00011577235772357724,
      "loss": 0.0015,
      "step": 519
    },
    {
      "epoch": 0.8455284552845529,
      "grad_norm": 0.024555252864956856,
      "learning_rate": 0.00011560975609756097,
      "loss": 0.0011,
      "step": 520
    },
    {
      "epoch": 0.8471544715447155,
      "grad_norm": 0.04176575317978859,
      "learning_rate": 0.00011544715447154472,
      "loss": 0.0026,
      "step": 521
    },
    {
      "epoch": 0.848780487804878,
      "grad_norm": 0.02413194440305233,
      "learning_rate": 0.00011528455284552846,
      "loss": 0.0008,
      "step": 522
    },
    {
      "epoch": 0.8504065040650407,
      "grad_norm": 0.037769243121147156,
      "learning_rate": 0.00011512195121951219,
      "loss": 0.0027,
      "step": 523
    },
    {
      "epoch": 0.8520325203252033,
      "grad_norm": 0.036895595490932465,
      "learning_rate": 0.00011495934959349595,
      "loss": 0.0029,
      "step": 524
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 0.03181309252977371,
      "learning_rate": 0.00011479674796747968,
      "loss": 0.0013,
      "step": 525
    },
    {
      "epoch": 0.8552845528455284,
      "grad_norm": 0.02323925495147705,
      "learning_rate": 0.00011463414634146342,
      "loss": 0.0009,
      "step": 526
    },
    {
      "epoch": 0.8569105691056911,
      "grad_norm": 0.05257396772503853,
      "learning_rate": 0.00011447154471544717,
      "loss": 0.0026,
      "step": 527
    },
    {
      "epoch": 0.8585365853658536,
      "grad_norm": 0.013914236798882484,
      "learning_rate": 0.0001143089430894309,
      "loss": 0.0006,
      "step": 528
    },
    {
      "epoch": 0.8601626016260162,
      "grad_norm": 0.011155262589454651,
      "learning_rate": 0.00011414634146341463,
      "loss": 0.0002,
      "step": 529
    },
    {
      "epoch": 0.8617886178861789,
      "grad_norm": 0.015880966559052467,
      "learning_rate": 0.00011398373983739839,
      "loss": 0.0007,
      "step": 530
    },
    {
      "epoch": 0.8634146341463415,
      "grad_norm": 0.025967376306653023,
      "learning_rate": 0.00011382113821138211,
      "loss": 0.0009,
      "step": 531
    },
    {
      "epoch": 0.865040650406504,
      "grad_norm": 0.01948608085513115,
      "learning_rate": 0.00011365853658536585,
      "loss": 0.0007,
      "step": 532
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.018302444368600845,
      "learning_rate": 0.0001134959349593496,
      "loss": 0.0007,
      "step": 533
    },
    {
      "epoch": 0.8682926829268293,
      "grad_norm": 0.028660833835601807,
      "learning_rate": 0.00011333333333333334,
      "loss": 0.0019,
      "step": 534
    },
    {
      "epoch": 0.8699186991869918,
      "grad_norm": 0.021785728633403778,
      "learning_rate": 0.00011317073170731707,
      "loss": 0.0011,
      "step": 535
    },
    {
      "epoch": 0.8715447154471545,
      "grad_norm": 0.031346190720796585,
      "learning_rate": 0.00011300813008130082,
      "loss": 0.001,
      "step": 536
    },
    {
      "epoch": 0.8731707317073171,
      "grad_norm": 0.02406279183924198,
      "learning_rate": 0.00011284552845528456,
      "loss": 0.0014,
      "step": 537
    },
    {
      "epoch": 0.8747967479674796,
      "grad_norm": 0.04375109821557999,
      "learning_rate": 0.00011268292682926828,
      "loss": 0.0031,
      "step": 538
    },
    {
      "epoch": 0.8764227642276423,
      "grad_norm": 0.018627827987074852,
      "learning_rate": 0.00011252032520325204,
      "loss": 0.0006,
      "step": 539
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 0.027309613302350044,
      "learning_rate": 0.00011235772357723578,
      "loss": 0.0008,
      "step": 540
    },
    {
      "epoch": 0.8796747967479674,
      "grad_norm": 0.03513088449835777,
      "learning_rate": 0.00011219512195121953,
      "loss": 0.0025,
      "step": 541
    },
    {
      "epoch": 0.8813008130081301,
      "grad_norm": 0.053457196801900864,
      "learning_rate": 0.00011203252032520327,
      "loss": 0.0035,
      "step": 542
    },
    {
      "epoch": 0.8829268292682927,
      "grad_norm": 0.017759982496500015,
      "learning_rate": 0.00011186991869918699,
      "loss": 0.0002,
      "step": 543
    },
    {
      "epoch": 0.8845528455284553,
      "grad_norm": 0.005492689553648233,
      "learning_rate": 0.00011170731707317074,
      "loss": 0.0002,
      "step": 544
    },
    {
      "epoch": 0.8861788617886179,
      "grad_norm": 0.058742325752973557,
      "learning_rate": 0.00011154471544715448,
      "loss": 0.005,
      "step": 545
    },
    {
      "epoch": 0.8878048780487805,
      "grad_norm": 0.018566960468888283,
      "learning_rate": 0.00011138211382113821,
      "loss": 0.0005,
      "step": 546
    },
    {
      "epoch": 0.8894308943089431,
      "grad_norm": 0.02755313739180565,
      "learning_rate": 0.00011121951219512196,
      "loss": 0.0012,
      "step": 547
    },
    {
      "epoch": 0.8910569105691057,
      "grad_norm": 0.024027865380048752,
      "learning_rate": 0.0001110569105691057,
      "loss": 0.0009,
      "step": 548
    },
    {
      "epoch": 0.8926829268292683,
      "grad_norm": 0.014305217191576958,
      "learning_rate": 0.00011089430894308943,
      "loss": 0.0006,
      "step": 549
    },
    {
      "epoch": 0.8943089430894309,
      "grad_norm": 0.03786611557006836,
      "learning_rate": 0.00011073170731707319,
      "loss": 0.0035,
      "step": 550
    },
    {
      "epoch": 0.8959349593495934,
      "grad_norm": 0.07740436494350433,
      "learning_rate": 0.00011056910569105692,
      "loss": 0.0047,
      "step": 551
    },
    {
      "epoch": 0.8975609756097561,
      "grad_norm": 0.028541529551148415,
      "learning_rate": 0.00011040650406504066,
      "loss": 0.001,
      "step": 552
    },
    {
      "epoch": 0.8991869918699187,
      "grad_norm": 0.026759367436170578,
      "learning_rate": 0.00011024390243902441,
      "loss": 0.0008,
      "step": 553
    },
    {
      "epoch": 0.9008130081300812,
      "grad_norm": 0.048677392303943634,
      "learning_rate": 0.00011008130081300813,
      "loss": 0.0044,
      "step": 554
    },
    {
      "epoch": 0.9024390243902439,
      "grad_norm": 0.01839292421936989,
      "learning_rate": 0.00010991869918699187,
      "loss": 0.0007,
      "step": 555
    },
    {
      "epoch": 0.9040650406504065,
      "grad_norm": 0.0452253632247448,
      "learning_rate": 0.00010975609756097563,
      "loss": 0.0012,
      "step": 556
    },
    {
      "epoch": 0.9056910569105691,
      "grad_norm": 0.012644516304135323,
      "learning_rate": 0.00010959349593495935,
      "loss": 0.0005,
      "step": 557
    },
    {
      "epoch": 0.9073170731707317,
      "grad_norm": 0.036521803587675095,
      "learning_rate": 0.00010943089430894309,
      "loss": 0.0019,
      "step": 558
    },
    {
      "epoch": 0.9089430894308943,
      "grad_norm": 0.04588788375258446,
      "learning_rate": 0.00010926829268292684,
      "loss": 0.0049,
      "step": 559
    },
    {
      "epoch": 0.9105691056910569,
      "grad_norm": 0.025674177333712578,
      "learning_rate": 0.00010910569105691057,
      "loss": 0.0009,
      "step": 560
    },
    {
      "epoch": 0.9121951219512195,
      "grad_norm": 0.05303553119301796,
      "learning_rate": 0.00010894308943089431,
      "loss": 0.0036,
      "step": 561
    },
    {
      "epoch": 0.9138211382113821,
      "grad_norm": 0.06171534210443497,
      "learning_rate": 0.00010878048780487806,
      "loss": 0.0034,
      "step": 562
    },
    {
      "epoch": 0.9154471544715447,
      "grad_norm": 0.032720163464546204,
      "learning_rate": 0.0001086178861788618,
      "loss": 0.0012,
      "step": 563
    },
    {
      "epoch": 0.9170731707317074,
      "grad_norm": 0.041798610240221024,
      "learning_rate": 0.00010845528455284552,
      "loss": 0.0008,
      "step": 564
    },
    {
      "epoch": 0.9186991869918699,
      "grad_norm": 0.05163143202662468,
      "learning_rate": 0.00010829268292682928,
      "loss": 0.0055,
      "step": 565
    },
    {
      "epoch": 0.9203252032520325,
      "grad_norm": 0.06489244103431702,
      "learning_rate": 0.00010813008130081302,
      "loss": 0.004,
      "step": 566
    },
    {
      "epoch": 0.9219512195121952,
      "grad_norm": 0.01523418165743351,
      "learning_rate": 0.00010796747967479674,
      "loss": 0.0004,
      "step": 567
    },
    {
      "epoch": 0.9235772357723577,
      "grad_norm": 0.006014470476657152,
      "learning_rate": 0.0001078048780487805,
      "loss": 0.0002,
      "step": 568
    },
    {
      "epoch": 0.9252032520325203,
      "grad_norm": 0.030240919440984726,
      "learning_rate": 0.00010764227642276423,
      "loss": 0.0008,
      "step": 569
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 0.01644095964729786,
      "learning_rate": 0.00010747967479674796,
      "loss": 0.0006,
      "step": 570
    },
    {
      "epoch": 0.9284552845528455,
      "grad_norm": 0.027487248182296753,
      "learning_rate": 0.00010731707317073172,
      "loss": 0.0008,
      "step": 571
    },
    {
      "epoch": 0.9300813008130081,
      "grad_norm": 0.02853054739534855,
      "learning_rate": 0.00010715447154471545,
      "loss": 0.0012,
      "step": 572
    },
    {
      "epoch": 0.9317073170731708,
      "grad_norm": 0.035733114928007126,
      "learning_rate": 0.00010699186991869919,
      "loss": 0.0022,
      "step": 573
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.0017652700189501047,
      "learning_rate": 0.00010682926829268294,
      "loss": 0.0001,
      "step": 574
    },
    {
      "epoch": 0.9349593495934959,
      "grad_norm": 0.033423926681280136,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.002,
      "step": 575
    },
    {
      "epoch": 0.9365853658536586,
      "grad_norm": 0.0750659927725792,
      "learning_rate": 0.0001065040650406504,
      "loss": 0.0023,
      "step": 576
    },
    {
      "epoch": 0.9382113821138212,
      "grad_norm": 0.011079270392656326,
      "learning_rate": 0.00010634146341463416,
      "loss": 0.0004,
      "step": 577
    },
    {
      "epoch": 0.9398373983739837,
      "grad_norm": 0.015469861216843128,
      "learning_rate": 0.00010617886178861788,
      "loss": 0.0006,
      "step": 578
    },
    {
      "epoch": 0.9414634146341463,
      "grad_norm": 0.018353410065174103,
      "learning_rate": 0.00010601626016260162,
      "loss": 0.0006,
      "step": 579
    },
    {
      "epoch": 0.943089430894309,
      "grad_norm": 0.028354918584227562,
      "learning_rate": 0.00010585365853658537,
      "loss": 0.002,
      "step": 580
    },
    {
      "epoch": 0.9447154471544715,
      "grad_norm": 0.04888761788606644,
      "learning_rate": 0.00010569105691056911,
      "loss": 0.0038,
      "step": 581
    },
    {
      "epoch": 0.9463414634146341,
      "grad_norm": 0.04139448329806328,
      "learning_rate": 0.00010552845528455284,
      "loss": 0.0017,
      "step": 582
    },
    {
      "epoch": 0.9479674796747968,
      "grad_norm": 0.046597670763731,
      "learning_rate": 0.00010536585365853659,
      "loss": 0.0023,
      "step": 583
    },
    {
      "epoch": 0.9495934959349593,
      "grad_norm": 0.005212120246142149,
      "learning_rate": 0.00010520325203252033,
      "loss": 0.0002,
      "step": 584
    },
    {
      "epoch": 0.9512195121951219,
      "grad_norm": 0.0357334278523922,
      "learning_rate": 0.00010504065040650406,
      "loss": 0.0017,
      "step": 585
    },
    {
      "epoch": 0.9528455284552846,
      "grad_norm": 0.03520767763257027,
      "learning_rate": 0.00010487804878048781,
      "loss": 0.0021,
      "step": 586
    },
    {
      "epoch": 0.9544715447154472,
      "grad_norm": 0.03216665983200073,
      "learning_rate": 0.00010471544715447155,
      "loss": 0.0006,
      "step": 587
    },
    {
      "epoch": 0.9560975609756097,
      "grad_norm": 0.01625550352036953,
      "learning_rate": 0.00010455284552845527,
      "loss": 0.0005,
      "step": 588
    },
    {
      "epoch": 0.9577235772357724,
      "grad_norm": 0.03498171269893646,
      "learning_rate": 0.00010439024390243904,
      "loss": 0.001,
      "step": 589
    },
    {
      "epoch": 0.959349593495935,
      "grad_norm": 0.021105607971549034,
      "learning_rate": 0.00010422764227642276,
      "loss": 0.0008,
      "step": 590
    },
    {
      "epoch": 0.9609756097560975,
      "grad_norm": 0.006631603930145502,
      "learning_rate": 0.00010406504065040652,
      "loss": 0.0002,
      "step": 591
    },
    {
      "epoch": 0.9626016260162602,
      "grad_norm": 0.01798001490533352,
      "learning_rate": 0.00010390243902439026,
      "loss": 0.0008,
      "step": 592
    },
    {
      "epoch": 0.9642276422764228,
      "grad_norm": 0.023252597078680992,
      "learning_rate": 0.00010373983739837398,
      "loss": 0.0008,
      "step": 593
    },
    {
      "epoch": 0.9658536585365853,
      "grad_norm": 0.01537206768989563,
      "learning_rate": 0.00010357723577235773,
      "loss": 0.0007,
      "step": 594
    },
    {
      "epoch": 0.967479674796748,
      "grad_norm": 0.03023386187851429,
      "learning_rate": 0.00010341463414634147,
      "loss": 0.0017,
      "step": 595
    },
    {
      "epoch": 0.9691056910569106,
      "grad_norm": 0.023562436923384666,
      "learning_rate": 0.0001032520325203252,
      "loss": 0.0012,
      "step": 596
    },
    {
      "epoch": 0.9707317073170731,
      "grad_norm": 0.019724881276488304,
      "learning_rate": 0.00010308943089430896,
      "loss": 0.0009,
      "step": 597
    },
    {
      "epoch": 0.9723577235772358,
      "grad_norm": 0.02794792503118515,
      "learning_rate": 0.00010292682926829269,
      "loss": 0.0013,
      "step": 598
    },
    {
      "epoch": 0.9739837398373984,
      "grad_norm": 0.018562523648142815,
      "learning_rate": 0.00010276422764227643,
      "loss": 0.0007,
      "step": 599
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 0.061098743230104446,
      "learning_rate": 0.00010260162601626018,
      "loss": 0.0051,
      "step": 600
    },
    {
      "epoch": 0.9772357723577236,
      "grad_norm": 0.02799287624657154,
      "learning_rate": 0.0001024390243902439,
      "loss": 0.0011,
      "step": 601
    },
    {
      "epoch": 0.9788617886178862,
      "grad_norm": 0.014771801419556141,
      "learning_rate": 0.00010227642276422765,
      "loss": 0.0004,
      "step": 602
    },
    {
      "epoch": 0.9804878048780488,
      "grad_norm": 0.046715617179870605,
      "learning_rate": 0.0001021138211382114,
      "loss": 0.003,
      "step": 603
    },
    {
      "epoch": 0.9821138211382113,
      "grad_norm": 0.002003920031711459,
      "learning_rate": 0.00010195121951219512,
      "loss": 0.0001,
      "step": 604
    },
    {
      "epoch": 0.983739837398374,
      "grad_norm": 0.025536799803376198,
      "learning_rate": 0.00010178861788617886,
      "loss": 0.0008,
      "step": 605
    },
    {
      "epoch": 0.9853658536585366,
      "grad_norm": 0.036047108471393585,
      "learning_rate": 0.00010162601626016262,
      "loss": 0.0024,
      "step": 606
    },
    {
      "epoch": 0.9869918699186991,
      "grad_norm": 0.052953463047742844,
      "learning_rate": 0.00010146341463414635,
      "loss": 0.0028,
      "step": 607
    },
    {
      "epoch": 0.9886178861788618,
      "grad_norm": 0.06137080118060112,
      "learning_rate": 0.00010130081300813008,
      "loss": 0.0022,
      "step": 608
    },
    {
      "epoch": 0.9902439024390244,
      "grad_norm": 0.05845019221305847,
      "learning_rate": 0.00010113821138211383,
      "loss": 0.0005,
      "step": 609
    },
    {
      "epoch": 0.991869918699187,
      "grad_norm": 0.006013177335262299,
      "learning_rate": 0.00010097560975609757,
      "loss": 0.0002,
      "step": 610
    },
    {
      "epoch": 0.9934959349593496,
      "grad_norm": 0.0023975882213562727,
      "learning_rate": 0.0001008130081300813,
      "loss": 0.0001,
      "step": 611
    },
    {
      "epoch": 0.9951219512195122,
      "grad_norm": 0.027645491063594818,
      "learning_rate": 0.00010065040650406505,
      "loss": 0.0012,
      "step": 612
    },
    {
      "epoch": 0.9967479674796748,
      "grad_norm": 0.029636425897479057,
      "learning_rate": 0.00010048780487804879,
      "loss": 0.0016,
      "step": 613
    },
    {
      "epoch": 0.9983739837398374,
      "grad_norm": 0.037054501473903656,
      "learning_rate": 0.00010032520325203251,
      "loss": 0.0017,
      "step": 614
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.056419797241687775,
      "learning_rate": 0.00010016260162601627,
      "loss": 0.0031,
      "step": 615
    },
    {
      "epoch": 1.0016260162601627,
      "grad_norm": 0.0429646261036396,
      "learning_rate": 0.0001,
      "loss": 0.0024,
      "step": 616
    },
    {
      "epoch": 1.0032520325203251,
      "grad_norm": 0.01944703236222267,
      "learning_rate": 9.983739837398374e-05,
      "loss": 0.0004,
      "step": 617
    },
    {
      "epoch": 1.0048780487804878,
      "grad_norm": 0.04017496854066849,
      "learning_rate": 9.967479674796748e-05,
      "loss": 0.0011,
      "step": 618
    },
    {
      "epoch": 1.0065040650406505,
      "grad_norm": 0.02971489168703556,
      "learning_rate": 9.951219512195122e-05,
      "loss": 0.0014,
      "step": 619
    },
    {
      "epoch": 1.008130081300813,
      "grad_norm": 0.03704826533794403,
      "learning_rate": 9.934959349593496e-05,
      "loss": 0.0023,
      "step": 620
    },
    {
      "epoch": 1.0097560975609756,
      "grad_norm": 0.0012523398036137223,
      "learning_rate": 9.91869918699187e-05,
      "loss": 0.0001,
      "step": 621
    },
    {
      "epoch": 1.0113821138211383,
      "grad_norm": 0.022918064147233963,
      "learning_rate": 9.902439024390244e-05,
      "loss": 0.0009,
      "step": 622
    },
    {
      "epoch": 1.0130081300813008,
      "grad_norm": 0.05926758050918579,
      "learning_rate": 9.886178861788619e-05,
      "loss": 0.0014,
      "step": 623
    },
    {
      "epoch": 1.0146341463414634,
      "grad_norm": 0.00916359107941389,
      "learning_rate": 9.869918699186992e-05,
      "loss": 0.0002,
      "step": 624
    },
    {
      "epoch": 1.016260162601626,
      "grad_norm": 0.029726484790444374,
      "learning_rate": 9.853658536585366e-05,
      "loss": 0.0009,
      "step": 625
    },
    {
      "epoch": 1.0178861788617886,
      "grad_norm": 0.01529078558087349,
      "learning_rate": 9.837398373983741e-05,
      "loss": 0.0005,
      "step": 626
    },
    {
      "epoch": 1.0195121951219512,
      "grad_norm": 0.03967956081032753,
      "learning_rate": 9.821138211382113e-05,
      "loss": 0.0017,
      "step": 627
    },
    {
      "epoch": 1.021138211382114,
      "grad_norm": 0.01647813431918621,
      "learning_rate": 9.804878048780489e-05,
      "loss": 0.0006,
      "step": 628
    },
    {
      "epoch": 1.0227642276422764,
      "grad_norm": 0.022827278822660446,
      "learning_rate": 9.788617886178862e-05,
      "loss": 0.0014,
      "step": 629
    },
    {
      "epoch": 1.024390243902439,
      "grad_norm": 0.038049113005399704,
      "learning_rate": 9.772357723577236e-05,
      "loss": 0.0016,
      "step": 630
    },
    {
      "epoch": 1.0260162601626017,
      "grad_norm": 0.013302645646035671,
      "learning_rate": 9.75609756097561e-05,
      "loss": 0.0005,
      "step": 631
    },
    {
      "epoch": 1.0276422764227642,
      "grad_norm": 0.017054682597517967,
      "learning_rate": 9.739837398373984e-05,
      "loss": 0.0007,
      "step": 632
    },
    {
      "epoch": 1.0292682926829269,
      "grad_norm": 0.1909637153148651,
      "learning_rate": 9.723577235772358e-05,
      "loss": 0.0074,
      "step": 633
    },
    {
      "epoch": 1.0308943089430895,
      "grad_norm": 0.047609224915504456,
      "learning_rate": 9.707317073170732e-05,
      "loss": 0.002,
      "step": 634
    },
    {
      "epoch": 1.032520325203252,
      "grad_norm": 0.03260936960577965,
      "learning_rate": 9.691056910569106e-05,
      "loss": 0.0013,
      "step": 635
    },
    {
      "epoch": 1.0341463414634147,
      "grad_norm": 0.042000457644462585,
      "learning_rate": 9.674796747967481e-05,
      "loss": 0.0018,
      "step": 636
    },
    {
      "epoch": 1.0357723577235773,
      "grad_norm": 0.03592798486351967,
      "learning_rate": 9.658536585365854e-05,
      "loss": 0.0013,
      "step": 637
    },
    {
      "epoch": 1.0373983739837398,
      "grad_norm": 0.03559374064207077,
      "learning_rate": 9.642276422764228e-05,
      "loss": 0.0025,
      "step": 638
    },
    {
      "epoch": 1.0390243902439025,
      "grad_norm": 0.06502026319503784,
      "learning_rate": 9.626016260162603e-05,
      "loss": 0.0014,
      "step": 639
    },
    {
      "epoch": 1.040650406504065,
      "grad_norm": 0.021712150424718857,
      "learning_rate": 9.609756097560975e-05,
      "loss": 0.0011,
      "step": 640
    },
    {
      "epoch": 1.0422764227642276,
      "grad_norm": 0.03215531259775162,
      "learning_rate": 9.59349593495935e-05,
      "loss": 0.0008,
      "step": 641
    },
    {
      "epoch": 1.0439024390243903,
      "grad_norm": 0.0188158992677927,
      "learning_rate": 9.577235772357725e-05,
      "loss": 0.0003,
      "step": 642
    },
    {
      "epoch": 1.0455284552845527,
      "grad_norm": 0.01183907687664032,
      "learning_rate": 9.560975609756097e-05,
      "loss": 0.0005,
      "step": 643
    },
    {
      "epoch": 1.0471544715447154,
      "grad_norm": 0.030446937307715416,
      "learning_rate": 9.544715447154472e-05,
      "loss": 0.0009,
      "step": 644
    },
    {
      "epoch": 1.048780487804878,
      "grad_norm": 0.030347781255841255,
      "learning_rate": 9.528455284552846e-05,
      "loss": 0.0015,
      "step": 645
    },
    {
      "epoch": 1.0504065040650405,
      "grad_norm": 0.037201784551143646,
      "learning_rate": 9.51219512195122e-05,
      "loss": 0.0019,
      "step": 646
    },
    {
      "epoch": 1.0520325203252032,
      "grad_norm": 0.0033089458011090755,
      "learning_rate": 9.495934959349594e-05,
      "loss": 0.0002,
      "step": 647
    },
    {
      "epoch": 1.053658536585366,
      "grad_norm": 0.050610434263944626,
      "learning_rate": 9.479674796747968e-05,
      "loss": 0.0035,
      "step": 648
    },
    {
      "epoch": 1.0552845528455284,
      "grad_norm": 0.03404596820473671,
      "learning_rate": 9.463414634146342e-05,
      "loss": 0.0013,
      "step": 649
    },
    {
      "epoch": 1.056910569105691,
      "grad_norm": 0.05695750564336777,
      "learning_rate": 9.447154471544716e-05,
      "loss": 0.0019,
      "step": 650
    },
    {
      "epoch": 1.0585365853658537,
      "grad_norm": 0.02056044153869152,
      "learning_rate": 9.43089430894309e-05,
      "loss": 0.0007,
      "step": 651
    },
    {
      "epoch": 1.0601626016260162,
      "grad_norm": 0.03315839543938637,
      "learning_rate": 9.414634146341463e-05,
      "loss": 0.0025,
      "step": 652
    },
    {
      "epoch": 1.0617886178861788,
      "grad_norm": 0.004039506893604994,
      "learning_rate": 9.398373983739837e-05,
      "loss": 0.0001,
      "step": 653
    },
    {
      "epoch": 1.0634146341463415,
      "grad_norm": 0.034976691007614136,
      "learning_rate": 9.382113821138213e-05,
      "loss": 0.0019,
      "step": 654
    },
    {
      "epoch": 1.065040650406504,
      "grad_norm": 0.024964898824691772,
      "learning_rate": 9.365853658536587e-05,
      "loss": 0.0005,
      "step": 655
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.05335121601819992,
      "learning_rate": 9.349593495934959e-05,
      "loss": 0.0033,
      "step": 656
    },
    {
      "epoch": 1.0682926829268293,
      "grad_norm": 0.08095656335353851,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.0075,
      "step": 657
    },
    {
      "epoch": 1.0699186991869918,
      "grad_norm": 0.002633501309901476,
      "learning_rate": 9.317073170731708e-05,
      "loss": 0.0001,
      "step": 658
    },
    {
      "epoch": 1.0715447154471545,
      "grad_norm": 0.03410814702510834,
      "learning_rate": 9.300813008130082e-05,
      "loss": 0.0018,
      "step": 659
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 0.026944279670715332,
      "learning_rate": 9.284552845528456e-05,
      "loss": 0.0011,
      "step": 660
    },
    {
      "epoch": 1.0747967479674796,
      "grad_norm": 0.05163886770606041,
      "learning_rate": 9.26829268292683e-05,
      "loss": 0.0042,
      "step": 661
    },
    {
      "epoch": 1.0764227642276423,
      "grad_norm": 0.08338946849107742,
      "learning_rate": 9.252032520325204e-05,
      "loss": 0.0038,
      "step": 662
    },
    {
      "epoch": 1.078048780487805,
      "grad_norm": 0.031474269926548004,
      "learning_rate": 9.235772357723578e-05,
      "loss": 0.0012,
      "step": 663
    },
    {
      "epoch": 1.0796747967479674,
      "grad_norm": 0.018869008868932724,
      "learning_rate": 9.219512195121952e-05,
      "loss": 0.0007,
      "step": 664
    },
    {
      "epoch": 1.08130081300813,
      "grad_norm": 0.006479234434664249,
      "learning_rate": 9.203252032520325e-05,
      "loss": 0.0002,
      "step": 665
    },
    {
      "epoch": 1.0829268292682928,
      "grad_norm": 0.030158808454871178,
      "learning_rate": 9.1869918699187e-05,
      "loss": 0.0018,
      "step": 666
    },
    {
      "epoch": 1.0845528455284552,
      "grad_norm": 0.02942316234111786,
      "learning_rate": 9.170731707317075e-05,
      "loss": 0.0015,
      "step": 667
    },
    {
      "epoch": 1.0861788617886179,
      "grad_norm": 0.021807601675391197,
      "learning_rate": 9.154471544715447e-05,
      "loss": 0.0007,
      "step": 668
    },
    {
      "epoch": 1.0878048780487806,
      "grad_norm": 0.0017295482102781534,
      "learning_rate": 9.138211382113821e-05,
      "loss": 0.0001,
      "step": 669
    },
    {
      "epoch": 1.089430894308943,
      "grad_norm": 0.03170632943511009,
      "learning_rate": 9.121951219512196e-05,
      "loss": 0.0013,
      "step": 670
    },
    {
      "epoch": 1.0910569105691057,
      "grad_norm": 0.007462899200618267,
      "learning_rate": 9.105691056910569e-05,
      "loss": 0.0002,
      "step": 671
    },
    {
      "epoch": 1.0926829268292684,
      "grad_norm": 0.020576415583491325,
      "learning_rate": 9.089430894308944e-05,
      "loss": 0.0006,
      "step": 672
    },
    {
      "epoch": 1.0943089430894308,
      "grad_norm": 0.03760772570967674,
      "learning_rate": 9.073170731707318e-05,
      "loss": 0.0019,
      "step": 673
    },
    {
      "epoch": 1.0959349593495935,
      "grad_norm": 0.03197696805000305,
      "learning_rate": 9.05691056910569e-05,
      "loss": 0.0014,
      "step": 674
    },
    {
      "epoch": 1.0975609756097562,
      "grad_norm": 0.01221508253365755,
      "learning_rate": 9.040650406504066e-05,
      "loss": 0.0004,
      "step": 675
    },
    {
      "epoch": 1.0991869918699186,
      "grad_norm": 0.03674213960766792,
      "learning_rate": 9.02439024390244e-05,
      "loss": 0.0015,
      "step": 676
    },
    {
      "epoch": 1.1008130081300813,
      "grad_norm": 0.04501178860664368,
      "learning_rate": 9.008130081300812e-05,
      "loss": 0.0022,
      "step": 677
    },
    {
      "epoch": 1.102439024390244,
      "grad_norm": 0.040992189198732376,
      "learning_rate": 8.991869918699188e-05,
      "loss": 0.0029,
      "step": 678
    },
    {
      "epoch": 1.1040650406504064,
      "grad_norm": 0.012830110266804695,
      "learning_rate": 8.975609756097561e-05,
      "loss": 0.0003,
      "step": 679
    },
    {
      "epoch": 1.1056910569105691,
      "grad_norm": 0.0034372990485280752,
      "learning_rate": 8.959349593495935e-05,
      "loss": 0.0001,
      "step": 680
    },
    {
      "epoch": 1.1073170731707318,
      "grad_norm": 0.0034539075568318367,
      "learning_rate": 8.943089430894309e-05,
      "loss": 0.0001,
      "step": 681
    },
    {
      "epoch": 1.1089430894308943,
      "grad_norm": 0.00249752146191895,
      "learning_rate": 8.926829268292683e-05,
      "loss": 0.0001,
      "step": 682
    },
    {
      "epoch": 1.110569105691057,
      "grad_norm": 0.0013938469346612692,
      "learning_rate": 8.910569105691058e-05,
      "loss": 0.0001,
      "step": 683
    },
    {
      "epoch": 1.1121951219512196,
      "grad_norm": 0.0025972335133701563,
      "learning_rate": 8.894308943089431e-05,
      "loss": 0.0001,
      "step": 684
    },
    {
      "epoch": 1.113821138211382,
      "grad_norm": 0.04472069814801216,
      "learning_rate": 8.878048780487805e-05,
      "loss": 0.0034,
      "step": 685
    },
    {
      "epoch": 1.1154471544715447,
      "grad_norm": 0.01597888022661209,
      "learning_rate": 8.86178861788618e-05,
      "loss": 0.0007,
      "step": 686
    },
    {
      "epoch": 1.1170731707317074,
      "grad_norm": 0.0380178764462471,
      "learning_rate": 8.845528455284553e-05,
      "loss": 0.0011,
      "step": 687
    },
    {
      "epoch": 1.1186991869918699,
      "grad_norm": 0.060590408742427826,
      "learning_rate": 8.829268292682928e-05,
      "loss": 0.002,
      "step": 688
    },
    {
      "epoch": 1.1203252032520326,
      "grad_norm": 0.018474208191037178,
      "learning_rate": 8.813008130081302e-05,
      "loss": 0.0011,
      "step": 689
    },
    {
      "epoch": 1.1219512195121952,
      "grad_norm": 0.05070538818836212,
      "learning_rate": 8.796747967479674e-05,
      "loss": 0.0042,
      "step": 690
    },
    {
      "epoch": 1.1235772357723577,
      "grad_norm": 0.02173328585922718,
      "learning_rate": 8.78048780487805e-05,
      "loss": 0.001,
      "step": 691
    },
    {
      "epoch": 1.1252032520325204,
      "grad_norm": 0.022582316771149635,
      "learning_rate": 8.764227642276423e-05,
      "loss": 0.0014,
      "step": 692
    },
    {
      "epoch": 1.126829268292683,
      "grad_norm": 0.028532950207591057,
      "learning_rate": 8.747967479674797e-05,
      "loss": 0.0013,
      "step": 693
    },
    {
      "epoch": 1.1284552845528455,
      "grad_norm": 0.010103787295520306,
      "learning_rate": 8.731707317073171e-05,
      "loss": 0.0002,
      "step": 694
    },
    {
      "epoch": 1.1300813008130082,
      "grad_norm": 0.0037283343262970448,
      "learning_rate": 8.715447154471545e-05,
      "loss": 0.0002,
      "step": 695
    },
    {
      "epoch": 1.1317073170731708,
      "grad_norm": 0.013854489661753178,
      "learning_rate": 8.699186991869919e-05,
      "loss": 0.0002,
      "step": 696
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.04147953540086746,
      "learning_rate": 8.682926829268293e-05,
      "loss": 0.0013,
      "step": 697
    },
    {
      "epoch": 1.134959349593496,
      "grad_norm": 0.03192045912146568,
      "learning_rate": 8.666666666666667e-05,
      "loss": 0.0013,
      "step": 698
    },
    {
      "epoch": 1.1365853658536587,
      "grad_norm": 0.04152821749448776,
      "learning_rate": 8.650406504065041e-05,
      "loss": 0.0019,
      "step": 699
    },
    {
      "epoch": 1.1382113821138211,
      "grad_norm": 0.02476237155497074,
      "learning_rate": 8.634146341463415e-05,
      "loss": 0.0016,
      "step": 700
    },
    {
      "epoch": 1.1398373983739838,
      "grad_norm": 0.02858577109873295,
      "learning_rate": 8.61788617886179e-05,
      "loss": 0.0013,
      "step": 701
    },
    {
      "epoch": 1.1414634146341462,
      "grad_norm": 0.04394200071692467,
      "learning_rate": 8.601626016260162e-05,
      "loss": 0.0022,
      "step": 702
    },
    {
      "epoch": 1.143089430894309,
      "grad_norm": 0.03343725949525833,
      "learning_rate": 8.585365853658536e-05,
      "loss": 0.0021,
      "step": 703
    },
    {
      "epoch": 1.1447154471544716,
      "grad_norm": 0.03560461848974228,
      "learning_rate": 8.569105691056912e-05,
      "loss": 0.0023,
      "step": 704
    },
    {
      "epoch": 1.146341463414634,
      "grad_norm": 0.04486897587776184,
      "learning_rate": 8.552845528455284e-05,
      "loss": 0.003,
      "step": 705
    },
    {
      "epoch": 1.1479674796747967,
      "grad_norm": 0.0025575978215783834,
      "learning_rate": 8.53658536585366e-05,
      "loss": 0.0001,
      "step": 706
    },
    {
      "epoch": 1.1495934959349594,
      "grad_norm": 0.06467323005199432,
      "learning_rate": 8.520325203252033e-05,
      "loss": 0.0044,
      "step": 707
    },
    {
      "epoch": 1.1512195121951219,
      "grad_norm": 0.034850675612688065,
      "learning_rate": 8.504065040650407e-05,
      "loss": 0.0016,
      "step": 708
    },
    {
      "epoch": 1.1528455284552845,
      "grad_norm": 0.03509968891739845,
      "learning_rate": 8.487804878048781e-05,
      "loss": 0.0026,
      "step": 709
    },
    {
      "epoch": 1.1544715447154472,
      "grad_norm": 0.016252266243100166,
      "learning_rate": 8.471544715447155e-05,
      "loss": 0.0006,
      "step": 710
    },
    {
      "epoch": 1.1560975609756097,
      "grad_norm": 0.020878033712506294,
      "learning_rate": 8.455284552845529e-05,
      "loss": 0.0008,
      "step": 711
    },
    {
      "epoch": 1.1577235772357723,
      "grad_norm": 0.019726216793060303,
      "learning_rate": 8.439024390243903e-05,
      "loss": 0.0008,
      "step": 712
    },
    {
      "epoch": 1.159349593495935,
      "grad_norm": 0.01386511791497469,
      "learning_rate": 8.422764227642277e-05,
      "loss": 0.0004,
      "step": 713
    },
    {
      "epoch": 1.1609756097560975,
      "grad_norm": 0.026266317814588547,
      "learning_rate": 8.406504065040652e-05,
      "loss": 0.0011,
      "step": 714
    },
    {
      "epoch": 1.1626016260162602,
      "grad_norm": 0.049737490713596344,
      "learning_rate": 8.390243902439024e-05,
      "loss": 0.0039,
      "step": 715
    },
    {
      "epoch": 1.1642276422764228,
      "grad_norm": 0.022305233404040337,
      "learning_rate": 8.373983739837398e-05,
      "loss": 0.0008,
      "step": 716
    },
    {
      "epoch": 1.1658536585365853,
      "grad_norm": 0.0281287282705307,
      "learning_rate": 8.357723577235774e-05,
      "loss": 0.0013,
      "step": 717
    },
    {
      "epoch": 1.167479674796748,
      "grad_norm": 0.05900326743721962,
      "learning_rate": 8.341463414634146e-05,
      "loss": 0.0025,
      "step": 718
    },
    {
      "epoch": 1.1691056910569106,
      "grad_norm": 0.023097706958651543,
      "learning_rate": 8.325203252032521e-05,
      "loss": 0.001,
      "step": 719
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 0.011109421029686928,
      "learning_rate": 8.308943089430895e-05,
      "loss": 0.0003,
      "step": 720
    },
    {
      "epoch": 1.1723577235772358,
      "grad_norm": 0.01939593441784382,
      "learning_rate": 8.292682926829268e-05,
      "loss": 0.0004,
      "step": 721
    },
    {
      "epoch": 1.1739837398373985,
      "grad_norm": 0.0554330088198185,
      "learning_rate": 8.276422764227643e-05,
      "loss": 0.0045,
      "step": 722
    },
    {
      "epoch": 1.175609756097561,
      "grad_norm": 0.007366561330854893,
      "learning_rate": 8.260162601626017e-05,
      "loss": 0.0002,
      "step": 723
    },
    {
      "epoch": 1.1772357723577236,
      "grad_norm": 0.04526344686746597,
      "learning_rate": 8.243902439024391e-05,
      "loss": 0.0035,
      "step": 724
    },
    {
      "epoch": 1.1788617886178863,
      "grad_norm": 0.0029018132481724024,
      "learning_rate": 8.227642276422765e-05,
      "loss": 0.0001,
      "step": 725
    },
    {
      "epoch": 1.1804878048780487,
      "grad_norm": 0.017720013856887817,
      "learning_rate": 8.211382113821139e-05,
      "loss": 0.0004,
      "step": 726
    },
    {
      "epoch": 1.1821138211382114,
      "grad_norm": 0.005069501232355833,
      "learning_rate": 8.195121951219513e-05,
      "loss": 0.0002,
      "step": 727
    },
    {
      "epoch": 1.183739837398374,
      "grad_norm": 0.02632121555507183,
      "learning_rate": 8.178861788617886e-05,
      "loss": 0.0007,
      "step": 728
    },
    {
      "epoch": 1.1853658536585365,
      "grad_norm": 0.037539489567279816,
      "learning_rate": 8.16260162601626e-05,
      "loss": 0.0023,
      "step": 729
    },
    {
      "epoch": 1.1869918699186992,
      "grad_norm": 0.026132963597774506,
      "learning_rate": 8.146341463414634e-05,
      "loss": 0.0009,
      "step": 730
    },
    {
      "epoch": 1.1886178861788619,
      "grad_norm": 0.021137988194823265,
      "learning_rate": 8.130081300813008e-05,
      "loss": 0.0007,
      "step": 731
    },
    {
      "epoch": 1.1902439024390243,
      "grad_norm": 0.015491721220314503,
      "learning_rate": 8.113821138211382e-05,
      "loss": 0.0004,
      "step": 732
    },
    {
      "epoch": 1.191869918699187,
      "grad_norm": 0.04042433947324753,
      "learning_rate": 8.097560975609757e-05,
      "loss": 0.0024,
      "step": 733
    },
    {
      "epoch": 1.1934959349593495,
      "grad_norm": 0.03946014866232872,
      "learning_rate": 8.08130081300813e-05,
      "loss": 0.0021,
      "step": 734
    },
    {
      "epoch": 1.1951219512195121,
      "grad_norm": 0.015603763982653618,
      "learning_rate": 8.065040650406505e-05,
      "loss": 0.0007,
      "step": 735
    },
    {
      "epoch": 1.1967479674796748,
      "grad_norm": 0.025023657828569412,
      "learning_rate": 8.048780487804879e-05,
      "loss": 0.0014,
      "step": 736
    },
    {
      "epoch": 1.1983739837398373,
      "grad_norm": 0.02182937040925026,
      "learning_rate": 8.032520325203252e-05,
      "loss": 0.0009,
      "step": 737
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.03864087164402008,
      "learning_rate": 8.016260162601627e-05,
      "loss": 0.0021,
      "step": 738
    },
    {
      "epoch": 1.2016260162601626,
      "grad_norm": 0.07311253249645233,
      "learning_rate": 8e-05,
      "loss": 0.0039,
      "step": 739
    },
    {
      "epoch": 1.203252032520325,
      "grad_norm": 0.040421824902296066,
      "learning_rate": 7.983739837398375e-05,
      "loss": 0.0019,
      "step": 740
    },
    {
      "epoch": 1.2048780487804878,
      "grad_norm": 0.03150799125432968,
      "learning_rate": 7.967479674796748e-05,
      "loss": 0.0022,
      "step": 741
    },
    {
      "epoch": 1.2065040650406504,
      "grad_norm": 0.016581544652581215,
      "learning_rate": 7.951219512195122e-05,
      "loss": 0.0008,
      "step": 742
    },
    {
      "epoch": 1.208130081300813,
      "grad_norm": 0.02439907006919384,
      "learning_rate": 7.934959349593496e-05,
      "loss": 0.0007,
      "step": 743
    },
    {
      "epoch": 1.2097560975609756,
      "grad_norm": 0.03728294372558594,
      "learning_rate": 7.91869918699187e-05,
      "loss": 0.0021,
      "step": 744
    },
    {
      "epoch": 1.2113821138211383,
      "grad_norm": 0.06997398287057877,
      "learning_rate": 7.902439024390244e-05,
      "loss": 0.0057,
      "step": 745
    },
    {
      "epoch": 1.2130081300813007,
      "grad_norm": 0.024203304201364517,
      "learning_rate": 7.886178861788618e-05,
      "loss": 0.001,
      "step": 746
    },
    {
      "epoch": 1.2146341463414634,
      "grad_norm": 0.04174242168664932,
      "learning_rate": 7.869918699186992e-05,
      "loss": 0.0031,
      "step": 747
    },
    {
      "epoch": 1.216260162601626,
      "grad_norm": 0.0011190110817551613,
      "learning_rate": 7.853658536585367e-05,
      "loss": 0.0001,
      "step": 748
    },
    {
      "epoch": 1.2178861788617885,
      "grad_norm": 0.031231727451086044,
      "learning_rate": 7.83739837398374e-05,
      "loss": 0.0011,
      "step": 749
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 0.029292035847902298,
      "learning_rate": 7.821138211382114e-05,
      "loss": 0.0016,
      "step": 750
    },
    {
      "epoch": 1.2211382113821139,
      "grad_norm": 0.02659698761999607,
      "learning_rate": 7.804878048780489e-05,
      "loss": 0.0011,
      "step": 751
    },
    {
      "epoch": 1.2227642276422763,
      "grad_norm": 0.016134900972247124,
      "learning_rate": 7.788617886178861e-05,
      "loss": 0.0002,
      "step": 752
    },
    {
      "epoch": 1.224390243902439,
      "grad_norm": 0.04828726872801781,
      "learning_rate": 7.772357723577237e-05,
      "loss": 0.0031,
      "step": 753
    },
    {
      "epoch": 1.2260162601626017,
      "grad_norm": 0.02658683806657791,
      "learning_rate": 7.75609756097561e-05,
      "loss": 0.0012,
      "step": 754
    },
    {
      "epoch": 1.2276422764227641,
      "grad_norm": 0.01646493747830391,
      "learning_rate": 7.739837398373983e-05,
      "loss": 0.0004,
      "step": 755
    },
    {
      "epoch": 1.2292682926829268,
      "grad_norm": 0.03583763167262077,
      "learning_rate": 7.723577235772358e-05,
      "loss": 0.0009,
      "step": 756
    },
    {
      "epoch": 1.2308943089430895,
      "grad_norm": 0.05903599038720131,
      "learning_rate": 7.707317073170732e-05,
      "loss": 0.0022,
      "step": 757
    },
    {
      "epoch": 1.232520325203252,
      "grad_norm": 0.06352763622999191,
      "learning_rate": 7.691056910569106e-05,
      "loss": 0.0019,
      "step": 758
    },
    {
      "epoch": 1.2341463414634146,
      "grad_norm": 0.021526366472244263,
      "learning_rate": 7.67479674796748e-05,
      "loss": 0.0009,
      "step": 759
    },
    {
      "epoch": 1.2357723577235773,
      "grad_norm": 0.0210378747433424,
      "learning_rate": 7.658536585365854e-05,
      "loss": 0.0006,
      "step": 760
    },
    {
      "epoch": 1.2373983739837398,
      "grad_norm": 0.03712065890431404,
      "learning_rate": 7.642276422764229e-05,
      "loss": 0.0011,
      "step": 761
    },
    {
      "epoch": 1.2390243902439024,
      "grad_norm": 0.026434071362018585,
      "learning_rate": 7.626016260162602e-05,
      "loss": 0.0009,
      "step": 762
    },
    {
      "epoch": 1.240650406504065,
      "grad_norm": 0.010956654325127602,
      "learning_rate": 7.609756097560976e-05,
      "loss": 0.0003,
      "step": 763
    },
    {
      "epoch": 1.2422764227642276,
      "grad_norm": 0.005856562405824661,
      "learning_rate": 7.593495934959351e-05,
      "loss": 0.0003,
      "step": 764
    },
    {
      "epoch": 1.2439024390243902,
      "grad_norm": 0.01852349378168583,
      "learning_rate": 7.577235772357723e-05,
      "loss": 0.0009,
      "step": 765
    },
    {
      "epoch": 1.245528455284553,
      "grad_norm": 0.029882092028856277,
      "learning_rate": 7.560975609756099e-05,
      "loss": 0.0014,
      "step": 766
    },
    {
      "epoch": 1.2471544715447154,
      "grad_norm": 0.03697939217090607,
      "learning_rate": 7.544715447154472e-05,
      "loss": 0.0025,
      "step": 767
    },
    {
      "epoch": 1.248780487804878,
      "grad_norm": 0.042975228279829025,
      "learning_rate": 7.528455284552845e-05,
      "loss": 0.0017,
      "step": 768
    },
    {
      "epoch": 1.2504065040650407,
      "grad_norm": 0.047283969819545746,
      "learning_rate": 7.51219512195122e-05,
      "loss": 0.0022,
      "step": 769
    },
    {
      "epoch": 1.2520325203252032,
      "grad_norm": 0.022287802770733833,
      "learning_rate": 7.495934959349594e-05,
      "loss": 0.0008,
      "step": 770
    },
    {
      "epoch": 1.2536585365853659,
      "grad_norm": 0.03897371515631676,
      "learning_rate": 7.479674796747968e-05,
      "loss": 0.0026,
      "step": 771
    },
    {
      "epoch": 1.2552845528455285,
      "grad_norm": 0.06182275712490082,
      "learning_rate": 7.463414634146342e-05,
      "loss": 0.0029,
      "step": 772
    },
    {
      "epoch": 1.256910569105691,
      "grad_norm": 0.028782378882169724,
      "learning_rate": 7.447154471544716e-05,
      "loss": 0.0014,
      "step": 773
    },
    {
      "epoch": 1.2585365853658537,
      "grad_norm": 0.013463403098285198,
      "learning_rate": 7.43089430894309e-05,
      "loss": 0.0002,
      "step": 774
    },
    {
      "epoch": 1.2601626016260163,
      "grad_norm": 0.04454164579510689,
      "learning_rate": 7.414634146341464e-05,
      "loss": 0.0034,
      "step": 775
    },
    {
      "epoch": 1.2617886178861788,
      "grad_norm": 0.018551746383309364,
      "learning_rate": 7.398373983739838e-05,
      "loss": 0.0007,
      "step": 776
    },
    {
      "epoch": 1.2634146341463415,
      "grad_norm": 0.04346828535199165,
      "learning_rate": 7.382113821138211e-05,
      "loss": 0.002,
      "step": 777
    },
    {
      "epoch": 1.2650406504065042,
      "grad_norm": 0.010087680071592331,
      "learning_rate": 7.365853658536585e-05,
      "loss": 0.0003,
      "step": 778
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.03682637959718704,
      "learning_rate": 7.34959349593496e-05,
      "loss": 0.0014,
      "step": 779
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 0.059697359800338745,
      "learning_rate": 7.333333333333333e-05,
      "loss": 0.0029,
      "step": 780
    },
    {
      "epoch": 1.269918699186992,
      "grad_norm": 0.010341721586883068,
      "learning_rate": 7.317073170731707e-05,
      "loss": 0.0003,
      "step": 781
    },
    {
      "epoch": 1.2715447154471544,
      "grad_norm": 0.016788313165307045,
      "learning_rate": 7.300813008130082e-05,
      "loss": 0.0007,
      "step": 782
    },
    {
      "epoch": 1.273170731707317,
      "grad_norm": 0.012154350988566875,
      "learning_rate": 7.284552845528456e-05,
      "loss": 0.0004,
      "step": 783
    },
    {
      "epoch": 1.2747967479674798,
      "grad_norm": 0.03602955490350723,
      "learning_rate": 7.26829268292683e-05,
      "loss": 0.0014,
      "step": 784
    },
    {
      "epoch": 1.2764227642276422,
      "grad_norm": 0.001596103422343731,
      "learning_rate": 7.252032520325204e-05,
      "loss": 0.0001,
      "step": 785
    },
    {
      "epoch": 1.278048780487805,
      "grad_norm": 0.015112660825252533,
      "learning_rate": 7.235772357723578e-05,
      "loss": 0.0004,
      "step": 786
    },
    {
      "epoch": 1.2796747967479676,
      "grad_norm": 0.03566775098443031,
      "learning_rate": 7.219512195121952e-05,
      "loss": 0.0017,
      "step": 787
    },
    {
      "epoch": 1.28130081300813,
      "grad_norm": 0.03487718477845192,
      "learning_rate": 7.203252032520326e-05,
      "loss": 0.0023,
      "step": 788
    },
    {
      "epoch": 1.2829268292682927,
      "grad_norm": 0.05723344534635544,
      "learning_rate": 7.1869918699187e-05,
      "loss": 0.0043,
      "step": 789
    },
    {
      "epoch": 1.2845528455284554,
      "grad_norm": 0.041066400706768036,
      "learning_rate": 7.170731707317073e-05,
      "loss": 0.0016,
      "step": 790
    },
    {
      "epoch": 1.2861788617886178,
      "grad_norm": 0.017240650951862335,
      "learning_rate": 7.154471544715447e-05,
      "loss": 0.0007,
      "step": 791
    },
    {
      "epoch": 1.2878048780487805,
      "grad_norm": 0.0021571707911789417,
      "learning_rate": 7.138211382113821e-05,
      "loss": 0.0001,
      "step": 792
    },
    {
      "epoch": 1.2894308943089432,
      "grad_norm": 0.0017902662511914968,
      "learning_rate": 7.121951219512195e-05,
      "loss": 0.0001,
      "step": 793
    },
    {
      "epoch": 1.2910569105691057,
      "grad_norm": 0.01766727864742279,
      "learning_rate": 7.105691056910569e-05,
      "loss": 0.0005,
      "step": 794
    },
    {
      "epoch": 1.2926829268292683,
      "grad_norm": 0.032771240919828415,
      "learning_rate": 7.089430894308944e-05,
      "loss": 0.0013,
      "step": 795
    },
    {
      "epoch": 1.294308943089431,
      "grad_norm": 0.010348583571612835,
      "learning_rate": 7.073170731707317e-05,
      "loss": 0.0003,
      "step": 796
    },
    {
      "epoch": 1.2959349593495935,
      "grad_norm": 0.029158087447285652,
      "learning_rate": 7.056910569105691e-05,
      "loss": 0.0014,
      "step": 797
    },
    {
      "epoch": 1.2975609756097561,
      "grad_norm": 0.034180156886577606,
      "learning_rate": 7.040650406504066e-05,
      "loss": 0.0016,
      "step": 798
    },
    {
      "epoch": 1.2991869918699188,
      "grad_norm": 0.07978436350822449,
      "learning_rate": 7.024390243902439e-05,
      "loss": 0.0041,
      "step": 799
    },
    {
      "epoch": 1.3008130081300813,
      "grad_norm": 0.03150654211640358,
      "learning_rate": 7.008130081300814e-05,
      "loss": 0.0016,
      "step": 800
    },
    {
      "epoch": 1.302439024390244,
      "grad_norm": 0.03474998101592064,
      "learning_rate": 6.991869918699188e-05,
      "loss": 0.0024,
      "step": 801
    },
    {
      "epoch": 1.3040650406504066,
      "grad_norm": 0.02554975263774395,
      "learning_rate": 6.97560975609756e-05,
      "loss": 0.0008,
      "step": 802
    },
    {
      "epoch": 1.305691056910569,
      "grad_norm": 0.019186638295650482,
      "learning_rate": 6.959349593495935e-05,
      "loss": 0.0005,
      "step": 803
    },
    {
      "epoch": 1.3073170731707318,
      "grad_norm": 0.033523060381412506,
      "learning_rate": 6.94308943089431e-05,
      "loss": 0.0016,
      "step": 804
    },
    {
      "epoch": 1.3089430894308944,
      "grad_norm": 0.035054828971624374,
      "learning_rate": 6.926829268292683e-05,
      "loss": 0.0004,
      "step": 805
    },
    {
      "epoch": 1.310569105691057,
      "grad_norm": 0.022861730307340622,
      "learning_rate": 6.910569105691057e-05,
      "loss": 0.0012,
      "step": 806
    },
    {
      "epoch": 1.3121951219512196,
      "grad_norm": 0.012856470420956612,
      "learning_rate": 6.894308943089431e-05,
      "loss": 0.0003,
      "step": 807
    },
    {
      "epoch": 1.3138211382113822,
      "grad_norm": 0.030541488900780678,
      "learning_rate": 6.878048780487805e-05,
      "loss": 0.0017,
      "step": 808
    },
    {
      "epoch": 1.3154471544715447,
      "grad_norm": 0.05276956781744957,
      "learning_rate": 6.861788617886179e-05,
      "loss": 0.0029,
      "step": 809
    },
    {
      "epoch": 1.3170731707317074,
      "grad_norm": 0.018205706030130386,
      "learning_rate": 6.845528455284553e-05,
      "loss": 0.0005,
      "step": 810
    },
    {
      "epoch": 1.31869918699187,
      "grad_norm": 0.0018701760563999414,
      "learning_rate": 6.829268292682928e-05,
      "loss": 0.0001,
      "step": 811
    },
    {
      "epoch": 1.3203252032520325,
      "grad_norm": 0.017087047919631004,
      "learning_rate": 6.8130081300813e-05,
      "loss": 0.0003,
      "step": 812
    },
    {
      "epoch": 1.3219512195121952,
      "grad_norm": 0.03171645104885101,
      "learning_rate": 6.796747967479676e-05,
      "loss": 0.0015,
      "step": 813
    },
    {
      "epoch": 1.3235772357723576,
      "grad_norm": 0.03122306615114212,
      "learning_rate": 6.78048780487805e-05,
      "loss": 0.0005,
      "step": 814
    },
    {
      "epoch": 1.3252032520325203,
      "grad_norm": 0.08359325677156448,
      "learning_rate": 6.764227642276422e-05,
      "loss": 0.0049,
      "step": 815
    },
    {
      "epoch": 1.326829268292683,
      "grad_norm": 0.009844671934843063,
      "learning_rate": 6.747967479674798e-05,
      "loss": 0.0003,
      "step": 816
    },
    {
      "epoch": 1.3284552845528455,
      "grad_norm": 0.012536824680864811,
      "learning_rate": 6.731707317073171e-05,
      "loss": 0.0005,
      "step": 817
    },
    {
      "epoch": 1.3300813008130081,
      "grad_norm": 0.04950235038995743,
      "learning_rate": 6.715447154471545e-05,
      "loss": 0.003,
      "step": 818
    },
    {
      "epoch": 1.3317073170731708,
      "grad_norm": 0.01930229179561138,
      "learning_rate": 6.699186991869919e-05,
      "loss": 0.0008,
      "step": 819
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.0014146368484944105,
      "learning_rate": 6.682926829268293e-05,
      "loss": 0.0001,
      "step": 820
    },
    {
      "epoch": 1.334959349593496,
      "grad_norm": 0.005265209823846817,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.0002,
      "step": 821
    },
    {
      "epoch": 1.3365853658536586,
      "grad_norm": 0.010611909441649914,
      "learning_rate": 6.650406504065041e-05,
      "loss": 0.0003,
      "step": 822
    },
    {
      "epoch": 1.338211382113821,
      "grad_norm": 0.016814876347780228,
      "learning_rate": 6.634146341463415e-05,
      "loss": 0.0005,
      "step": 823
    },
    {
      "epoch": 1.3398373983739837,
      "grad_norm": 0.034960780292749405,
      "learning_rate": 6.617886178861789e-05,
      "loss": 0.0013,
      "step": 824
    },
    {
      "epoch": 1.3414634146341464,
      "grad_norm": 0.05859651789069176,
      "learning_rate": 6.601626016260163e-05,
      "loss": 0.0068,
      "step": 825
    },
    {
      "epoch": 1.3430894308943089,
      "grad_norm": 0.026598798111081123,
      "learning_rate": 6.585365853658538e-05,
      "loss": 0.001,
      "step": 826
    },
    {
      "epoch": 1.3447154471544716,
      "grad_norm": 0.026956040412187576,
      "learning_rate": 6.56910569105691e-05,
      "loss": 0.0008,
      "step": 827
    },
    {
      "epoch": 1.346341463414634,
      "grad_norm": 0.04086047783493996,
      "learning_rate": 6.552845528455284e-05,
      "loss": 0.0016,
      "step": 828
    },
    {
      "epoch": 1.3479674796747967,
      "grad_norm": 0.03592829033732414,
      "learning_rate": 6.53658536585366e-05,
      "loss": 0.0019,
      "step": 829
    },
    {
      "epoch": 1.3495934959349594,
      "grad_norm": 0.017650289461016655,
      "learning_rate": 6.520325203252032e-05,
      "loss": 0.0006,
      "step": 830
    },
    {
      "epoch": 1.3512195121951218,
      "grad_norm": 0.035161375999450684,
      "learning_rate": 6.504065040650407e-05,
      "loss": 0.0015,
      "step": 831
    },
    {
      "epoch": 1.3528455284552845,
      "grad_norm": 0.002574712038040161,
      "learning_rate": 6.487804878048781e-05,
      "loss": 0.0001,
      "step": 832
    },
    {
      "epoch": 1.3544715447154472,
      "grad_norm": 0.0038868507836014032,
      "learning_rate": 6.471544715447154e-05,
      "loss": 0.0001,
      "step": 833
    },
    {
      "epoch": 1.3560975609756096,
      "grad_norm": 0.03635488077998161,
      "learning_rate": 6.455284552845529e-05,
      "loss": 0.0018,
      "step": 834
    },
    {
      "epoch": 1.3577235772357723,
      "grad_norm": 0.007008187007158995,
      "learning_rate": 6.439024390243903e-05,
      "loss": 0.0002,
      "step": 835
    },
    {
      "epoch": 1.359349593495935,
      "grad_norm": 0.0014871800085529685,
      "learning_rate": 6.422764227642277e-05,
      "loss": 0.0001,
      "step": 836
    },
    {
      "epoch": 1.3609756097560974,
      "grad_norm": 0.046621985733509064,
      "learning_rate": 6.406504065040651e-05,
      "loss": 0.0015,
      "step": 837
    },
    {
      "epoch": 1.3626016260162601,
      "grad_norm": 0.013912569731473923,
      "learning_rate": 6.390243902439025e-05,
      "loss": 0.0004,
      "step": 838
    },
    {
      "epoch": 1.3642276422764228,
      "grad_norm": 0.033401042222976685,
      "learning_rate": 6.373983739837398e-05,
      "loss": 0.0025,
      "step": 839
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 0.025863932445645332,
      "learning_rate": 6.357723577235772e-05,
      "loss": 0.0011,
      "step": 840
    },
    {
      "epoch": 1.367479674796748,
      "grad_norm": 0.04431946575641632,
      "learning_rate": 6.341463414634146e-05,
      "loss": 0.0016,
      "step": 841
    },
    {
      "epoch": 1.3691056910569106,
      "grad_norm": 0.0409301295876503,
      "learning_rate": 6.325203252032522e-05,
      "loss": 0.0013,
      "step": 842
    },
    {
      "epoch": 1.370731707317073,
      "grad_norm": 0.0290212444961071,
      "learning_rate": 6.308943089430894e-05,
      "loss": 0.0017,
      "step": 843
    },
    {
      "epoch": 1.3723577235772357,
      "grad_norm": 0.024670831859111786,
      "learning_rate": 6.292682926829268e-05,
      "loss": 0.0011,
      "step": 844
    },
    {
      "epoch": 1.3739837398373984,
      "grad_norm": 0.032729215919971466,
      "learning_rate": 6.276422764227643e-05,
      "loss": 0.0018,
      "step": 845
    },
    {
      "epoch": 1.3756097560975609,
      "grad_norm": 0.01594599336385727,
      "learning_rate": 6.260162601626016e-05,
      "loss": 0.0005,
      "step": 846
    },
    {
      "epoch": 1.3772357723577235,
      "grad_norm": 0.045273423194885254,
      "learning_rate": 6.243902439024391e-05,
      "loss": 0.0019,
      "step": 847
    },
    {
      "epoch": 1.3788617886178862,
      "grad_norm": 0.0294855497777462,
      "learning_rate": 6.227642276422765e-05,
      "loss": 0.0007,
      "step": 848
    },
    {
      "epoch": 1.3804878048780487,
      "grad_norm": 0.006356599275022745,
      "learning_rate": 6.211382113821137e-05,
      "loss": 0.0002,
      "step": 849
    },
    {
      "epoch": 1.3821138211382114,
      "grad_norm": 0.0025189686566591263,
      "learning_rate": 6.195121951219513e-05,
      "loss": 0.0001,
      "step": 850
    },
    {
      "epoch": 1.383739837398374,
      "grad_norm": 0.03501362353563309,
      "learning_rate": 6.178861788617887e-05,
      "loss": 0.0012,
      "step": 851
    },
    {
      "epoch": 1.3853658536585365,
      "grad_norm": 0.03845740854740143,
      "learning_rate": 6.16260162601626e-05,
      "loss": 0.0017,
      "step": 852
    },
    {
      "epoch": 1.3869918699186992,
      "grad_norm": 0.0071070557460188866,
      "learning_rate": 6.146341463414634e-05,
      "loss": 0.0002,
      "step": 853
    },
    {
      "epoch": 1.3886178861788618,
      "grad_norm": 0.020770419389009476,
      "learning_rate": 6.130081300813008e-05,
      "loss": 0.0005,
      "step": 854
    },
    {
      "epoch": 1.3902439024390243,
      "grad_norm": 0.021135680377483368,
      "learning_rate": 6.113821138211382e-05,
      "loss": 0.0006,
      "step": 855
    },
    {
      "epoch": 1.391869918699187,
      "grad_norm": 0.029920637607574463,
      "learning_rate": 6.097560975609756e-05,
      "loss": 0.0024,
      "step": 856
    },
    {
      "epoch": 1.3934959349593496,
      "grad_norm": 0.03999887779355049,
      "learning_rate": 6.081300813008131e-05,
      "loss": 0.0017,
      "step": 857
    },
    {
      "epoch": 1.395121951219512,
      "grad_norm": 0.02911357209086418,
      "learning_rate": 6.065040650406504e-05,
      "loss": 0.0017,
      "step": 858
    },
    {
      "epoch": 1.3967479674796748,
      "grad_norm": 0.0029607857577502728,
      "learning_rate": 6.0487804878048785e-05,
      "loss": 0.0001,
      "step": 859
    },
    {
      "epoch": 1.3983739837398375,
      "grad_norm": 0.049892302602529526,
      "learning_rate": 6.0325203252032524e-05,
      "loss": 0.0021,
      "step": 860
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.026736320927739143,
      "learning_rate": 6.016260162601627e-05,
      "loss": 0.001,
      "step": 861
    },
    {
      "epoch": 1.4016260162601626,
      "grad_norm": 0.03711846098303795,
      "learning_rate": 6e-05,
      "loss": 0.0019,
      "step": 862
    },
    {
      "epoch": 1.4032520325203253,
      "grad_norm": 0.023400111123919487,
      "learning_rate": 5.983739837398375e-05,
      "loss": 0.0007,
      "step": 863
    },
    {
      "epoch": 1.4048780487804877,
      "grad_norm": 0.027883222326636314,
      "learning_rate": 5.9674796747967486e-05,
      "loss": 0.0011,
      "step": 864
    },
    {
      "epoch": 1.4065040650406504,
      "grad_norm": 0.03472716733813286,
      "learning_rate": 5.951219512195122e-05,
      "loss": 0.0017,
      "step": 865
    },
    {
      "epoch": 1.408130081300813,
      "grad_norm": 0.016840392723679543,
      "learning_rate": 5.9349593495934964e-05,
      "loss": 0.0008,
      "step": 866
    },
    {
      "epoch": 1.4097560975609755,
      "grad_norm": 0.02682189829647541,
      "learning_rate": 5.91869918699187e-05,
      "loss": 0.0009,
      "step": 867
    },
    {
      "epoch": 1.4113821138211382,
      "grad_norm": 0.01446550339460373,
      "learning_rate": 5.902439024390244e-05,
      "loss": 0.0005,
      "step": 868
    },
    {
      "epoch": 1.4130081300813009,
      "grad_norm": 0.0024712667800486088,
      "learning_rate": 5.886178861788618e-05,
      "loss": 0.0001,
      "step": 869
    },
    {
      "epoch": 1.4146341463414633,
      "grad_norm": 0.02058710902929306,
      "learning_rate": 5.869918699186993e-05,
      "loss": 0.0006,
      "step": 870
    },
    {
      "epoch": 1.416260162601626,
      "grad_norm": 0.014165706001222134,
      "learning_rate": 5.853658536585366e-05,
      "loss": 0.0005,
      "step": 871
    },
    {
      "epoch": 1.4178861788617887,
      "grad_norm": 0.052673786878585815,
      "learning_rate": 5.83739837398374e-05,
      "loss": 0.0018,
      "step": 872
    },
    {
      "epoch": 1.4195121951219511,
      "grad_norm": 0.01455094013363123,
      "learning_rate": 5.8211382113821144e-05,
      "loss": 0.0005,
      "step": 873
    },
    {
      "epoch": 1.4211382113821138,
      "grad_norm": 0.03104962222278118,
      "learning_rate": 5.8048780487804876e-05,
      "loss": 0.0007,
      "step": 874
    },
    {
      "epoch": 1.4227642276422765,
      "grad_norm": 0.02777290903031826,
      "learning_rate": 5.788617886178862e-05,
      "loss": 0.0008,
      "step": 875
    },
    {
      "epoch": 1.424390243902439,
      "grad_norm": 0.04508015140891075,
      "learning_rate": 5.772357723577236e-05,
      "loss": 0.0025,
      "step": 876
    },
    {
      "epoch": 1.4260162601626016,
      "grad_norm": 0.012500105425715446,
      "learning_rate": 5.756097560975609e-05,
      "loss": 0.0005,
      "step": 877
    },
    {
      "epoch": 1.4276422764227643,
      "grad_norm": 0.019412418827414513,
      "learning_rate": 5.739837398373984e-05,
      "loss": 0.001,
      "step": 878
    },
    {
      "epoch": 1.4292682926829268,
      "grad_norm": 0.015331138856709003,
      "learning_rate": 5.7235772357723584e-05,
      "loss": 0.0005,
      "step": 879
    },
    {
      "epoch": 1.4308943089430894,
      "grad_norm": 0.017977464944124222,
      "learning_rate": 5.7073170731707317e-05,
      "loss": 0.0005,
      "step": 880
    },
    {
      "epoch": 1.4325203252032521,
      "grad_norm": 0.00481083057820797,
      "learning_rate": 5.6910569105691056e-05,
      "loss": 0.0001,
      "step": 881
    },
    {
      "epoch": 1.4341463414634146,
      "grad_norm": 0.004120458383113146,
      "learning_rate": 5.67479674796748e-05,
      "loss": 0.0001,
      "step": 882
    },
    {
      "epoch": 1.4357723577235773,
      "grad_norm": 0.018505482003092766,
      "learning_rate": 5.6585365853658533e-05,
      "loss": 0.0007,
      "step": 883
    },
    {
      "epoch": 1.43739837398374,
      "grad_norm": 0.04924919828772545,
      "learning_rate": 5.642276422764228e-05,
      "loss": 0.0017,
      "step": 884
    },
    {
      "epoch": 1.4390243902439024,
      "grad_norm": 0.041665174067020416,
      "learning_rate": 5.626016260162602e-05,
      "loss": 0.0027,
      "step": 885
    },
    {
      "epoch": 1.440650406504065,
      "grad_norm": 0.045501165091991425,
      "learning_rate": 5.6097560975609764e-05,
      "loss": 0.0026,
      "step": 886
    },
    {
      "epoch": 1.4422764227642277,
      "grad_norm": 0.062135424464941025,
      "learning_rate": 5.5934959349593496e-05,
      "loss": 0.0051,
      "step": 887
    },
    {
      "epoch": 1.4439024390243902,
      "grad_norm": 0.035114239901304245,
      "learning_rate": 5.577235772357724e-05,
      "loss": 0.0019,
      "step": 888
    },
    {
      "epoch": 1.4455284552845529,
      "grad_norm": 0.01891905628144741,
      "learning_rate": 5.560975609756098e-05,
      "loss": 0.0006,
      "step": 889
    },
    {
      "epoch": 1.4471544715447155,
      "grad_norm": 0.04631790146231651,
      "learning_rate": 5.544715447154471e-05,
      "loss": 0.002,
      "step": 890
    },
    {
      "epoch": 1.448780487804878,
      "grad_norm": 0.024357866495847702,
      "learning_rate": 5.528455284552846e-05,
      "loss": 0.001,
      "step": 891
    },
    {
      "epoch": 1.4504065040650407,
      "grad_norm": 0.02210555598139763,
      "learning_rate": 5.5121951219512205e-05,
      "loss": 0.0015,
      "step": 892
    },
    {
      "epoch": 1.4520325203252034,
      "grad_norm": 0.01836499385535717,
      "learning_rate": 5.495934959349594e-05,
      "loss": 0.0008,
      "step": 893
    },
    {
      "epoch": 1.4536585365853658,
      "grad_norm": 0.0019926901441067457,
      "learning_rate": 5.4796747967479676e-05,
      "loss": 0.0001,
      "step": 894
    },
    {
      "epoch": 1.4552845528455285,
      "grad_norm": 0.05936133861541748,
      "learning_rate": 5.463414634146342e-05,
      "loss": 0.0042,
      "step": 895
    },
    {
      "epoch": 1.4569105691056912,
      "grad_norm": 0.0028400460723787546,
      "learning_rate": 5.4471544715447154e-05,
      "loss": 0.0001,
      "step": 896
    },
    {
      "epoch": 1.4585365853658536,
      "grad_norm": 0.022414326667785645,
      "learning_rate": 5.43089430894309e-05,
      "loss": 0.0006,
      "step": 897
    },
    {
      "epoch": 1.4601626016260163,
      "grad_norm": 0.0011914061615243554,
      "learning_rate": 5.414634146341464e-05,
      "loss": 0.0001,
      "step": 898
    },
    {
      "epoch": 1.461788617886179,
      "grad_norm": 0.0057814219035208225,
      "learning_rate": 5.398373983739837e-05,
      "loss": 0.0001,
      "step": 899
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 0.020270945504307747,
      "learning_rate": 5.3821138211382116e-05,
      "loss": 0.0006,
      "step": 900
    },
    {
      "epoch": 1.465040650406504,
      "grad_norm": 0.03846568241715431,
      "learning_rate": 5.365853658536586e-05,
      "loss": 0.0018,
      "step": 901
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.014008896425366402,
      "learning_rate": 5.3495934959349594e-05,
      "loss": 0.0007,
      "step": 902
    },
    {
      "epoch": 1.4682926829268292,
      "grad_norm": 0.02652185782790184,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.0012,
      "step": 903
    },
    {
      "epoch": 1.469918699186992,
      "grad_norm": 0.003407970769330859,
      "learning_rate": 5.317073170731708e-05,
      "loss": 0.0001,
      "step": 904
    },
    {
      "epoch": 1.4715447154471546,
      "grad_norm": 0.021310793235898018,
      "learning_rate": 5.300813008130081e-05,
      "loss": 0.0002,
      "step": 905
    },
    {
      "epoch": 1.473170731707317,
      "grad_norm": 0.02850591577589512,
      "learning_rate": 5.284552845528456e-05,
      "loss": 0.0011,
      "step": 906
    },
    {
      "epoch": 1.4747967479674797,
      "grad_norm": 0.0033399187959730625,
      "learning_rate": 5.2682926829268296e-05,
      "loss": 0.0001,
      "step": 907
    },
    {
      "epoch": 1.4764227642276424,
      "grad_norm": 0.003704000962898135,
      "learning_rate": 5.252032520325203e-05,
      "loss": 0.0001,
      "step": 908
    },
    {
      "epoch": 1.4780487804878049,
      "grad_norm": 0.032001376152038574,
      "learning_rate": 5.2357723577235774e-05,
      "loss": 0.0014,
      "step": 909
    },
    {
      "epoch": 1.4796747967479675,
      "grad_norm": 0.08005566149950027,
      "learning_rate": 5.219512195121952e-05,
      "loss": 0.0052,
      "step": 910
    },
    {
      "epoch": 1.4813008130081302,
      "grad_norm": 0.05054713785648346,
      "learning_rate": 5.203252032520326e-05,
      "loss": 0.0032,
      "step": 911
    },
    {
      "epoch": 1.4829268292682927,
      "grad_norm": 0.025982370600104332,
      "learning_rate": 5.186991869918699e-05,
      "loss": 0.0011,
      "step": 912
    },
    {
      "epoch": 1.4845528455284553,
      "grad_norm": 0.03127973899245262,
      "learning_rate": 5.1707317073170736e-05,
      "loss": 0.0015,
      "step": 913
    },
    {
      "epoch": 1.486178861788618,
      "grad_norm": 0.053527433425188065,
      "learning_rate": 5.154471544715448e-05,
      "loss": 0.0026,
      "step": 914
    },
    {
      "epoch": 1.4878048780487805,
      "grad_norm": 0.008927177637815475,
      "learning_rate": 5.1382113821138214e-05,
      "loss": 0.0001,
      "step": 915
    },
    {
      "epoch": 1.4894308943089432,
      "grad_norm": 0.0369708277285099,
      "learning_rate": 5.121951219512195e-05,
      "loss": 0.0023,
      "step": 916
    },
    {
      "epoch": 1.4910569105691056,
      "grad_norm": 0.0712713673710823,
      "learning_rate": 5.10569105691057e-05,
      "loss": 0.0076,
      "step": 917
    },
    {
      "epoch": 1.4926829268292683,
      "grad_norm": 0.024789582937955856,
      "learning_rate": 5.089430894308943e-05,
      "loss": 0.0011,
      "step": 918
    },
    {
      "epoch": 1.494308943089431,
      "grad_norm": 0.0015157109592109919,
      "learning_rate": 5.073170731707318e-05,
      "loss": 0.0001,
      "step": 919
    },
    {
      "epoch": 1.4959349593495934,
      "grad_norm": 0.03379243239760399,
      "learning_rate": 5.0569105691056916e-05,
      "loss": 0.0014,
      "step": 920
    },
    {
      "epoch": 1.497560975609756,
      "grad_norm": 0.027450911700725555,
      "learning_rate": 5.040650406504065e-05,
      "loss": 0.0013,
      "step": 921
    },
    {
      "epoch": 1.4991869918699188,
      "grad_norm": 0.02915968932211399,
      "learning_rate": 5.0243902439024394e-05,
      "loss": 0.0015,
      "step": 922
    },
    {
      "epoch": 1.5008130081300814,
      "grad_norm": 0.016046937555074692,
      "learning_rate": 5.008130081300813e-05,
      "loss": 0.0004,
      "step": 923
    },
    {
      "epoch": 1.502439024390244,
      "grad_norm": 0.029242385178804398,
      "learning_rate": 4.991869918699187e-05,
      "loss": 0.0017,
      "step": 924
    },
    {
      "epoch": 1.5040650406504064,
      "grad_norm": 0.029902609065175056,
      "learning_rate": 4.975609756097561e-05,
      "loss": 0.0009,
      "step": 925
    },
    {
      "epoch": 1.5056910569105693,
      "grad_norm": 0.05645956099033356,
      "learning_rate": 4.959349593495935e-05,
      "loss": 0.0029,
      "step": 926
    },
    {
      "epoch": 1.5073170731707317,
      "grad_norm": 0.023966293781995773,
      "learning_rate": 4.9430894308943096e-05,
      "loss": 0.001,
      "step": 927
    },
    {
      "epoch": 1.5089430894308942,
      "grad_norm": 0.04049196466803551,
      "learning_rate": 4.926829268292683e-05,
      "loss": 0.0016,
      "step": 928
    },
    {
      "epoch": 1.510569105691057,
      "grad_norm": 0.029291067272424698,
      "learning_rate": 4.910569105691057e-05,
      "loss": 0.0009,
      "step": 929
    },
    {
      "epoch": 1.5121951219512195,
      "grad_norm": 0.0014774961164221168,
      "learning_rate": 4.894308943089431e-05,
      "loss": 0.0001,
      "step": 930
    },
    {
      "epoch": 1.513821138211382,
      "grad_norm": 0.02654145658016205,
      "learning_rate": 4.878048780487805e-05,
      "loss": 0.001,
      "step": 931
    },
    {
      "epoch": 1.5154471544715449,
      "grad_norm": 0.014605099335312843,
      "learning_rate": 4.861788617886179e-05,
      "loss": 0.0005,
      "step": 932
    },
    {
      "epoch": 1.5170731707317073,
      "grad_norm": 0.04984039440751076,
      "learning_rate": 4.845528455284553e-05,
      "loss": 0.0029,
      "step": 933
    },
    {
      "epoch": 1.5186991869918698,
      "grad_norm": 0.01183188147842884,
      "learning_rate": 4.829268292682927e-05,
      "loss": 0.0001,
      "step": 934
    },
    {
      "epoch": 1.5203252032520327,
      "grad_norm": 0.05293390154838562,
      "learning_rate": 4.8130081300813014e-05,
      "loss": 0.0028,
      "step": 935
    },
    {
      "epoch": 1.5219512195121951,
      "grad_norm": 0.017004190012812614,
      "learning_rate": 4.796747967479675e-05,
      "loss": 0.0008,
      "step": 936
    },
    {
      "epoch": 1.5235772357723576,
      "grad_norm": 0.04286020249128342,
      "learning_rate": 4.7804878048780485e-05,
      "loss": 0.003,
      "step": 937
    },
    {
      "epoch": 1.5252032520325203,
      "grad_norm": 0.029925726354122162,
      "learning_rate": 4.764227642276423e-05,
      "loss": 0.0014,
      "step": 938
    },
    {
      "epoch": 1.526829268292683,
      "grad_norm": 0.033477120101451874,
      "learning_rate": 4.747967479674797e-05,
      "loss": 0.0016,
      "step": 939
    },
    {
      "epoch": 1.5284552845528454,
      "grad_norm": 0.014858480542898178,
      "learning_rate": 4.731707317073171e-05,
      "loss": 0.0004,
      "step": 940
    },
    {
      "epoch": 1.530081300813008,
      "grad_norm": 0.0014671620447188616,
      "learning_rate": 4.715447154471545e-05,
      "loss": 0.0001,
      "step": 941
    },
    {
      "epoch": 1.5317073170731708,
      "grad_norm": 0.0206635519862175,
      "learning_rate": 4.699186991869919e-05,
      "loss": 0.0009,
      "step": 942
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.041611701250076294,
      "learning_rate": 4.682926829268293e-05,
      "loss": 0.0015,
      "step": 943
    },
    {
      "epoch": 1.534959349593496,
      "grad_norm": 0.029949447140097618,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0006,
      "step": 944
    },
    {
      "epoch": 1.5365853658536586,
      "grad_norm": 0.0221801046282053,
      "learning_rate": 4.650406504065041e-05,
      "loss": 0.0008,
      "step": 945
    },
    {
      "epoch": 1.538211382113821,
      "grad_norm": 0.005553074646741152,
      "learning_rate": 4.634146341463415e-05,
      "loss": 0.0001,
      "step": 946
    },
    {
      "epoch": 1.5398373983739837,
      "grad_norm": 0.042763277888298035,
      "learning_rate": 4.617886178861789e-05,
      "loss": 0.0024,
      "step": 947
    },
    {
      "epoch": 1.5414634146341464,
      "grad_norm": 0.05334215611219406,
      "learning_rate": 4.601626016260163e-05,
      "loss": 0.0008,
      "step": 948
    },
    {
      "epoch": 1.5430894308943088,
      "grad_norm": 0.05429137870669365,
      "learning_rate": 4.585365853658537e-05,
      "loss": 0.0033,
      "step": 949
    },
    {
      "epoch": 1.5447154471544715,
      "grad_norm": 0.031510286033153534,
      "learning_rate": 4.5691056910569105e-05,
      "loss": 0.0013,
      "step": 950
    },
    {
      "epoch": 1.5463414634146342,
      "grad_norm": 0.028086507692933083,
      "learning_rate": 4.5528455284552844e-05,
      "loss": 0.0013,
      "step": 951
    },
    {
      "epoch": 1.5479674796747966,
      "grad_norm": 0.029254214838147163,
      "learning_rate": 4.536585365853659e-05,
      "loss": 0.0013,
      "step": 952
    },
    {
      "epoch": 1.5495934959349593,
      "grad_norm": 0.028921162709593773,
      "learning_rate": 4.520325203252033e-05,
      "loss": 0.0009,
      "step": 953
    },
    {
      "epoch": 1.551219512195122,
      "grad_norm": 0.029225826263427734,
      "learning_rate": 4.504065040650406e-05,
      "loss": 0.0011,
      "step": 954
    },
    {
      "epoch": 1.5528455284552845,
      "grad_norm": 0.034912530332803726,
      "learning_rate": 4.487804878048781e-05,
      "loss": 0.0017,
      "step": 955
    },
    {
      "epoch": 1.5544715447154471,
      "grad_norm": 0.016504758968949318,
      "learning_rate": 4.4715447154471546e-05,
      "loss": 0.0005,
      "step": 956
    },
    {
      "epoch": 1.5560975609756098,
      "grad_norm": 0.01281148660928011,
      "learning_rate": 4.455284552845529e-05,
      "loss": 0.0003,
      "step": 957
    },
    {
      "epoch": 1.5577235772357723,
      "grad_norm": 0.04916563630104065,
      "learning_rate": 4.4390243902439024e-05,
      "loss": 0.0032,
      "step": 958
    },
    {
      "epoch": 1.559349593495935,
      "grad_norm": 0.020413292571902275,
      "learning_rate": 4.422764227642276e-05,
      "loss": 0.001,
      "step": 959
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 0.01751362346112728,
      "learning_rate": 4.406504065040651e-05,
      "loss": 0.0004,
      "step": 960
    },
    {
      "epoch": 1.56260162601626,
      "grad_norm": 0.01444808766245842,
      "learning_rate": 4.390243902439025e-05,
      "loss": 0.0004,
      "step": 961
    },
    {
      "epoch": 1.5642276422764227,
      "grad_norm": 0.023561783134937286,
      "learning_rate": 4.373983739837399e-05,
      "loss": 0.0007,
      "step": 962
    },
    {
      "epoch": 1.5658536585365854,
      "grad_norm": 0.014826640486717224,
      "learning_rate": 4.3577235772357726e-05,
      "loss": 0.0004,
      "step": 963
    },
    {
      "epoch": 1.5674796747967479,
      "grad_norm": 0.017405454069375992,
      "learning_rate": 4.3414634146341465e-05,
      "loss": 0.0007,
      "step": 964
    },
    {
      "epoch": 1.5691056910569106,
      "grad_norm": 0.01570434868335724,
      "learning_rate": 4.3252032520325204e-05,
      "loss": 0.0005,
      "step": 965
    },
    {
      "epoch": 1.5707317073170732,
      "grad_norm": 0.07215236127376556,
      "learning_rate": 4.308943089430895e-05,
      "loss": 0.0053,
      "step": 966
    },
    {
      "epoch": 1.5723577235772357,
      "grad_norm": 0.05248936638236046,
      "learning_rate": 4.292682926829268e-05,
      "loss": 0.0033,
      "step": 967
    },
    {
      "epoch": 1.5739837398373984,
      "grad_norm": 0.02480217069387436,
      "learning_rate": 4.276422764227642e-05,
      "loss": 0.001,
      "step": 968
    },
    {
      "epoch": 1.575609756097561,
      "grad_norm": 0.016672195866703987,
      "learning_rate": 4.2601626016260166e-05,
      "loss": 0.0005,
      "step": 969
    },
    {
      "epoch": 1.5772357723577235,
      "grad_norm": 0.03791004419326782,
      "learning_rate": 4.2439024390243905e-05,
      "loss": 0.0019,
      "step": 970
    },
    {
      "epoch": 1.5788617886178862,
      "grad_norm": 0.037721384316682816,
      "learning_rate": 4.2276422764227644e-05,
      "loss": 0.0015,
      "step": 971
    },
    {
      "epoch": 1.5804878048780489,
      "grad_norm": 0.0021585484500974417,
      "learning_rate": 4.211382113821138e-05,
      "loss": 0.0001,
      "step": 972
    },
    {
      "epoch": 1.5821138211382113,
      "grad_norm": 0.013544055633246899,
      "learning_rate": 4.195121951219512e-05,
      "loss": 0.0006,
      "step": 973
    },
    {
      "epoch": 1.583739837398374,
      "grad_norm": 0.026744475588202477,
      "learning_rate": 4.178861788617887e-05,
      "loss": 0.0015,
      "step": 974
    },
    {
      "epoch": 1.5853658536585367,
      "grad_norm": 0.019259674474596977,
      "learning_rate": 4.162601626016261e-05,
      "loss": 0.0009,
      "step": 975
    },
    {
      "epoch": 1.5869918699186991,
      "grad_norm": 0.04638724774122238,
      "learning_rate": 4.146341463414634e-05,
      "loss": 0.0022,
      "step": 976
    },
    {
      "epoch": 1.5886178861788618,
      "grad_norm": 0.0193481482565403,
      "learning_rate": 4.1300813008130085e-05,
      "loss": 0.0008,
      "step": 977
    },
    {
      "epoch": 1.5902439024390245,
      "grad_norm": 0.061783384531736374,
      "learning_rate": 4.1138211382113824e-05,
      "loss": 0.0034,
      "step": 978
    },
    {
      "epoch": 1.591869918699187,
      "grad_norm": 0.021055785939097404,
      "learning_rate": 4.097560975609756e-05,
      "loss": 0.0007,
      "step": 979
    },
    {
      "epoch": 1.5934959349593496,
      "grad_norm": 0.001135829254053533,
      "learning_rate": 4.08130081300813e-05,
      "loss": 0.0001,
      "step": 980
    },
    {
      "epoch": 1.5951219512195123,
      "grad_norm": 0.036033619195222855,
      "learning_rate": 4.065040650406504e-05,
      "loss": 0.0026,
      "step": 981
    },
    {
      "epoch": 1.5967479674796747,
      "grad_norm": 0.04939013347029686,
      "learning_rate": 4.0487804878048786e-05,
      "loss": 0.0032,
      "step": 982
    },
    {
      "epoch": 1.5983739837398374,
      "grad_norm": 0.003381914459168911,
      "learning_rate": 4.0325203252032525e-05,
      "loss": 0.0001,
      "step": 983
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.03629617393016815,
      "learning_rate": 4.016260162601626e-05,
      "loss": 0.0016,
      "step": 984
    },
    {
      "epoch": 1.6016260162601625,
      "grad_norm": 0.07204094529151917,
      "learning_rate": 4e-05,
      "loss": 0.0086,
      "step": 985
    },
    {
      "epoch": 1.6032520325203252,
      "grad_norm": 0.02422051504254341,
      "learning_rate": 3.983739837398374e-05,
      "loss": 0.0007,
      "step": 986
    },
    {
      "epoch": 1.604878048780488,
      "grad_norm": 0.005777527578175068,
      "learning_rate": 3.967479674796748e-05,
      "loss": 0.0001,
      "step": 987
    },
    {
      "epoch": 1.6065040650406504,
      "grad_norm": 0.009323480539023876,
      "learning_rate": 3.951219512195122e-05,
      "loss": 0.0002,
      "step": 988
    },
    {
      "epoch": 1.608130081300813,
      "grad_norm": 0.02205595374107361,
      "learning_rate": 3.934959349593496e-05,
      "loss": 0.0006,
      "step": 989
    },
    {
      "epoch": 1.6097560975609757,
      "grad_norm": 0.008246059529483318,
      "learning_rate": 3.91869918699187e-05,
      "loss": 0.0002,
      "step": 990
    },
    {
      "epoch": 1.6113821138211382,
      "grad_norm": 0.03708760812878609,
      "learning_rate": 3.9024390243902444e-05,
      "loss": 0.0022,
      "step": 991
    },
    {
      "epoch": 1.6130081300813008,
      "grad_norm": 0.03455396369099617,
      "learning_rate": 3.886178861788618e-05,
      "loss": 0.0018,
      "step": 992
    },
    {
      "epoch": 1.6146341463414635,
      "grad_norm": 0.013891465961933136,
      "learning_rate": 3.8699186991869915e-05,
      "loss": 0.0007,
      "step": 993
    },
    {
      "epoch": 1.616260162601626,
      "grad_norm": 0.02255198359489441,
      "learning_rate": 3.853658536585366e-05,
      "loss": 0.001,
      "step": 994
    },
    {
      "epoch": 1.6178861788617886,
      "grad_norm": 0.015795670449733734,
      "learning_rate": 3.83739837398374e-05,
      "loss": 0.0004,
      "step": 995
    },
    {
      "epoch": 1.6195121951219513,
      "grad_norm": 0.02875988557934761,
      "learning_rate": 3.8211382113821145e-05,
      "loss": 0.0012,
      "step": 996
    },
    {
      "epoch": 1.6211382113821138,
      "grad_norm": 0.0023494975175708532,
      "learning_rate": 3.804878048780488e-05,
      "loss": 0.0001,
      "step": 997
    },
    {
      "epoch": 1.6227642276422765,
      "grad_norm": 0.025373050943017006,
      "learning_rate": 3.788617886178862e-05,
      "loss": 0.0009,
      "step": 998
    },
    {
      "epoch": 1.6243902439024391,
      "grad_norm": 0.048518478870391846,
      "learning_rate": 3.772357723577236e-05,
      "loss": 0.0024,
      "step": 999
    },
    {
      "epoch": 1.6260162601626016,
      "grad_norm": 0.03426399827003479,
      "learning_rate": 3.75609756097561e-05,
      "loss": 0.0025,
      "step": 1000
    },
    {
      "epoch": 1.6276422764227643,
      "grad_norm": 0.06424593925476074,
      "learning_rate": 3.739837398373984e-05,
      "loss": 0.0025,
      "step": 1001
    },
    {
      "epoch": 1.629268292682927,
      "grad_norm": 0.02555185742676258,
      "learning_rate": 3.723577235772358e-05,
      "loss": 0.0011,
      "step": 1002
    },
    {
      "epoch": 1.6308943089430894,
      "grad_norm": 0.04188069701194763,
      "learning_rate": 3.707317073170732e-05,
      "loss": 0.0022,
      "step": 1003
    },
    {
      "epoch": 1.632520325203252,
      "grad_norm": 0.027885882183909416,
      "learning_rate": 3.691056910569106e-05,
      "loss": 0.0011,
      "step": 1004
    },
    {
      "epoch": 1.6341463414634148,
      "grad_norm": 0.0008365905960090458,
      "learning_rate": 3.67479674796748e-05,
      "loss": 0.0001,
      "step": 1005
    },
    {
      "epoch": 1.6357723577235772,
      "grad_norm": 0.021350057795643806,
      "learning_rate": 3.6585365853658535e-05,
      "loss": 0.0006,
      "step": 1006
    },
    {
      "epoch": 1.6373983739837399,
      "grad_norm": 0.003318195231258869,
      "learning_rate": 3.642276422764228e-05,
      "loss": 0.0001,
      "step": 1007
    },
    {
      "epoch": 1.6390243902439026,
      "grad_norm": 0.0285592470318079,
      "learning_rate": 3.626016260162602e-05,
      "loss": 0.0019,
      "step": 1008
    },
    {
      "epoch": 1.640650406504065,
      "grad_norm": 0.04645520821213722,
      "learning_rate": 3.609756097560976e-05,
      "loss": 0.0017,
      "step": 1009
    },
    {
      "epoch": 1.6422764227642277,
      "grad_norm": 0.0015238821506500244,
      "learning_rate": 3.59349593495935e-05,
      "loss": 0.0001,
      "step": 1010
    },
    {
      "epoch": 1.6439024390243904,
      "grad_norm": 0.03200877085328102,
      "learning_rate": 3.577235772357724e-05,
      "loss": 0.0012,
      "step": 1011
    },
    {
      "epoch": 1.6455284552845528,
      "grad_norm": 0.04644981771707535,
      "learning_rate": 3.5609756097560976e-05,
      "loss": 0.0018,
      "step": 1012
    },
    {
      "epoch": 1.6471544715447153,
      "grad_norm": 0.0412626788020134,
      "learning_rate": 3.544715447154472e-05,
      "loss": 0.0017,
      "step": 1013
    },
    {
      "epoch": 1.6487804878048782,
      "grad_norm": 0.04482826963067055,
      "learning_rate": 3.5284552845528454e-05,
      "loss": 0.0031,
      "step": 1014
    },
    {
      "epoch": 1.6504065040650406,
      "grad_norm": 0.04082280024886131,
      "learning_rate": 3.512195121951219e-05,
      "loss": 0.002,
      "step": 1015
    },
    {
      "epoch": 1.652032520325203,
      "grad_norm": 0.016485830768942833,
      "learning_rate": 3.495934959349594e-05,
      "loss": 0.0005,
      "step": 1016
    },
    {
      "epoch": 1.653658536585366,
      "grad_norm": 0.0010302804876118898,
      "learning_rate": 3.479674796747968e-05,
      "loss": 0.0001,
      "step": 1017
    },
    {
      "epoch": 1.6552845528455284,
      "grad_norm": 0.016771400347352028,
      "learning_rate": 3.4634146341463416e-05,
      "loss": 0.0005,
      "step": 1018
    },
    {
      "epoch": 1.656910569105691,
      "grad_norm": 0.026453979313373566,
      "learning_rate": 3.4471544715447155e-05,
      "loss": 0.0007,
      "step": 1019
    },
    {
      "epoch": 1.6585365853658538,
      "grad_norm": 0.026278909295797348,
      "learning_rate": 3.4308943089430894e-05,
      "loss": 0.0007,
      "step": 1020
    },
    {
      "epoch": 1.6601626016260163,
      "grad_norm": 0.02314259484410286,
      "learning_rate": 3.414634146341464e-05,
      "loss": 0.0008,
      "step": 1021
    },
    {
      "epoch": 1.6617886178861787,
      "grad_norm": 0.031302113085985184,
      "learning_rate": 3.398373983739838e-05,
      "loss": 0.0014,
      "step": 1022
    },
    {
      "epoch": 1.6634146341463416,
      "grad_norm": 0.040554922074079514,
      "learning_rate": 3.382113821138211e-05,
      "loss": 0.0017,
      "step": 1023
    },
    {
      "epoch": 1.665040650406504,
      "grad_norm": 0.1335289180278778,
      "learning_rate": 3.365853658536586e-05,
      "loss": 0.0117,
      "step": 1024
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.0019720287527889013,
      "learning_rate": 3.3495934959349596e-05,
      "loss": 0.0001,
      "step": 1025
    },
    {
      "epoch": 1.6682926829268294,
      "grad_norm": 0.0027605798095464706,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0001,
      "step": 1026
    },
    {
      "epoch": 1.6699186991869919,
      "grad_norm": 0.03014337457716465,
      "learning_rate": 3.3170731707317074e-05,
      "loss": 0.002,
      "step": 1027
    },
    {
      "epoch": 1.6715447154471543,
      "grad_norm": 0.025263426825404167,
      "learning_rate": 3.300813008130081e-05,
      "loss": 0.0011,
      "step": 1028
    },
    {
      "epoch": 1.6731707317073172,
      "grad_norm": 0.0025931894779205322,
      "learning_rate": 3.284552845528455e-05,
      "loss": 0.0001,
      "step": 1029
    },
    {
      "epoch": 1.6747967479674797,
      "grad_norm": 0.04008264094591141,
      "learning_rate": 3.26829268292683e-05,
      "loss": 0.0014,
      "step": 1030
    },
    {
      "epoch": 1.6764227642276421,
      "grad_norm": 0.03898628056049347,
      "learning_rate": 3.2520325203252037e-05,
      "loss": 0.0011,
      "step": 1031
    },
    {
      "epoch": 1.678048780487805,
      "grad_norm": 0.04826435446739197,
      "learning_rate": 3.235772357723577e-05,
      "loss": 0.003,
      "step": 1032
    },
    {
      "epoch": 1.6796747967479675,
      "grad_norm": 0.039444345980882645,
      "learning_rate": 3.2195121951219514e-05,
      "loss": 0.0013,
      "step": 1033
    },
    {
      "epoch": 1.68130081300813,
      "grad_norm": 0.020663242787122726,
      "learning_rate": 3.2032520325203253e-05,
      "loss": 0.0009,
      "step": 1034
    },
    {
      "epoch": 1.6829268292682928,
      "grad_norm": 0.04284120723605156,
      "learning_rate": 3.186991869918699e-05,
      "loss": 0.0027,
      "step": 1035
    },
    {
      "epoch": 1.6845528455284553,
      "grad_norm": 0.0016198183875530958,
      "learning_rate": 3.170731707317073e-05,
      "loss": 0.0001,
      "step": 1036
    },
    {
      "epoch": 1.6861788617886178,
      "grad_norm": 0.035568930208683014,
      "learning_rate": 3.154471544715447e-05,
      "loss": 0.002,
      "step": 1037
    },
    {
      "epoch": 1.6878048780487804,
      "grad_norm": 0.014967869035899639,
      "learning_rate": 3.1382113821138216e-05,
      "loss": 0.0003,
      "step": 1038
    },
    {
      "epoch": 1.689430894308943,
      "grad_norm": 0.0344373993575573,
      "learning_rate": 3.1219512195121955e-05,
      "loss": 0.0023,
      "step": 1039
    },
    {
      "epoch": 1.6910569105691056,
      "grad_norm": 0.020588040351867676,
      "learning_rate": 3.105691056910569e-05,
      "loss": 0.0005,
      "step": 1040
    },
    {
      "epoch": 1.6926829268292682,
      "grad_norm": 0.02682362124323845,
      "learning_rate": 3.089430894308943e-05,
      "loss": 0.0008,
      "step": 1041
    },
    {
      "epoch": 1.694308943089431,
      "grad_norm": 0.04461943358182907,
      "learning_rate": 3.073170731707317e-05,
      "loss": 0.0007,
      "step": 1042
    },
    {
      "epoch": 1.6959349593495934,
      "grad_norm": 0.03380343317985535,
      "learning_rate": 3.056910569105691e-05,
      "loss": 0.0017,
      "step": 1043
    },
    {
      "epoch": 1.697560975609756,
      "grad_norm": 0.0018318280344828963,
      "learning_rate": 3.0406504065040653e-05,
      "loss": 0.0001,
      "step": 1044
    },
    {
      "epoch": 1.6991869918699187,
      "grad_norm": 0.01415235549211502,
      "learning_rate": 3.0243902439024392e-05,
      "loss": 0.0004,
      "step": 1045
    },
    {
      "epoch": 1.7008130081300812,
      "grad_norm": 0.016564946621656418,
      "learning_rate": 3.0081300813008135e-05,
      "loss": 0.0005,
      "step": 1046
    },
    {
      "epoch": 1.7024390243902439,
      "grad_norm": 0.043574534356594086,
      "learning_rate": 2.9918699186991874e-05,
      "loss": 0.0024,
      "step": 1047
    },
    {
      "epoch": 1.7040650406504065,
      "grad_norm": 0.03337196260690689,
      "learning_rate": 2.975609756097561e-05,
      "loss": 0.0017,
      "step": 1048
    },
    {
      "epoch": 1.705691056910569,
      "grad_norm": 0.03822007030248642,
      "learning_rate": 2.959349593495935e-05,
      "loss": 0.0018,
      "step": 1049
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 0.024903293699026108,
      "learning_rate": 2.943089430894309e-05,
      "loss": 0.0012,
      "step": 1050
    },
    {
      "epoch": 1.7089430894308943,
      "grad_norm": 0.03597740828990936,
      "learning_rate": 2.926829268292683e-05,
      "loss": 0.0016,
      "step": 1051
    },
    {
      "epoch": 1.7105691056910568,
      "grad_norm": 0.01822315715253353,
      "learning_rate": 2.9105691056910572e-05,
      "loss": 0.0005,
      "step": 1052
    },
    {
      "epoch": 1.7121951219512195,
      "grad_norm": 0.023513231426477432,
      "learning_rate": 2.894308943089431e-05,
      "loss": 0.0007,
      "step": 1053
    },
    {
      "epoch": 1.7138211382113822,
      "grad_norm": 0.00705025065690279,
      "learning_rate": 2.8780487804878046e-05,
      "loss": 0.0002,
      "step": 1054
    },
    {
      "epoch": 1.7154471544715446,
      "grad_norm": 0.009230757132172585,
      "learning_rate": 2.8617886178861792e-05,
      "loss": 0.0002,
      "step": 1055
    },
    {
      "epoch": 1.7170731707317073,
      "grad_norm": 0.006040479987859726,
      "learning_rate": 2.8455284552845528e-05,
      "loss": 0.0001,
      "step": 1056
    },
    {
      "epoch": 1.71869918699187,
      "grad_norm": 0.0030613746494054794,
      "learning_rate": 2.8292682926829267e-05,
      "loss": 0.0001,
      "step": 1057
    },
    {
      "epoch": 1.7203252032520324,
      "grad_norm": 0.01661555841565132,
      "learning_rate": 2.813008130081301e-05,
      "loss": 0.0006,
      "step": 1058
    },
    {
      "epoch": 1.721951219512195,
      "grad_norm": 0.01203865185379982,
      "learning_rate": 2.7967479674796748e-05,
      "loss": 0.0004,
      "step": 1059
    },
    {
      "epoch": 1.7235772357723578,
      "grad_norm": 0.03186129778623581,
      "learning_rate": 2.780487804878049e-05,
      "loss": 0.0003,
      "step": 1060
    },
    {
      "epoch": 1.7252032520325202,
      "grad_norm": 0.028757760301232338,
      "learning_rate": 2.764227642276423e-05,
      "loss": 0.001,
      "step": 1061
    },
    {
      "epoch": 1.726829268292683,
      "grad_norm": 0.03127975016832352,
      "learning_rate": 2.747967479674797e-05,
      "loss": 0.002,
      "step": 1062
    },
    {
      "epoch": 1.7284552845528456,
      "grad_norm": 0.0022727299947291613,
      "learning_rate": 2.731707317073171e-05,
      "loss": 0.0001,
      "step": 1063
    },
    {
      "epoch": 1.730081300813008,
      "grad_norm": 0.031038396060466766,
      "learning_rate": 2.715447154471545e-05,
      "loss": 0.0007,
      "step": 1064
    },
    {
      "epoch": 1.7317073170731707,
      "grad_norm": 0.020297035574913025,
      "learning_rate": 2.6991869918699185e-05,
      "loss": 0.0007,
      "step": 1065
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.04939039796590805,
      "learning_rate": 2.682926829268293e-05,
      "loss": 0.0029,
      "step": 1066
    },
    {
      "epoch": 1.7349593495934958,
      "grad_norm": 0.018960069864988327,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0007,
      "step": 1067
    },
    {
      "epoch": 1.7365853658536585,
      "grad_norm": 0.0024158870801329613,
      "learning_rate": 2.6504065040650406e-05,
      "loss": 0.0001,
      "step": 1068
    },
    {
      "epoch": 1.7382113821138212,
      "grad_norm": 0.04824569448828697,
      "learning_rate": 2.6341463414634148e-05,
      "loss": 0.0032,
      "step": 1069
    },
    {
      "epoch": 1.7398373983739837,
      "grad_norm": 0.04507574811577797,
      "learning_rate": 2.6178861788617887e-05,
      "loss": 0.0028,
      "step": 1070
    },
    {
      "epoch": 1.7414634146341463,
      "grad_norm": 0.0008850684971548617,
      "learning_rate": 2.601626016260163e-05,
      "loss": 0.0001,
      "step": 1071
    },
    {
      "epoch": 1.743089430894309,
      "grad_norm": 0.012785190716385841,
      "learning_rate": 2.5853658536585368e-05,
      "loss": 0.0003,
      "step": 1072
    },
    {
      "epoch": 1.7447154471544715,
      "grad_norm": 0.015205205418169498,
      "learning_rate": 2.5691056910569107e-05,
      "loss": 0.0005,
      "step": 1073
    },
    {
      "epoch": 1.7463414634146341,
      "grad_norm": 0.02964911237359047,
      "learning_rate": 2.552845528455285e-05,
      "loss": 0.0011,
      "step": 1074
    },
    {
      "epoch": 1.7479674796747968,
      "grad_norm": 0.001195345539599657,
      "learning_rate": 2.536585365853659e-05,
      "loss": 0.0001,
      "step": 1075
    },
    {
      "epoch": 1.7495934959349593,
      "grad_norm": 0.03437893092632294,
      "learning_rate": 2.5203252032520324e-05,
      "loss": 0.001,
      "step": 1076
    },
    {
      "epoch": 1.751219512195122,
      "grad_norm": 0.02289631776511669,
      "learning_rate": 2.5040650406504066e-05,
      "loss": 0.0009,
      "step": 1077
    },
    {
      "epoch": 1.7528455284552846,
      "grad_norm": 0.02761707454919815,
      "learning_rate": 2.4878048780487805e-05,
      "loss": 0.0008,
      "step": 1078
    },
    {
      "epoch": 1.754471544715447,
      "grad_norm": 0.022121798247098923,
      "learning_rate": 2.4715447154471548e-05,
      "loss": 0.0006,
      "step": 1079
    },
    {
      "epoch": 1.7560975609756098,
      "grad_norm": 0.02700864151120186,
      "learning_rate": 2.4552845528455283e-05,
      "loss": 0.001,
      "step": 1080
    },
    {
      "epoch": 1.7577235772357724,
      "grad_norm": 0.03297257050871849,
      "learning_rate": 2.4390243902439026e-05,
      "loss": 0.0018,
      "step": 1081
    },
    {
      "epoch": 1.759349593495935,
      "grad_norm": 0.018693462014198303,
      "learning_rate": 2.4227642276422765e-05,
      "loss": 0.0005,
      "step": 1082
    },
    {
      "epoch": 1.7609756097560976,
      "grad_norm": 0.001619628514163196,
      "learning_rate": 2.4065040650406507e-05,
      "loss": 0.0001,
      "step": 1083
    },
    {
      "epoch": 1.7626016260162602,
      "grad_norm": 0.002801658120006323,
      "learning_rate": 2.3902439024390243e-05,
      "loss": 0.0001,
      "step": 1084
    },
    {
      "epoch": 1.7642276422764227,
      "grad_norm": 0.022421959787607193,
      "learning_rate": 2.3739837398373985e-05,
      "loss": 0.0009,
      "step": 1085
    },
    {
      "epoch": 1.7658536585365854,
      "grad_norm": 0.017804283648729324,
      "learning_rate": 2.3577235772357724e-05,
      "loss": 0.0005,
      "step": 1086
    },
    {
      "epoch": 1.767479674796748,
      "grad_norm": 0.02957669273018837,
      "learning_rate": 2.3414634146341466e-05,
      "loss": 0.0007,
      "step": 1087
    },
    {
      "epoch": 1.7691056910569105,
      "grad_norm": 0.06721198558807373,
      "learning_rate": 2.3252032520325205e-05,
      "loss": 0.0049,
      "step": 1088
    },
    {
      "epoch": 1.7707317073170732,
      "grad_norm": 0.022788260132074356,
      "learning_rate": 2.3089430894308944e-05,
      "loss": 0.001,
      "step": 1089
    },
    {
      "epoch": 1.7723577235772359,
      "grad_norm": 0.03354328125715256,
      "learning_rate": 2.2926829268292687e-05,
      "loss": 0.0012,
      "step": 1090
    },
    {
      "epoch": 1.7739837398373983,
      "grad_norm": 0.009073177352547646,
      "learning_rate": 2.2764227642276422e-05,
      "loss": 0.0002,
      "step": 1091
    },
    {
      "epoch": 1.775609756097561,
      "grad_norm": 0.0257561132311821,
      "learning_rate": 2.2601626016260165e-05,
      "loss": 0.0008,
      "step": 1092
    },
    {
      "epoch": 1.7772357723577237,
      "grad_norm": 0.03987741470336914,
      "learning_rate": 2.2439024390243904e-05,
      "loss": 0.0021,
      "step": 1093
    },
    {
      "epoch": 1.7788617886178861,
      "grad_norm": 0.04598459601402283,
      "learning_rate": 2.2276422764227646e-05,
      "loss": 0.0017,
      "step": 1094
    },
    {
      "epoch": 1.7804878048780488,
      "grad_norm": 0.03776735067367554,
      "learning_rate": 2.211382113821138e-05,
      "loss": 0.0019,
      "step": 1095
    },
    {
      "epoch": 1.7821138211382115,
      "grad_norm": 0.0025660484097898006,
      "learning_rate": 2.1951219512195124e-05,
      "loss": 0.0001,
      "step": 1096
    },
    {
      "epoch": 1.783739837398374,
      "grad_norm": 0.01768294721841812,
      "learning_rate": 2.1788617886178863e-05,
      "loss": 0.0006,
      "step": 1097
    },
    {
      "epoch": 1.7853658536585366,
      "grad_norm": 0.00934229139238596,
      "learning_rate": 2.1626016260162602e-05,
      "loss": 0.0003,
      "step": 1098
    },
    {
      "epoch": 1.7869918699186993,
      "grad_norm": 0.0368935652077198,
      "learning_rate": 2.146341463414634e-05,
      "loss": 0.0016,
      "step": 1099
    },
    {
      "epoch": 1.7886178861788617,
      "grad_norm": 0.0011456826468929648,
      "learning_rate": 2.1300813008130083e-05,
      "loss": 0.0001,
      "step": 1100
    },
    {
      "epoch": 1.7902439024390244,
      "grad_norm": 0.0333004929125309,
      "learning_rate": 2.1138211382113822e-05,
      "loss": 0.0018,
      "step": 1101
    },
    {
      "epoch": 1.791869918699187,
      "grad_norm": 0.04220584034919739,
      "learning_rate": 2.097560975609756e-05,
      "loss": 0.0016,
      "step": 1102
    },
    {
      "epoch": 1.7934959349593496,
      "grad_norm": 0.02317630499601364,
      "learning_rate": 2.0813008130081303e-05,
      "loss": 0.0005,
      "step": 1103
    },
    {
      "epoch": 1.7951219512195122,
      "grad_norm": 0.017827820032835007,
      "learning_rate": 2.0650406504065042e-05,
      "loss": 0.0005,
      "step": 1104
    },
    {
      "epoch": 1.796747967479675,
      "grad_norm": 0.029060957953333855,
      "learning_rate": 2.048780487804878e-05,
      "loss": 0.0012,
      "step": 1105
    },
    {
      "epoch": 1.7983739837398374,
      "grad_norm": 0.016415011137723923,
      "learning_rate": 2.032520325203252e-05,
      "loss": 0.0007,
      "step": 1106
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.00908346101641655,
      "learning_rate": 2.0162601626016263e-05,
      "loss": 0.0001,
      "step": 1107
    },
    {
      "epoch": 1.8016260162601627,
      "grad_norm": 0.01793704926967621,
      "learning_rate": 2e-05,
      "loss": 0.0004,
      "step": 1108
    },
    {
      "epoch": 1.8032520325203252,
      "grad_norm": 0.021621113643050194,
      "learning_rate": 1.983739837398374e-05,
      "loss": 0.0006,
      "step": 1109
    },
    {
      "epoch": 1.8048780487804879,
      "grad_norm": 0.031169088557362556,
      "learning_rate": 1.967479674796748e-05,
      "loss": 0.0016,
      "step": 1110
    },
    {
      "epoch": 1.8065040650406505,
      "grad_norm": 0.02565784566104412,
      "learning_rate": 1.9512195121951222e-05,
      "loss": 0.0011,
      "step": 1111
    },
    {
      "epoch": 1.808130081300813,
      "grad_norm": 0.011173420585691929,
      "learning_rate": 1.9349593495934958e-05,
      "loss": 0.0003,
      "step": 1112
    },
    {
      "epoch": 1.8097560975609757,
      "grad_norm": 0.029847728088498116,
      "learning_rate": 1.91869918699187e-05,
      "loss": 0.0005,
      "step": 1113
    },
    {
      "epoch": 1.8113821138211383,
      "grad_norm": 0.03106047213077545,
      "learning_rate": 1.902439024390244e-05,
      "loss": 0.0012,
      "step": 1114
    },
    {
      "epoch": 1.8130081300813008,
      "grad_norm": 0.0020210999064147472,
      "learning_rate": 1.886178861788618e-05,
      "loss": 0.0001,
      "step": 1115
    },
    {
      "epoch": 1.8146341463414632,
      "grad_norm": 0.01663784682750702,
      "learning_rate": 1.869918699186992e-05,
      "loss": 0.0005,
      "step": 1116
    },
    {
      "epoch": 1.8162601626016261,
      "grad_norm": 0.038778990507125854,
      "learning_rate": 1.853658536585366e-05,
      "loss": 0.0023,
      "step": 1117
    },
    {
      "epoch": 1.8178861788617886,
      "grad_norm": 0.02158982679247856,
      "learning_rate": 1.83739837398374e-05,
      "loss": 0.0006,
      "step": 1118
    },
    {
      "epoch": 1.819512195121951,
      "grad_norm": 0.014308913610875607,
      "learning_rate": 1.821138211382114e-05,
      "loss": 0.0003,
      "step": 1119
    },
    {
      "epoch": 1.821138211382114,
      "grad_norm": 0.01935270056128502,
      "learning_rate": 1.804878048780488e-05,
      "loss": 0.0006,
      "step": 1120
    },
    {
      "epoch": 1.8227642276422764,
      "grad_norm": 0.025139320641756058,
      "learning_rate": 1.788617886178862e-05,
      "loss": 0.0007,
      "step": 1121
    },
    {
      "epoch": 1.8243902439024389,
      "grad_norm": 0.06712574511766434,
      "learning_rate": 1.772357723577236e-05,
      "loss": 0.0018,
      "step": 1122
    },
    {
      "epoch": 1.8260162601626018,
      "grad_norm": 0.029164815321564674,
      "learning_rate": 1.7560975609756096e-05,
      "loss": 0.0015,
      "step": 1123
    },
    {
      "epoch": 1.8276422764227642,
      "grad_norm": 0.018202390521764755,
      "learning_rate": 1.739837398373984e-05,
      "loss": 0.0006,
      "step": 1124
    },
    {
      "epoch": 1.8292682926829267,
      "grad_norm": 0.020510690286755562,
      "learning_rate": 1.7235772357723578e-05,
      "loss": 0.0006,
      "step": 1125
    },
    {
      "epoch": 1.8308943089430896,
      "grad_norm": 0.022907083854079247,
      "learning_rate": 1.707317073170732e-05,
      "loss": 0.0011,
      "step": 1126
    },
    {
      "epoch": 1.832520325203252,
      "grad_norm": 0.047068655490875244,
      "learning_rate": 1.6910569105691056e-05,
      "loss": 0.0012,
      "step": 1127
    },
    {
      "epoch": 1.8341463414634145,
      "grad_norm": 0.013537407852709293,
      "learning_rate": 1.6747967479674798e-05,
      "loss": 0.0003,
      "step": 1128
    },
    {
      "epoch": 1.8357723577235774,
      "grad_norm": 0.00808072742074728,
      "learning_rate": 1.6585365853658537e-05,
      "loss": 0.0002,
      "step": 1129
    },
    {
      "epoch": 1.8373983739837398,
      "grad_norm": 0.05639174208045006,
      "learning_rate": 1.6422764227642276e-05,
      "loss": 0.0013,
      "step": 1130
    },
    {
      "epoch": 1.8390243902439023,
      "grad_norm": 0.0025940516497939825,
      "learning_rate": 1.6260162601626018e-05,
      "loss": 0.0001,
      "step": 1131
    },
    {
      "epoch": 1.8406504065040652,
      "grad_norm": 0.07226412743330002,
      "learning_rate": 1.6097560975609757e-05,
      "loss": 0.0068,
      "step": 1132
    },
    {
      "epoch": 1.8422764227642277,
      "grad_norm": 0.003129689721390605,
      "learning_rate": 1.5934959349593496e-05,
      "loss": 0.0001,
      "step": 1133
    },
    {
      "epoch": 1.84390243902439,
      "grad_norm": 0.028201742097735405,
      "learning_rate": 1.5772357723577235e-05,
      "loss": 0.0013,
      "step": 1134
    },
    {
      "epoch": 1.845528455284553,
      "grad_norm": 0.02249806374311447,
      "learning_rate": 1.5609756097560978e-05,
      "loss": 0.0013,
      "step": 1135
    },
    {
      "epoch": 1.8471544715447155,
      "grad_norm": 0.015867242589592934,
      "learning_rate": 1.5447154471544717e-05,
      "loss": 0.0004,
      "step": 1136
    },
    {
      "epoch": 1.848780487804878,
      "grad_norm": 0.014312727376818657,
      "learning_rate": 1.5284552845528455e-05,
      "loss": 0.0004,
      "step": 1137
    },
    {
      "epoch": 1.8504065040650408,
      "grad_norm": 0.007667591795325279,
      "learning_rate": 1.5121951219512196e-05,
      "loss": 0.0001,
      "step": 1138
    },
    {
      "epoch": 1.8520325203252033,
      "grad_norm": 0.07340442389249802,
      "learning_rate": 1.4959349593495937e-05,
      "loss": 0.0039,
      "step": 1139
    },
    {
      "epoch": 1.8536585365853657,
      "grad_norm": 0.016845688223838806,
      "learning_rate": 1.4796747967479676e-05,
      "loss": 0.0006,
      "step": 1140
    },
    {
      "epoch": 1.8552845528455284,
      "grad_norm": 0.03637265786528587,
      "learning_rate": 1.4634146341463415e-05,
      "loss": 0.0018,
      "step": 1141
    },
    {
      "epoch": 1.856910569105691,
      "grad_norm": 0.022235678508877754,
      "learning_rate": 1.4471544715447155e-05,
      "loss": 0.0007,
      "step": 1142
    },
    {
      "epoch": 1.8585365853658535,
      "grad_norm": 0.05199795588850975,
      "learning_rate": 1.4308943089430896e-05,
      "loss": 0.0035,
      "step": 1143
    },
    {
      "epoch": 1.8601626016260162,
      "grad_norm": 0.03837718442082405,
      "learning_rate": 1.4146341463414633e-05,
      "loss": 0.0018,
      "step": 1144
    },
    {
      "epoch": 1.8617886178861789,
      "grad_norm": 0.023779455572366714,
      "learning_rate": 1.3983739837398374e-05,
      "loss": 0.0003,
      "step": 1145
    },
    {
      "epoch": 1.8634146341463413,
      "grad_norm": 0.03173859044909477,
      "learning_rate": 1.3821138211382115e-05,
      "loss": 0.0012,
      "step": 1146
    },
    {
      "epoch": 1.865040650406504,
      "grad_norm": 0.001957499422132969,
      "learning_rate": 1.3658536585365855e-05,
      "loss": 0.0001,
      "step": 1147
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.0024112744722515345,
      "learning_rate": 1.3495934959349593e-05,
      "loss": 0.0001,
      "step": 1148
    },
    {
      "epoch": 1.8682926829268292,
      "grad_norm": 0.021481182426214218,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0011,
      "step": 1149
    },
    {
      "epoch": 1.8699186991869918,
      "grad_norm": 0.04561104625463486,
      "learning_rate": 1.3170731707317074e-05,
      "loss": 0.0025,
      "step": 1150
    },
    {
      "epoch": 1.8715447154471545,
      "grad_norm": 0.08663590252399445,
      "learning_rate": 1.3008130081300815e-05,
      "loss": 0.0031,
      "step": 1151
    },
    {
      "epoch": 1.873170731707317,
      "grad_norm": 0.0015419494593515992,
      "learning_rate": 1.2845528455284554e-05,
      "loss": 0.0001,
      "step": 1152
    },
    {
      "epoch": 1.8747967479674796,
      "grad_norm": 0.052117154002189636,
      "learning_rate": 1.2682926829268294e-05,
      "loss": 0.002,
      "step": 1153
    },
    {
      "epoch": 1.8764227642276423,
      "grad_norm": 0.011937848292291164,
      "learning_rate": 1.2520325203252033e-05,
      "loss": 0.0003,
      "step": 1154
    },
    {
      "epoch": 1.8780487804878048,
      "grad_norm": 0.017203371971845627,
      "learning_rate": 1.2357723577235774e-05,
      "loss": 0.0009,
      "step": 1155
    },
    {
      "epoch": 1.8796747967479674,
      "grad_norm": 0.022121993824839592,
      "learning_rate": 1.2195121951219513e-05,
      "loss": 0.0011,
      "step": 1156
    },
    {
      "epoch": 1.8813008130081301,
      "grad_norm": 0.001671174424700439,
      "learning_rate": 1.2032520325203254e-05,
      "loss": 0.0001,
      "step": 1157
    },
    {
      "epoch": 1.8829268292682926,
      "grad_norm": 0.025797665119171143,
      "learning_rate": 1.1869918699186992e-05,
      "loss": 0.0012,
      "step": 1158
    },
    {
      "epoch": 1.8845528455284553,
      "grad_norm": 0.04443657398223877,
      "learning_rate": 1.1707317073170733e-05,
      "loss": 0.0023,
      "step": 1159
    },
    {
      "epoch": 1.886178861788618,
      "grad_norm": 0.03167606145143509,
      "learning_rate": 1.1544715447154472e-05,
      "loss": 0.0013,
      "step": 1160
    },
    {
      "epoch": 1.8878048780487804,
      "grad_norm": 0.017619842663407326,
      "learning_rate": 1.1382113821138211e-05,
      "loss": 0.0006,
      "step": 1161
    },
    {
      "epoch": 1.889430894308943,
      "grad_norm": 0.04814261943101883,
      "learning_rate": 1.1219512195121952e-05,
      "loss": 0.0026,
      "step": 1162
    },
    {
      "epoch": 1.8910569105691057,
      "grad_norm": 0.017359178513288498,
      "learning_rate": 1.105691056910569e-05,
      "loss": 0.0007,
      "step": 1163
    },
    {
      "epoch": 1.8926829268292682,
      "grad_norm": 0.021797478199005127,
      "learning_rate": 1.0894308943089431e-05,
      "loss": 0.0006,
      "step": 1164
    },
    {
      "epoch": 1.8943089430894309,
      "grad_norm": 0.018363989889621735,
      "learning_rate": 1.073170731707317e-05,
      "loss": 0.0004,
      "step": 1165
    },
    {
      "epoch": 1.8959349593495936,
      "grad_norm": 0.0030904682353138924,
      "learning_rate": 1.0569105691056911e-05,
      "loss": 0.0001,
      "step": 1166
    },
    {
      "epoch": 1.897560975609756,
      "grad_norm": 0.027427013963460922,
      "learning_rate": 1.0406504065040652e-05,
      "loss": 0.0012,
      "step": 1167
    },
    {
      "epoch": 1.8991869918699187,
      "grad_norm": 0.017053866758942604,
      "learning_rate": 1.024390243902439e-05,
      "loss": 0.0005,
      "step": 1168
    },
    {
      "epoch": 1.9008130081300814,
      "grad_norm": 0.023760784417390823,
      "learning_rate": 1.0081300813008131e-05,
      "loss": 0.0011,
      "step": 1169
    },
    {
      "epoch": 1.9024390243902438,
      "grad_norm": 0.036323197185993195,
      "learning_rate": 9.91869918699187e-06,
      "loss": 0.0015,
      "step": 1170
    },
    {
      "epoch": 1.9040650406504065,
      "grad_norm": 0.0011583393206819892,
      "learning_rate": 9.756097560975611e-06,
      "loss": 0.0001,
      "step": 1171
    },
    {
      "epoch": 1.9056910569105692,
      "grad_norm": 0.022603392601013184,
      "learning_rate": 9.59349593495935e-06,
      "loss": 0.0011,
      "step": 1172
    },
    {
      "epoch": 1.9073170731707316,
      "grad_norm": 0.01900571770966053,
      "learning_rate": 9.43089430894309e-06,
      "loss": 0.0006,
      "step": 1173
    },
    {
      "epoch": 1.9089430894308943,
      "grad_norm": 0.014893371611833572,
      "learning_rate": 9.26829268292683e-06,
      "loss": 0.0004,
      "step": 1174
    },
    {
      "epoch": 1.910569105691057,
      "grad_norm": 0.05806029587984085,
      "learning_rate": 9.10569105691057e-06,
      "loss": 0.0028,
      "step": 1175
    },
    {
      "epoch": 1.9121951219512194,
      "grad_norm": 0.03153526037931442,
      "learning_rate": 8.94308943089431e-06,
      "loss": 0.0013,
      "step": 1176
    },
    {
      "epoch": 1.9138211382113821,
      "grad_norm": 0.002414993941783905,
      "learning_rate": 8.780487804878048e-06,
      "loss": 0.0001,
      "step": 1177
    },
    {
      "epoch": 1.9154471544715448,
      "grad_norm": 0.029624512419104576,
      "learning_rate": 8.617886178861789e-06,
      "loss": 0.0011,
      "step": 1178
    },
    {
      "epoch": 1.9170731707317072,
      "grad_norm": 0.029083741828799248,
      "learning_rate": 8.455284552845528e-06,
      "loss": 0.0007,
      "step": 1179
    },
    {
      "epoch": 1.91869918699187,
      "grad_norm": 0.03760691359639168,
      "learning_rate": 8.292682926829268e-06,
      "loss": 0.0017,
      "step": 1180
    },
    {
      "epoch": 1.9203252032520326,
      "grad_norm": 0.031772077083587646,
      "learning_rate": 8.130081300813009e-06,
      "loss": 0.0014,
      "step": 1181
    },
    {
      "epoch": 1.921951219512195,
      "grad_norm": 0.024253636598587036,
      "learning_rate": 7.967479674796748e-06,
      "loss": 0.0013,
      "step": 1182
    },
    {
      "epoch": 1.9235772357723577,
      "grad_norm": 0.04156636819243431,
      "learning_rate": 7.804878048780489e-06,
      "loss": 0.001,
      "step": 1183
    },
    {
      "epoch": 1.9252032520325204,
      "grad_norm": 0.02544698677957058,
      "learning_rate": 7.642276422764228e-06,
      "loss": 0.0011,
      "step": 1184
    },
    {
      "epoch": 1.9268292682926829,
      "grad_norm": 0.016023343428969383,
      "learning_rate": 7.479674796747968e-06,
      "loss": 0.0004,
      "step": 1185
    },
    {
      "epoch": 1.9284552845528455,
      "grad_norm": 0.021765774115920067,
      "learning_rate": 7.317073170731707e-06,
      "loss": 0.0007,
      "step": 1186
    },
    {
      "epoch": 1.9300813008130082,
      "grad_norm": 0.025316927582025528,
      "learning_rate": 7.154471544715448e-06,
      "loss": 0.0009,
      "step": 1187
    },
    {
      "epoch": 1.9317073170731707,
      "grad_norm": 0.052294325083494186,
      "learning_rate": 6.991869918699187e-06,
      "loss": 0.0016,
      "step": 1188
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.028452469035983086,
      "learning_rate": 6.829268292682928e-06,
      "loss": 0.0009,
      "step": 1189
    },
    {
      "epoch": 1.934959349593496,
      "grad_norm": 0.02812102623283863,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.001,
      "step": 1190
    },
    {
      "epoch": 1.9365853658536585,
      "grad_norm": 0.021374739706516266,
      "learning_rate": 6.504065040650407e-06,
      "loss": 0.0011,
      "step": 1191
    },
    {
      "epoch": 1.9382113821138212,
      "grad_norm": 0.014225897379219532,
      "learning_rate": 6.341463414634147e-06,
      "loss": 0.0003,
      "step": 1192
    },
    {
      "epoch": 1.9398373983739838,
      "grad_norm": 0.03854348137974739,
      "learning_rate": 6.178861788617887e-06,
      "loss": 0.0022,
      "step": 1193
    },
    {
      "epoch": 1.9414634146341463,
      "grad_norm": 0.01623712293803692,
      "learning_rate": 6.016260162601627e-06,
      "loss": 0.0006,
      "step": 1194
    },
    {
      "epoch": 1.943089430894309,
      "grad_norm": 0.018856678158044815,
      "learning_rate": 5.853658536585367e-06,
      "loss": 0.0005,
      "step": 1195
    },
    {
      "epoch": 1.9447154471544716,
      "grad_norm": 0.0010097093181684613,
      "learning_rate": 5.6910569105691056e-06,
      "loss": 0.0001,
      "step": 1196
    },
    {
      "epoch": 1.946341463414634,
      "grad_norm": 0.0009596573654562235,
      "learning_rate": 5.528455284552845e-06,
      "loss": 0.0001,
      "step": 1197
    },
    {
      "epoch": 1.9479674796747968,
      "grad_norm": 0.055859412997961044,
      "learning_rate": 5.365853658536585e-06,
      "loss": 0.0025,
      "step": 1198
    },
    {
      "epoch": 1.9495934959349595,
      "grad_norm": 0.02996722050011158,
      "learning_rate": 5.203252032520326e-06,
      "loss": 0.0014,
      "step": 1199
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 0.023344147950410843,
      "learning_rate": 5.040650406504066e-06,
      "loss": 0.0011,
      "step": 1200
    },
    {
      "epoch": 1.9528455284552846,
      "grad_norm": 0.0009118337766267359,
      "learning_rate": 4.8780487804878055e-06,
      "loss": 0.0001,
      "step": 1201
    },
    {
      "epoch": 1.9544715447154473,
      "grad_norm": 0.0029892371967434883,
      "learning_rate": 4.715447154471545e-06,
      "loss": 0.0001,
      "step": 1202
    },
    {
      "epoch": 1.9560975609756097,
      "grad_norm": 0.001134783262386918,
      "learning_rate": 4.552845528455285e-06,
      "loss": 0.0001,
      "step": 1203
    },
    {
      "epoch": 1.9577235772357724,
      "grad_norm": 0.01822792738676071,
      "learning_rate": 4.390243902439024e-06,
      "loss": 0.0004,
      "step": 1204
    },
    {
      "epoch": 1.959349593495935,
      "grad_norm": 0.04041174426674843,
      "learning_rate": 4.227642276422764e-06,
      "loss": 0.0023,
      "step": 1205
    },
    {
      "epoch": 1.9609756097560975,
      "grad_norm": 0.01843838021159172,
      "learning_rate": 4.0650406504065046e-06,
      "loss": 0.0007,
      "step": 1206
    },
    {
      "epoch": 1.9626016260162602,
      "grad_norm": 0.0347866453230381,
      "learning_rate": 3.902439024390244e-06,
      "loss": 0.0018,
      "step": 1207
    },
    {
      "epoch": 1.9642276422764229,
      "grad_norm": 0.018424589186906815,
      "learning_rate": 3.739837398373984e-06,
      "loss": 0.0004,
      "step": 1208
    },
    {
      "epoch": 1.9658536585365853,
      "grad_norm": 0.032554369419813156,
      "learning_rate": 3.577235772357724e-06,
      "loss": 0.0008,
      "step": 1209
    },
    {
      "epoch": 1.967479674796748,
      "grad_norm": 0.0395028218626976,
      "learning_rate": 3.414634146341464e-06,
      "loss": 0.0017,
      "step": 1210
    },
    {
      "epoch": 1.9691056910569107,
      "grad_norm": 0.013718499802052975,
      "learning_rate": 3.2520325203252037e-06,
      "loss": 0.0004,
      "step": 1211
    },
    {
      "epoch": 1.9707317073170731,
      "grad_norm": 0.023876165971159935,
      "learning_rate": 3.0894308943089435e-06,
      "loss": 0.0008,
      "step": 1212
    },
    {
      "epoch": 1.9723577235772358,
      "grad_norm": 0.019994627684354782,
      "learning_rate": 2.9268292682926833e-06,
      "loss": 0.0006,
      "step": 1213
    },
    {
      "epoch": 1.9739837398373985,
      "grad_norm": 0.031627148389816284,
      "learning_rate": 2.7642276422764227e-06,
      "loss": 0.0018,
      "step": 1214
    },
    {
      "epoch": 1.975609756097561,
      "grad_norm": 0.027036288753151894,
      "learning_rate": 2.601626016260163e-06,
      "loss": 0.0007,
      "step": 1215
    },
    {
      "epoch": 1.9772357723577236,
      "grad_norm": 0.00974267441779375,
      "learning_rate": 2.4390243902439027e-06,
      "loss": 0.0002,
      "step": 1216
    },
    {
      "epoch": 1.9788617886178863,
      "grad_norm": 0.01950746402144432,
      "learning_rate": 2.2764227642276426e-06,
      "loss": 0.0007,
      "step": 1217
    },
    {
      "epoch": 1.9804878048780488,
      "grad_norm": 0.037647172808647156,
      "learning_rate": 2.113821138211382e-06,
      "loss": 0.0021,
      "step": 1218
    },
    {
      "epoch": 1.9821138211382112,
      "grad_norm": 0.000934131327085197,
      "learning_rate": 1.951219512195122e-06,
      "loss": 0.0001,
      "step": 1219
    },
    {
      "epoch": 1.9837398373983741,
      "grad_norm": 0.01740243099629879,
      "learning_rate": 1.788617886178862e-06,
      "loss": 0.0001,
      "step": 1220
    },
    {
      "epoch": 1.9853658536585366,
      "grad_norm": 0.0021137886215001345,
      "learning_rate": 1.6260162601626018e-06,
      "loss": 0.0001,
      "step": 1221
    },
    {
      "epoch": 1.986991869918699,
      "grad_norm": 0.01898357830941677,
      "learning_rate": 1.4634146341463416e-06,
      "loss": 0.0005,
      "step": 1222
    },
    {
      "epoch": 1.988617886178862,
      "grad_norm": 0.024499306455254555,
      "learning_rate": 1.3008130081300815e-06,
      "loss": 0.001,
      "step": 1223
    },
    {
      "epoch": 1.9902439024390244,
      "grad_norm": 0.030974596738815308,
      "learning_rate": 1.1382113821138213e-06,
      "loss": 0.0009,
      "step": 1224
    },
    {
      "epoch": 1.9918699186991868,
      "grad_norm": 0.022479278966784477,
      "learning_rate": 9.75609756097561e-07,
      "loss": 0.0008,
      "step": 1225
    },
    {
      "epoch": 1.9934959349593497,
      "grad_norm": 0.035234980285167694,
      "learning_rate": 8.130081300813009e-07,
      "loss": 0.0011,
      "step": 1226
    },
    {
      "epoch": 1.9951219512195122,
      "grad_norm": 0.05741525813937187,
      "learning_rate": 6.504065040650407e-07,
      "loss": 0.0027,
      "step": 1227
    },
    {
      "epoch": 1.9967479674796746,
      "grad_norm": 0.017429886385798454,
      "learning_rate": 4.878048780487805e-07,
      "loss": 0.0006,
      "step": 1228
    },
    {
      "epoch": 1.9983739837398375,
      "grad_norm": 0.0008520183037035167,
      "learning_rate": 3.2520325203252037e-07,
      "loss": 0.0001,
      "step": 1229
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.03083091415464878,
      "learning_rate": 1.6260162601626018e-07,
      "loss": 0.0012,
      "step": 1230
    }
  ],
  "logging_steps": 1,
  "max_steps": 1230,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9193822428856320.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
